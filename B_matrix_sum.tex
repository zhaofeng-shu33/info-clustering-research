\documentclass{article}

\title{Info-Clustering Metric}
\author{zhaofeng-shu33}
\input{macros.tex}
\begin{document}

\maketitle
Suppose $U$ is a discrete distribution over $\{1, 2,\dots, K\}$, $X_1, \dots, X_n$ are independent conditioned on $U$.
Further suppose that each $X_i$ is weakly dependent with $U$.  That is, 
\begin{equation}\label{XUk}
P_{X_i | U=k}(x) = P_{X_i} (x)( 1 + \epsilon {\phi^{(k,i)}(x ) \over \sqrt{P_{X_i}(x)}} )
\end{equation}
We then have
\begin{align}
P_{X_i, X_j | U = k}(x_i, x_j)
=& P_{X_i | U=k}(x_i)
P_{X_j | U=k}(x_j) \notag \\
=& P_{X_i}(x_i)P_{X_j}(x_j)
(1 + \epsilon(\frac{\phi^{(k,i)}(x_i)}{P_{X_i}(x_i)}
+ \frac{\phi^{(k,j)}(x_j)}{P_{X_j}(x_j)}) +
\epsilon^2\frac{\phi^{(k,i)}(x_i)
\phi^{(k,j)}(x_j)}{P_{X_i}(x_i)P_{X_j}(x_j)})\label{eq:XiXj}
\end{align}
Since 
\begin{align*}
P_{X_i}(x) &= \sum_{k=1}^{K} P_{X_i | U=k}(x) P_U(u_k) \\
& =  \sum_{k=1}^{K}P_U(u_k)P_{X_i} (x)( 1 + \epsilon {\phi^{(k,i)}(x ) \over \sqrt{P_{X_i}(x)}} ) \textrm{ from } \eqref{XUk}\\
\Rightarrow & \sum_{k=1}^{K} P_U(u_k){\phi^{(k, i)}(x) \over \sqrt{P_{X_i}(x)}} =0,\forall i, x\in \mathcal{X}
\end{align*}
From \eqref{eq:XiXj} we have
\begin{equation}\label{eq:PXiXj}
P_{X_i, X_j}(x_i, x_j) = P_{X_i}(x_i)
P_{X_j}(x_j) (1+\epsilon^2 \sum_{k=1}^K P_U(u_k)
\frac{\phi^{(k,i)}(x_i)
\phi^{(k,j)}(x_j)}{P_{X_i}(x_i)P_{X_j}(x_j)})
)
\end{equation}
For more than 2 random variables:
\begin{align*}
P_{X_1,\dots,X_n}(x_1,\dots,x_n)  &= \sum_{k=1}^{K} P_{X_1,\dots,X_n | U=k}(x_1,\dots,x_n) P_U(u_k) \\
&=  \sum_{k=1}^{K}P_U(u_k) \prod_{i=1}^n P_{X_i|U=k}(x_i)\\
&= \sum_{k=1}^{K} P_U(u_k)\prod_{i=1}^n \left(P_{X_i} (x_i)( 1 + \epsilon {\phi^{(k,i)}(x_i ) \over \sqrt{P_{X_i}(x_i)}} )\right)\\
&=  \sum_{k=1}^{K}P_U(u_k) (\prod_{i=1}^n  P_{X_i} (x_i))
\left( 1 + \epsilon\sum_{i=1}^n {\phi^{(k,i)}(x_i) \over \sqrt{P_{X_i}(x_i)}} + \epsilon^2\sum_{i\neq j}{\phi^{k,i}(x_i)\phi^{k,j}(x_j)\over \sqrt{P_{X_i}(x_i)P_{X_j}(x_j)} }\right)+o(\epsilon^2) \\
&= (\prod_{i=1}^n  P_{X_i} (x_i))
(1+\epsilon\sum_{i=1}^n \sum_{k=1}^{K} P_U(u_k){\phi^{(k,i)}(x_i) \over \sqrt{P_{X_i}(x_i)}} \\
&+\epsilon^2 \sum_{k=1}^{K} P_U(u_k)\sum_{i\neq j}{\phi^{k,i}(x_i)\phi^{k,j}(x_j)\over \sqrt{P_{X_i}(x_i)P_{X_j}(x_j)} } ) + o(\epsilon^2)\\
&= (\prod_{i=1}^n  P_{X_i} (x_i))
\left(1 +\epsilon^2\sum_{i\neq j} \sum_{k=1}^{K}P_U(u_k){\phi^{k,i}(x_i)\phi^{k,j}(x_j)\over \sqrt{P_{X_i}(x_i)P_{X_j}(x_j)} }\right) + o(\epsilon^2)
\end{align*}
From \eqref{eq:PXiXj},
let $\widetilde{B}_{ij}(x_i, x_j)={P_{X_i, X_j}(x_i,x_j) - P_{X_i}(x_i)P_{X_j}(x_j) \over \sqrt{P_{X_i}(x_i)P_{X_j}(x_j)}} $, then we have:
\begin{align*}
\epsilon^2\sum_{k=1}^{K}P_U(u_k)
{\phi^{k,i}(x_i)\phi^{k,j}(x_j)\over \sqrt{P_{X_i}(x_i)P_{X_j}(x_j)} } & = {P_{X_i, X_j}(x_i, x_j) - P_{X_i}(x_i)P_{X_j}(x_j) \over P_{X_i}(x_i)P_{X_j}(x_j)} \\
& = {\widetilde{B}_{ij}(x_i, x_j) \over \sqrt{P_{X_i}(x_i)P_{X_j}(x_j)} }
\end{align*}
Therefore, we have 
\begin{equation}
P_{X_1,\dots,X_n}(x_1,\dots,x_n) =  (\prod_{i=1}^n  P_{X_i} (x_i))\left ( 1 + \sum_{i\neq j}{\widetilde{B}_{ij}(x_i, x_j) \over \sqrt{P_{X_i}(x_i)P_{X_j}(x_j)} }\right) +o(\epsilon^2)
\end{equation}
Then $P_{X_1,\dots, X_n}$ is in the neighborhood of $P_{X_1}\dots P_{X_n}$ with $$\phi(x_1,\dots, x_n)=
\sqrt{P_{X_1}(x_1)\dots P_{X_n}(x_n)}\left(\sum_{i\neq j}{\widetilde{B}_{ij}(x_i, x_j) \over \sqrt{P_{X_i}(x_i)P_{X_j}(x_j)} }\right)+o(\epsilon^2)$$

Using the conclusion from information geometry:
\begin{align*}
D(P_{X_1,\dots, X_n}|| P_{X_1}\dots P_{X_n}) & ={1 \over 2} \sum_{x_1,\dots,x_n}\phi^2(x_1,\dots, x_n) \\
& = {1\over 2}\sum_{x_1,\dots,x_n} (\prod_{i=1}^n  P_{X_i} (x_i)) \left(\sum_{i\neq j}{\widetilde{B}_{ij}(x_i, x_j) \over \sqrt{P_{X_i}(x_i)P_{X_j}(x_j)} }\right)^2 +o(\epsilon^2) 
\end{align*}
From the definition of $\widetilde{B}_{ij}$, the above is the summation over $\norm{\widetilde{B}_{ij}}^2_F$(the coupling item is zero). Therefore we get the second order approximation:
\begin{equation}
D(P_{X_1,\dots, X_n}|| P_{X_1}\dots P_{X_n}) \approx  {1 \over 2} \sum_{i\neq j} \norm{\widetilde{B}_{ij}}^2_F 
\end{equation}
Let $\Pi$ be the partition of $V=\{1,2,\dots, n\}$. Then we can show that 
\begin{equation}
D(P_{X_V} || \prod_{C\in \Pi} P_{X_{C}}) \approx {1 \over 2}\sum_{\substack{(i,j) \not\in C\\ C\in \Pi}} \norm{\widetilde{B}_{ij}}_F^2
\end{equation}
Then the multivariate mutual information can be defined as:
\begin{equation}
I(Z_V) = \min_{\Pi} {1 \over 2( \abs{\Pi} - 1) } \sum_{\substack{(i,j) \not\in C\\ C\in \Pi}} \norm{\widetilde{B}_{ij}}_F^2
\end{equation}
Specifically, when $n=2$ we have 
\begin{equation}
I(X;Y) \approx {1 \over 2} \norm{\widetilde{B}_{XY}}_F^2
\end{equation}
let $B(x,y)=\frac{P_{X,Y}(x,y)}{\sqrt{P_X(x)P_Y(y)}}$, then we have $\widetilde{B}(x,y)=B(x,y)-\sqrt{P_X(x)P_Y(y)}$ and 
\begin{equation}
\norm{\widetilde{B}}_F^2 = \norm{B}_F^2 -1
\end{equation}

\end{document}

