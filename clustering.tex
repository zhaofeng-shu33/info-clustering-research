\documentclass{article}

\input{macros.tex}
\newif\ifpublish
\newif\ifshowNonSelf
\newif\ifshowIrrelevant
\publishfalse
\showNonSelftrue
\showIrrelevantfalse
\begin{document}
\title{Introduction to info-clustering and DT algorithm}
\author{zhaofeng-shu33}
\maketitle
\tableofcontents
\section{Info-Clustering Background}
In this article, the theory of info-clustering (formulated by \cite{ic}) and Dilworth truncation algorithm (proposed by \cite{psp}) is reviewed.

Hierarchical clustering is a popular clustering method. It produces a clustering tree. 
An information-theoretic approach to hierarchical clustering is formulated in \cite{ic}.

The problem with Info-Clustering is that it is sensible to how graph weight is given, which makes it not practical in application.

The idea of this formulation is as follows:

Suppose we have a set of objects $Z_1, Z_2, \dots, Z_{\abs{V}}$ to be clustered, and for a given threshold $\gamma \in \mathbb{R}$, the set of clusters is defined as:
\begin{equation}\label{eq:def_CgammaZV}
C_{\gamma}(Z_V) = \textrm{maximal}\{ B \in V \vert \abs{B} > 1, I(Z_B) > \gamma \}
\end{equation}
This definition is well defined if \eqref{eq:P} holds. See Theorem \ref{thm:laminarity} for detail.

$C_{\gamma}(Z_V)$ consists of collection of disjoint subsets of $V$.
The definition of $C_{\gamma}(Z_V)$ excludes the singleton element,
we use the notation $\P_{\gamma}$ to denote the partition associated with $C_{\gamma}(Z_V)$. $\P_{\gamma}$ is obtained by adding $C_{\gamma}(Z_V)$ with all the other singletons.
$\P_{\gamma}$ is a partition of $V$.

\begin{theorem}\label{thm:hierarchical}
$\P_{\gamma_2} \preceq \P_{\gamma_1}$ if $\gamma_1 < \gamma_2$.
\end{theorem}
\begin{proof}
For $C \in \P_{\gamma_2}$, if $\abs{C}=1$, then  $C\in \P_{\gamma_1}$;
if $\abs{C}>1$, then $C \in C_{\gamma_2}(Z_V)$ and we have $I(Z_C) > \gamma_2 > \gamma_1$.
As a result, there exists $C' \in C_{\gamma_1}(Z_V)$ such that $C\subseteq C' \in \P_{\gamma_1}$.
\end{proof}
\begin{remark}
Theorem \ref{thm:hierarchical} shows that for a series of threshold $\gamma$, $\P_{\gamma}$ forms a \textbf{hierarchical clustering} structure.
\end{remark}

It can be seen there are finite values $\gamma$ such that $C_{\gamma}(Z_V)$ differs.
We call them the critical values of info-clustering.
More specifically:
\begin{equation*}
C_{\gamma}(Z_V) = \begin{cases}
\{V\} & \gamma < \gamma_1 \\
C_{\gamma_i}(Z_V) & \gamma \in [\gamma_i, \gamma_{i+1}), 1\leq i < N \\
\emptyset & \gamma \geq \gamma_N
\end{cases}
\end{equation*}
Obviously, the first critical value is $I(Z_V)$, and the critical value set is $\{\gamma_1, \dots, \gamma_N \}$.
Let $\P_{\gamma_0} = \{V\}$, then the hierarchical partition structure is $\P_{\gamma_0} \prec \P_{\gamma_1} \prec \dots \P_{\gamma_N}$.
where $\P_{\gamma_N} = \{1, 2, \dots, \abs{V}\}$.

The key is how to define $I(Z_B)$, which is called the multivariate mutual information(MMI) in \cite{ic}. Generally speaking, the definition of $I(Z_B)$ should have the following three properties:
\begin{itemize}
\item satisfies equation \eqref{eq:P}
\item computable.
\item theoretically interpretable.
\end{itemize}

We give a definition as follows:
\begin{align}\label{eq:IZV}
I(Z_V) & = \min_{\mathcal{P} \in \Pi'(V)} I_{\mathcal{P}}(Z_V) \\
\label{eq:newDef}  I_{\mathcal{P}}(Z_V) & = \frac{1}{ 2( \abs{\mathcal{P}} - 1) } \sum_{\substack{(i,j) \not\in C\\ C\in \mathcal{P}}} w_{ij}
\end{align}

\begin{remark}
From the definition of $I(Z_V)$, we can get the scaling-invariant property of info-clustering. 
That is, multiplying all $ w_{ij}$ by the same constant does not affect the clustering result.
\end{remark}
\section{Info-Clustering Property}
\subsection{Graph weight interpretation}
$w_{ij}$ can be treated as a kind of similarity metric between two objects. It is naturally to associate $w_{ij}$ with the edge weight in a graph.
If we treat each index as a vertex in a graph, then the partition $\P$ divides these vertices into $\abs{\P}$ groups.  Besides, we define
\begin{align}
f(C) & = \frac{1}{2}\sum_{i\in C, j \in V\backslash C} w_{ij} \\
f_B(C) & = \frac{1}{2}\sum_{i\in C, j \in B\backslash C} w_{ij}  \textrm{  for }C \subseteq B  \\
\end{align}
We can write 
\begin{align}
f[\P] & = \sum_{C \in \P} f(C) \\
I_{\mathcal{P}}(Z_V)  &= \frac{1}{\abs{\P}  - 1}f(\P) 
\end{align}
we call $I(Z_V)$ multivariate similarity between it measures the similarity shared by multiple objects.
\subsection{Property}
Below we show some mathematical properties of $I(Z_V)$.
\ifpublish
\input{notation.tex}
\input{examples.tex}
\fi
\input{lemma.tex}
From Lemma \ref{lem:LargeIZV}, we can show that
\begin{theorem}[Laminarity]\label{thm:laminarity}
\begin{equation}\label{eq:P}
I(Z_{C_1 \cup C_2}) \geq \min\{ I(Z_{C_1}), I(Z_{C_2})\}, \textrm{ for } C_1\cap C_2 \neq \emptyset
\end{equation}
\end{theorem}
This property is vital in the definition of $C_{\gamma}(Z_V)$. Suppose we have
$I(B_1)>\gamma, I(B_2)>\gamma$. If $B_1 \cap B_2 \neq \emptyset$, then $I(Z_{B_1\cup B_2})>\gamma$ and the maximal meaning in the definition of $C_{\gamma}(Z_V)$ holds.
Therefore $C_{\gamma}(Z_V)$
From literature \cite{ic}, this property implies we can use the iteration method to compute the entire clustering
provided we can compute the first critical value and the first set of clusters.
\begin{proof}
	We proceed by contradiction. Let $V=C_1 \cup C_2$ and suppose $I(Z_V) < I(Z_{C_1}),
	I(Z_V) < I(Z_{C_2})$. There exists $\P$ such that $I_{\P}(Z_V) = I(Z_V)$.
	Then by Lemma \ref{lem:LargeIZV} we can get $C_1 \subset B_1, C_2 \subset B_2, B_1, B_2 \in
	\P$. Since $C_1 \cap C_2 \neq \emptyset \Rightarrow B_1 = B_2$. Therefore $V=C_1 \cup C_2 \subset B_1$ which contradicts $\abs{\P}>1$.
\end{proof}
\begin{theorem}\label{thm:lattice}
Let $\Pi^* =  \{ P \in \Pi: I_{\P}(Z_V) = I(Z_V)\}$, then $\Pi^*$ has the lattice structure. That is, 
if $I(Z_V) = I_{\P_1}(Z_V) = I_{\P_2}(Z_V)$, then $I(Z_V) 
=  I_{\P_1 \vee \P_2}(Z_V) = 
 I_{\P_1 \wedge \P_2}(Z_V)$
\end{theorem}
\begin{proof}
	
Consider $C \in \P_1 \vee \P_2$, $C\not\in \P_1$. we will show that $I(Z_C) = I(Z_V)$.

We have $C = \cup_{i=1}^k B_i ,B_i\in \P_1, k>1$. 
From Lemma \ref{lem:smallZB} we have $I(Z_C) \leq I(Z_V)$.
Consider the relationship of $C$ with $\P_2$. if $C\in \P_2$, by Lemma \ref{lem:elementLarge} we have
$I(Z_C) \geq I(Z_V)$. The equality $I(Z_C) = I(Z_V)$ holds.

Suppose  $C = \cup_{j=1}^r D_j ,D_j\in \P_2, r>1$. Then $B_i \neq D_j$ by the definition of $\P_1\vee \P_2$. From Lemma \ref{lem:elementLarge}, $I(B_i)\geq I(Z_V), I(D_j) \geq I(Z_V)$
Therefore, we can found $D_t$ such that $B_1 \cap D_t\neq\emptyset$. Then from the laminarity property of MMI, $I(B_1 \cup D_t)\geq I(Z_V)$. If $B_1 \cup D_t \subsetneq C$, we can find another $B_i$ or $D_j$ which has nonempty intersection with  $B_1 \cup D_t$. Otherwise $\{B_1 \cup D_t, B_2, \dots B_k\}$
forms a partition of $C$ and $D_t \subset B_1$; $\{D_t \cup B_1\}\cup \{ D_1, \dots D_r\}\backslash \{D_t\}$
forms a partition of $C$ and $B_1 \subset D_t$. That is $B_1 = D_t$, a contradiction with $B_1 \neq D_t$. 
We can continue this process. If this process stops when $(\cup B_i)\cup(\cup D_j) \subsetneq C$. we then have $\cup B_i = \cup D_j \in \P_1 \vee \P_2$, a contradiction with $C \in \P_1 \vee \P_2$. 
Therefore, this process stops when the union equals $C$ and we will have $I(Z_C) \geq I(Z_V)$.

Since $\P_1 \preceq \P_1 \vee \P_2 $, using Lemma \ref{lem:multi_combination}, we have
\begin{equation*}
I_{\P_1}(Z_V) = \Pr(\P_1 \vee \P_2) I_{\P_1 \vee \P_2}(Z_V) + \sum_{Q \in \P\backslash\P'} \Pr(Q) I_{Q}(Z_{\cup_{C\in Q}C})
\end{equation*}
Since $I_{Q}(Z_{\cup_{C\in Q}C})\geq I(Z_{\cup_{C\in Q}C}) = I(Z_V)$, we have
\begin{equation*}
I(Z_V) \geq \Pr(\P_1 \vee \P_2) I_{\P_1 \vee \P_2}(Z_V) + (1-\Pr(\P_1 \vee \P_2)) I(Z_V)	
\end{equation*}
We get $I_{\P_1 \vee \P_2}(Z_V) \leq I(Z_V)$, then we have $I_{\P_1 \vee \P_2}(Z_V) =  I(Z_V)$.

On the other hand, consider $C\in \P_1, \not\in \P_1\wedge\P_2$ and $C = \cup_{i=1}^k B_i, k>1$ where $B_i \in \P_1 \wedge \P_2$.
Consider $\P_C = \{B_1, \dots, B_k\}$. Each $B_i$ belongs to one of subsets of $\P_2$. Suppose $B_1, B_2 \subseteq D_i \in \P_2$. Then $B_1\cup B_2 \subseteq D_i$,
which contradicts the maximal property of $\P_1\wedge \P_2$. Therefore, we have $\P_D = \{D_1, D_2, \dots D_k\}\subseteq \P_2$ such that $B_i \subset D_i$.
From Lemma \ref{lem:smallZB}, we have $I_{\P_D}(Z_D) \leq I(Z_V)$. Since $\abs{\P_D} = \abs{\P_C}$, From the definition we have $I_{\P_C}(Z_C) \leq I_{\P_D}(Z_D) \leq I(Z_V)$.
Then using Lemma \ref{lem:ref_combination}, let $\P' = \P_C \cup \P_1\backslash \{C\}$ we have
\begin{align*}
I_{\P'}(Z_V) & = \theta I_{\P_1}(Z_V) + (1-\theta) I_{\P_C}(Z_C) \\
& \leq \theta I(Z_V) + (1- \theta)I(Z_V) = I(Z_V)
\end{align*} 
Since $I(Z_V)$ is minimum value, then we have $I_{\P'}(Z_V)=I(Z_V)$.
$\P'\wedge \P_2 = \P_1 \wedge \P_2$ and we can repeat the above procedure using $\P_1 = \P'$. It terminates in finite steps since $\abs{V} > \abs{\P'} > \abs{\P_1}$.
\end{proof}

From Theorem \ref{thm:lattice}, we have the corollary:
\begin{corollary}
There is a unique finest partition $\P^* \in \Pi^*$, referred to as the \textbf{fundamental partition}.
\end{corollary}
\begin{proof}
Since $\Pi^*$ has the lattice structure, the meet of all its member is the unique finest partition $\P^*$.
\end{proof}
\begin{theorem}\label{thm:strict_larger_mi}
The fundamental partition $\P^*$ with the singletons removed is the set of all maximal subsets $B \subseteq V$ with strictly larger mutual information. 
That is, we have
$I(Z_B) > I(Z_V)$ for $B \in \P^*, \abs{B}>1$.
\end{theorem}
\begin{proof}
Suppose $I(Z_B)=I_{\P_B}(Z_B)$.
Let $\P' = \P_B \cup  \P^* \backslash \{B\}$.
Then by Lemma \ref{lem:ref_combination}, there exists $\theta \in (0,1)$ such that 
\begin{equation}\label{eq:convexZ}
I_{\P'}(Z_V) = \theta I_{\P^*}(Z_V) + (1-\theta) I_{\P_B}(Z_B)
\end{equation}
Since $\P^*$ is finest, we have $I_{\P'}(Z_V) > I(Z_V)$, then from \eqref{eq:convexZ}, we have 
$I(Z_V) < \theta I(Z_V) + (1-\theta) I(Z_B) \Rightarrow I(Z_B) > I(Z_V)$.

\end{proof}

From Theorem \ref{thm:strict_larger_mi} we have 
\begin{theorem}
Suppose $\P_{\gamma_i} = \{C, C\in V \}$, then
\begin{equation}
\gamma_{i+1} = \displaystyle\min_{C\in \P_{\gamma_i}, \abs{C}>1} I(Z_C) 
\end{equation}
Denote the collection of set which achieves $\gamma_{i+1}$ as $C_{i+1}:=\{C \in V | I(Z_C) = \gamma_{i+1}\}$, then
\begin{equation}
\P_{\gamma_{i+1}}/\P_{\gamma_i} = \bigcup_{C \in C_{i+1}} \P_C^*
\end{equation}
\end{theorem}
\begin{proof}
	Suppose $B \in \P_{\gamma_{i+1}}$, since $I(Z_B)>\gamma_{i+1} > \gamma_i$, then by the definition of info-clusters, $B$ is a subset of $C$ for $C \in \P_{\gamma_i}$.	
	If $I(Z_C) > \gamma_{i+1}, \forall C\in \P_{\gamma_i}$, then $B$ is not maximal since $C$ also satisfies $I(Z_C) > \gamma_{i+1}$.
	Therefore, let  $\gamma^* = \min_{C \in \P_{\gamma_i}, \abs{C}>1} I(Z_C), \gamma_{i+1} \geq \gamma^*$.
	Suppose $I(Z_C')=\gamma^*$, then there exists $B\in \P^*_{C'}$ such that $I(Z_B) > I(Z_C') = \gamma^*$ and $B$ is maximal.
	Hence $\gamma^*$ makes $\P_{\gamma_{i+1}}$ strictly finer than $\P_{\gamma_i}$. And $\gamma_{i+1} = \gamma^*$.
	
	For $B \in \bigcup_{C \in C_{i+1}} \P_C^*$, by theorem \ref{thm:strict_larger_mi}, we have $I(Z_B) > I(Z_C) =  \gamma_{i+1}$ and $B$ is maximal,
	then $B \in \P_{\gamma_{i+1}}/\P_{\gamma_{i}}$;
	for $B \in \P_{\gamma_{i+1}}/\P_{\gamma_{i}}$, $B\subsetneq C \in \P_{\gamma_i}$, since $B$ is maximal, $I(Z_C) \leq \gamma_{i+1}$, 
	then $I(Z_B) > I(Z_C)$,
	by Lemma \ref{lem:LargeIZV} and maximal property of $B$, $B \in \P^*_C$.
\end{proof}
\begin{proposition}[info-clustering]\label{prop:ta}
The following two procedures generate the same hierarchical tree. Each non-leaf node of the hierarchical tree is represented by a subset $S \subset V$, which is called a cluster of graph $G$.
\begin{enumerate}
\item For a graph $G$, suppose $I_{\P^*}(Z_V)=I(Z_V)$, each subset of $\P^*$ is a child of hierarchical tree root $V$. For each child node set $C$, use partition $\P_C$($I(Z_C)=I_{\P_C}(Z_C)$) to split it until this branch is divided into singletons.
\item For a graph $G$, suppose $I(Z_C) = \max_{B\subseteq V} I(Z_B)$ and $C$ is maximal, merge singleton element of $C$ together and contract $G$. At each step select the set with maximal multivariate similarity to contract the graph until the graph is contracted to a single node, which is the tree root.
\end{enumerate}
\end{proposition}
Theorem \ref{thm:laminarity} implies the maximal requirement for the bottom-up approach is unambiguous. First we notice that if the subsets are non-overlapping, we can merge them respectively. Therefore, we only need to consider $C_1, C_2$ such that $I(Z_{C_1}) = I(Z_{C_2}) = \max_{B\subseteq V} I(Z_B)$ and $C_1\cap C_2 \neq \emptyset$. , $I(Z_{C_1\cup C_2}) \geq \max_{B\subseteq V} I(Z_B)$ therefore $C_1\cup C_2$ is also a solution to the maximization.

We can make the connection of the two approaches with the original definition of info-clusters in 
equation \eqref{eq:def_CgammaZV}.

The smallest critical value of $\mathcal{C}_{\gamma}(Z_V)$ is $I(Z_V)$  and for each $B \in \P$, since  our $\P$ is fundamental partition we have $I(Z_B) > I(Z_V)$ by Theorem \ref{thm:strict_larger_mi} for $\abs{B}>1$. Take $\gamma = I(Z_V)$, by Lemma \ref{lem:LargeIZV}, if $I(Z_C) > \gamma$ we have $C\subseteq B' \in \P$. Therefore $B$ is maximal and $B\in \mathcal{C}_{\gamma}(Z_V)$. The same argument can be done for subtrees by replacing $V$ by sub-root. Therefore, all tree node sets by top-down approach belong to $\mathcal{C}_{\gamma}(Z_V)$. By Theorem 2.4 in \cite{ic}, we conclude all clusters in $C_{\gamma}(Z_V)$ is a tree node set by top-down approach.

The largest critical value of $\mathcal{C}_{\gamma}(Z_V)$ is $\gamma_N = \max_{B\subseteq V} I(Z_B)$. Take $\gamma = \gamma_N - \epsilon$ where $\epsilon$ is sufficient small, since $I(Z_C) > \gamma$ and $C$ is maximal, we have $C \in \mathcal{C}_{\gamma}(Z_V)$. For each tree node set, since it is also maximal for some $\gamma$, all of them belong to $\mathcal{C}_{\gamma}(Z_V)$. By Theorem 4.2 in \cite{agg}, we conclude all clusters in $C_{\gamma}(Z_V)$ is a tree node set by bottom-up approach.

By the construction of the clustering tree, we can associate each tree node with a threshold value, which is multivariate similarity computed at that step.

Below we show the connection of info-clustering problem with principal sequence of partition(PSP) problem. The conclusion is that all their critical values 
and finest partitions coincide.

\section{Dilworth truncation}


\begin{align}
I(Z_V) &= \max\{ \lambda \in \R_+: \lambda \leq f(\P)/(\abs{\P} - 1) \text{ for all partition } \P \in \Pi'(V)\} \\
& = \max\{ \lambda \in \R_+:  -\lambda \leq f(\P) - \abs{\P} \lambda \} \text{ for all partition } \P \in \Pi'(V)\} \\
\label{eq:Connection}& = \max\{ \lambda \in \R_+: -\lambda \leq h(\lambda)\}
\end{align}
where 
\begin{equation}\label{eq:hLambda}
h(\lambda) = \min_{\P} \sum_{C\in \P} (f(C)-\lambda)
\end{equation}
$h(\lambda)$ is the Dilworth truncation(DT) of the submodular function $f(C)$ and can be computed by \textbf{DT} Algorithm \ref{alg:dt}. $h(\lambda)$
is piecewise linear. 

\begin{lemma}\label{lem:lattice_structure}
Let $f$ be a submodular function on the subset of $V$. Let $\P_1, \P_2$ be partitions
at which $f$ reaches its minimum. Then $f$ reaches a minimum at $\P_1 \vee \P_2$ and $\P_1 \wedge \P_2$.
\end{lemma}
\begin{proof}
See Theorem 3.5 of literature \cite{psp}.	
\end{proof}
\begin{lemma}\label{lem:hierarchical}
Let $f$ be a submodular function on the subset of $V$. Let $\P_1, \P_2$ be partitions
at which $h(\lambda_i)(i=1,2)$ reaches its minimum. 
If $\lambda_1 < \lambda_2$, then $\P_2 \preceq \P_1$.
\end{lemma}
\begin{proof}
	See Theorem 3.7 of literature \cite{psp}.	
\end{proof}


In $h(\lambda)$, for each turning point of line segments, 
from Lemma \ref{lem:lattice_structure},
the optimal partition forms a lattice. 

The connection of$I(Z_V)$ with DT is through equation \eqref{eq:Connection}. More precisely, $I(Z_V)$
is the solution of equation $-\lambda = h(\lambda)$.
And then we show its connection with $h(\lambda)$.
\begin{theorem}\label{thm:f}
$I(Z_V)$ is the smallest value $\lambda$ that satisfies
\begin{equation}\label{eq:rir}
-\lambda = h_{\lambda}(\P)
\end{equation}
for some $\P \in \Pi'(V)$. Let $\Pi^*$ be the set of all such partitions satisfying \eqref{eq:rir}
for $\lambda = I(Z_V)$.
\end{theorem}
\begin{proof}
$I(Z_V)$ satisfies equation \eqref{eq:rir} for some $\P$. We only need to show the equality never holds for $\lambda < I(Z_V)$ and all partition $\P$. Indeed, we show $-\lambda < h_{\lambda}(\P)$.
\begin{align*}
-\lambda < h_{\lambda}(\P) &\iff -\lambda < f[\P] - \abs{\P} \lambda \\
& \iff \lambda < \frac{ f[\P] }{ \abs{\P} - 1 } \\
& \Leftarrow \lambda < \min_{\P \in \Pi'(V)} \frac{f[\P]}{  \abs{\P} - 1 } \\
& \iff \lambda < I(Z_V)
\end{align*}
\end{proof}
\begin{remark}
Geometrically speaking, we know that the first line segment of $h(\lambda)$ is $\lambda$. Theorem \ref{thm:f} shows that $I(Z_V)$ is the first critical value of $h(\lambda)$. It is the x coordinate of the intersection point of $-\lambda$ and a line with slope large than 1. $\Pi^*(Z_V) $ are just the collection of partitions which minimize $h_{\lambda}(\P)$ at $\lambda = I(Z_V)$.
\end{remark}

\begin{corollary}\label{cor:F}
$\Pi^*= \{ P \in \Pi: I_{\P}(Z_V) = I(Z_V)\}$
\end{corollary}
\begin{proof}
Let $\gamma = I(Z_V)$
\begin{align*}
-\gamma = h_{\gamma}(\P) & \iff  \gamma = { f[\P] \over \abs{\P} - 1 } \\
& \iff I(Z_V) = I_{\P}(Z_V)
\end{align*}
\end{proof}
This connection is similar with RIR in literature \cite{ic}. We have $f(V)=0$ in our definition and the first line segment is $-\lambda$. Therefore
$(I(Z_V),  y_1)$ is the first turning point of $h(\lambda)$.
\begin{corollary}\label{cor:large}
	\begin{equation}
	\lambda \geq I(Z_V) \iff h(\lambda) \leq -\lambda
	\end{equation}
\end{corollary}
\begin{proof}
	From equation \eqref{eq:hLambda} or from the geometric meaning.
\end{proof}

Using the Corollary \ref{cor:large}, we can give an algorithm to compute $I(Z_V)$.
\begin{algorithm}\label{alg:first_critical_value_computing}
	\begin{algorithmic}[1]
		\REQUIRE Statistics of $Z_V$ sufficient for calculating $f(B)$ for all $B \subseteq V$
		\ENSURE $I(Z_V)$ and partition $\P'$ such that $I_{\P'}(Z_V) = I(Z_V)$
		\STATE $\P \leftarrow \{ \{i \} | i \in V\}$
		\STATE $\lambda = I_{\P}(Z_V)$
		\STATE $(h(\lambda), \P)=DT(f,\lambda)$
		\WHILE{$h(\lambda) < -\lambda$}
		\STATE $\lambda = I_{\P}(Z_V)$
		\ENDWHILE
		\STATE $I(Z_V) = \lambda, \P' = \P$
	\end{algorithmic}
\end{algorithm}

We have shown that the first critical value and partition match in the two problems. We can show all their critical values and partitions are the same. See Theorem 3.3 in \cite{ic} for detail.

\subsection{Computing first critical value via fundamental partition}
Algorithm \ref{alg:first_critical_value_computing} shows the detail. This is also the algorithm to compute the graph strength.

\subsection{clustering by fundamental partition}
\begin{algorithm}
\begin{algorithmic}[1]
\REQUIRE Statistics of $Z_V$ sufficient for calculating \texttt{FundamentalPartition}$(B)$ for all $B \subseteq V:\abs{B}>1$
\ENSURE $\mathcal{S}$ is a list of $(I(Z_C),C)$ for $ C \in \mathcal{C}(Z_V)$, which gives
$\Gamma(Z_V) = \{ \gamma' | (\gamma', B) \in \mathcal{S}\} $ and $ \mathcal{C}_{\gamma}(Z_V)
= \mathrm{maximal}\{B | (\gamma', B) \in \mathcal{S}, \gamma' > \gamma \}$
\STATE $\mathcal{S},\mathcal{T} \leftarrow$ empty queues
\STATE enqueue $V$ to $\mathcal{T}$
\WHILE{$\mathcal{T}$ is non-empty}
\STATE $B \leftarrow $ dequeue $\mathcal{T}$
\STATE $(\gamma, \P) \leftarrow$\texttt{FundamentalPartition}$(B)$
\STATE enqueue $(\gamma, B)$ to $\mathcal{S}$
\STATE enqueue all non-singleton elements of $\P$ to $\mathcal{T}$
\ENDWHILE
\FUNCTION{\texttt{FundamentalPartition}$(B)$}
  \RETURN $(I(Z_B), \P^*(Z_B))$
\ENDFUNCTION
\end{algorithmic}
\end{algorithm}

The computation of fundamental partition is not easy. Therefore the above algorithm is not practical.

\subsection{Hierarchical clustering by PSP}
We inspect the property of $h(\lambda)$ from \eqref{eq:hLambda}. Since $h(\lambda)$ is piecewise linear, it is characterized by its turning points where the slope changes.
At turning point $p_i$, the optimal solution to achieve $h(\lambda)$ forms a lattice $\Pi_i$.
Define $\mathcal{P}_i = \min \Pi_i$, i.e. the finest partition from $\Pi_i$. Then we can show that
$\P_1 \succ \P_2 \dots \succ \P_N$, where $\P_1$ is the fundamental partition of $Z_V$ and $\P_N$ contains singleton elements only.

\section{Relation with other models}
\subsection{Relation to PIN model}
In PIN(Pairwise Independent Network)(literature \cite{pin}, see definition 2.1 and 2.2 for detail).
We can show that ($E$ is the edge set, $e \in C$ represents the edge $e$ is in the subgraph $C$)
\begin{align*}
\sum_{C \in \P} H(Z_C) - H(Z_V) & = \sum_{C \in \P} (\sum_{e \in C} H_e + \sum_{\substack{e = (u,v) \\ u \in C, v\not\in C }} H_e) - \sum_{e \in E} H_e \\
& = {1 \over 2}\sum_{C \in \P}  \sum_{\substack{e = (u,v) \\ u \in C, v\not\in C }} H_e
\end{align*}
Therefore, in PIN model, we have
\begin{equation}
I(Z_V) = \min_{ \P \in \Pi'(V)} \frac{ \sum_{C \in \P} c(\delta_{G}(C))}{2(\abs{\P} - 1 )}
\end{equation}
where $\delta_{G}(C) = \{e = (u, v) \in E | u \in C, v \not\in C\} $ and $c(A) = \sum_{e\in A} H_e$.
This is (4.21a) in literature \cite{ic}
\subsection{Relatition with entropy formulation}
In literature \cite{ic}, the definition of MMI is based on the submodular function is $H(Z_C)$, which is the joint entropy. It has the non-deceasing property such that the larger $C$ is, the bigger value $H(Z_C)$ takes. However, in our formulation of MMI, we use the graph cut function, it does not have the monotonic property and $f(V) = 0$ from the definition. Therefore, our work is not a special case of literature \cite{ic} even through many conclusions are similar.

In PIN model, the two definitions coincide. But the PIN model has strong assumptions and is not practical. Our formulation of MMI is suitable to be applied directly to cluster cases in real work application.
\subsection{Relation with mac}
minimum average cost is a similar but different clustering method.

In literature \cite{mac}, only the first critical value and partition information are used to do the clustering task. The threshold is controlled by $\beta$ not our $\lambda$.

\subsection{Relation with cross information potential}
In our formulation, $f(C)$ can be regarded as the unnormalized cross information potential \cite{gokcay2002information} when the Gaussian kernel similarity metric is used.

The unnormalized cross information for $A$ and $B$ is defined as 
\begin{equation}
\textrm{U-CIP}(A, B) = \sum_{i \in A, j \in B} G(x_i - x_j, \sigma^2)
\end{equation}
where $G$ is pdf of Gaussian distribution. This quantity measures shared information between two different random varibles. In this interpretation, we treat each cluster as a random variable and data within this cluster as its sample. The distribution for this cluster random variable is estimated by Parzen Window Method using Gaussian kernel. Also a special Renyi entropy is used. For our problem, U-CIP measures the cross information potential between cluster $C$ and the others $V\backslash C$.
\section{Contributions}
\begin{enumerate}
\item Use graph cut function to reformulate info-clustering.
\item Implementation of the algorithm in C++, which is fast and can be used by others.
\item The algorithm can be customized by using different similarity metric between two objects.
\end{enumerate}
\appendix
\section{Algorithm Description}
\begin{figure}[!ht]
\centering
\includegraphics[width=5cm]{pic/pyramid.eps}
\caption{pyramid structure of info-clustering algorithm}\label{fig:ps}
\end{figure}
We use method of principal sequence of partition(PSP) to implement the info-clustering algorithm. As figure \ref{fig:ps} shows, PSP is at the highest level of algorithm pyramid. By divide and conquer technique, PSP calls at most $\abs{V}$ times of so-called Dilworth truncation(DT) algorithm. The relationship between PSP and DT is shown in Algorithm \ref{alg:psp}. Algorithm \ref{alg:psp} follows Algorithm 3 in \cite{ic}, but we use different data structures.
\begin{algorithm}
\caption{PSP algorithm}\label{alg:psp}
\begin{algorithmic}[1]
\REQUIRE Statistics of $Z_V$ sufficient for calculating $f(B)$ for all $B \subseteq V$
\ENSURE A sorted critical value array \textbf{L} and a reverse ordered array \textbf{PSP} containing $\P_1,\dots, P_k$.
\STATE \textbf{L}  $\leftarrow$ empty list.
\STATE $Q\leftarrow \{V\}, P \leftarrow \{ \{i \} | i \in V\}$
\STATE $\mathbf{PSP}= [Q, P]$
\STATE \texttt{Split}$(Q,P)$
\STATE sort $L$ and sort $\mathbf{PSP}$ with respect to $\succeq$ 
\FUNCTION{\texttt{Split}$(Q,P)$}
 \STATE\label{alg:gamma} $\gamma' = {1 \over \abs{P} - \abs{Q}} (f(P)-f(Q))$
 \STATE $h' = {1 \over \abs{P} - \abs{Q}}(\abs{P} f(Q) - \abs{Q} f(P))$
 \STATE $(\tilde{h}, P') = \texttt{DT}(f,\gamma')$ \footnotemark
 \IF{$\tilde{h} = h'$}
 	\STATE insert $\gamma'$ to $\mathbf{L}$
 \ELSE
 	\STATE insert $P'$ to $\mathbf{PSP}$
 	\STATE \texttt{Split}$(Q, P')$
 	\STATE \texttt{Split}$(P',P)$
 \ENDIF
\ENDFUNCTION
\end{algorithmic}
\end{algorithm}
\footnotetext{$\tilde{h} \leq h'$ by theory}
In Algorithm \ref{alg:psp}, the function \texttt{Split} first calculates the intersection point of two lines: $f(P)-\abs{P}\lambda$ and $f(Q)-\abs{Q}\lambda$(in line~\ref{alg:gamma}) and then calls the DT algorithm to get the minimal value and minimal partition at $\gamma'$. From literature \cite{mac}, when there is partition between $Q$ and $P$, the $Split$ algorithm recursively calls itself to calculate all partitions.

For the middle level(DT) in the pyramid, it calls at most $\abs{V}$ times submodular function minimization(SFM) algorithm.
The algorithm is the same with procedure \texttt{FINDPARTITION} from literature \cite{mac}.

Actually, it uses Edmonds' greedy algorithm to finish this task. Consider the polyhedron defined by $f_{\lambda}(C) = f(C)-\lambda$($h_{\lambda}(\P)= \sum_{C \in \P} f_{\lambda}(C)$):
\begin{equation}
P(f_{\lambda}) = \{ x \in \R |  x(C) \leq f_{\lambda}(C), \emptyset \neq \forall C \subseteq V\}
\end{equation}

A non-empty set $C$ is called \textit{tight} for a vector $x \in P(f_{\lambda})$ if $x(C) = f_{\lambda}(C)$.
If we can find a vector $ x^* \in P(f_{\lambda}) $ and a complete partition $\P^*$ containing only tight partitions,
then $\P$ satisifies
\begin{align*}
h_{\lambda}(\P^*) &= \sum_{C \in \P^*} f_{\lambda}(C) \\
 &=\sum_{C\in P^*} x^*(C)  \\
& = \sum_{C \in \P} x^*(C), \forall \P \subset \Pi(V) \\
& \leq \sum_{C\in \P} f_{\lambda}(C), \textrm{since } x^* \in P(f_{\lambda}) \\
& = h_{\lambda}(\P),\forall \P \subset \Pi(V) 
\end{align*}

Let $V=\{1, 2, \dots, n\}$, the greedy algorithm to find $x^*$ and $\P^*$ is given in \ref{alg:dt}
\begin{algorithm}
\caption{Dilworth truncation algorithm(\texttt{DT(f,$\lambda$)})}\label{alg:dt}
\begin{algorithmic}[1]
\REQUIRE function $f$ and $\lambda$ which are requirements in the definition of $h_{\lambda}(\P)$.
\ENSURE the partition $\P^*$ s.t. $h(\lambda) = h_{\lambda}(\P^*)$ and the value of $h(\lambda)$
\STATE $V^0 = \emptyset, x = $ \texttt{zero vector}, $\mathcal{A} = $ \texttt{empty partition} 
\FOR{$l=1, 2, \dots, n$}
\STATE $V^l = \{l\} \cup V^{l-1}$
\STATE\label{alg:tight} compute $x^*_l = \displaystyle\min_{ A: l \in A \subseteq V^l} f_{\lambda}(A)- x(A)$ and $T^l$ is one of the minimizer.
\STATE $U^l = T^l \cup [\cup \{A | A \in \mathcal{A}, A \cap T^l \neq \emptyset\}] $
\STATE $\mathcal{A} = \{U^l\} \cup \{A | A \in \mathcal{A}, A \cap T^l = \emptyset \}$
\ENDFOR
\STATE $\P^* = \mathcal{A}, h_{\lambda} = x^*(V)$
\end{algorithmic}
\end{algorithm}

In line \ref{alg:tight} of Algorithm \ref{alg:dt}, the set $T^l$ remains tight.

SFM algorithm has been proved to have combinatorial polynomial time algorithm. However, in applications provable combinatorial algorithm for SFM does not perform well. General implementation like Fujishige-Wolfe minimum norm algorithm( in literature \cite{fwrobust}) fails because of intractable accuracy.
To finish our job, we focus on the specific problem to minimize 
\begin{equation}\label{eq:alpha}
\alpha^l = \min \{ f(S) - x^{l-1}(S) - \lambda: l \in \forall S \subset V^l\}
\end{equation}
When $f(S)$ is digraph cut function, we can convert the problem to minimum $s-t$ cut problem(in Algorithm 2, literature \cite{pin}). 
\begin{equation}\label{eq:beta}
\beta^l = \min_{T \subseteq V^l \backslash \{s\}: l \in T} c(V^l \backslash T, T)
\end{equation} 
The s-t cut graph is initialized with $\{s,1,2,\dots, l\}$, in which $s$ is a newly added node and $l$ is the target node. For the weight,
$w_{si}  = \max\{0, - x^{l-1}_i\}, w_{il} = \max\{0, x^{l-1}_i\} + c_{il}$ where $c_{il}$ is the original weight of the graph. Notice the position of minus sign is not important, we only need to ensure
$w_{si} + w_{il} = c_{il} + \abs{x^{l-1}_i}$

The optimal solution matches while the optimal values differs a constant value. 
\begin{equation}
\beta^l - \alpha^l = \lambda + \sum_{x^{l-1}_v>0, 1\leq v<l}x_{v}
\end{equation} 

If there are multiple solutions to \eqref{eq:alpha}. The solution set equals. However, in 
practical computations, the maximal flow algorithm can only compute one set (minimizer).
If our max flow algorithm gives maximum source size result, then using  \eqref{eq:beta} method we will get the solution to $\alpha^l$ with minimum $S$.

If we want the solution to $\alpha^l$ with minimum $S$, we can change the position of source and sink, to solve the following problem.
\begin{equation}\label{eq:gamma}
\gamma^{l} = \min_{S \in V^l \backslash \{t\}, l \in S} c(S, V\backslash S)
\end{equation}
In equation \eqref{eq:gamma}, $l$ is treated as source node.  Also we have
\begin{equation}
\gamma^l - \alpha^l = \lambda + \sum_{x_v>0, 1\leq v<l}x_{v}
\end{equation} 
The s-t cut graph is initialized with $\{1,2,\dots, l,t\}$, in which $t$ is a newly added node and $l$ is the source node. For the weight,
$w_{it}  = \max\{0, - x^{l-1}_i\}, w_{li} = \max\{0, x^{l-1}_i\} + c_{il}$.
\subsection{Example}

In this subsection, we use an illustrative example to explain the algorithm pyramid.
The first simulation setting(Fig~\ref{fig:4p}) is the same with subsection 5.1 in \cite{mac}.  We use principal sequence of partition to obtain the following result.
Aslo, figures on the same line are from one execution of \texttt{split}.

In the second simulation environment(Fig~\ref{fig:3c}), we generate 60, 100, 140 points uniformly on three circles and 10 random points on the line segment. The similarity metric is based on the polar coordinates of each point.

\begin{figure}[!ht]
\includegraphics[width=12cm]{pic/4part.eps}
\caption{Illustrative example from four Gaussians}\label{fig:4p}
\end{figure}

\begin{figure}[!ht]
\includegraphics[width=12cm]{pic/3circle.eps}
\caption{Illustrative example from three circles}\label{fig:3c}
\end{figure}
\subsection{Comparison with other clustering method}
In this section, we use 4 different clustering methods and test them on 5 different datasets. The clustering methods are kmeans, spectral clustering, affine propagation and our info-clustering algorithms. We use adjusted rand score as accuracy criterion. The 5 datasets are 4 Gaussian blobs, 3 circles and the other three are from UCI datasets(Iris, Glass and Libras).
\begin{table}[!ht]
\centering
\InputIfFileExists{compare.tex}{}{}
\caption{clustering accuracy for the proposed and existing algorithms}
\end{table}
\subsection{Application to Outlier detection}
Since Info-clustering algorithm often produces singletons as a cluster. It is very suitable to apply info-clustering to detect anomaly in datasets.

Given a dataset with $n$ training data point in $m$ dimensional space. We measure their pairwise similarity by $w_{ij} = \exp\{-\frac{\norm{x_i - x_j}^2}{m}\} $. Then we can apply info-clustering algorithm. The largest critical value after running the algorithm is denoted by $\gamma_N$. We select the clustering result corresponding to last by two partition with the largest cluster as normal points and the others as anomaly (usually they are just singletons).
Suppose there are $N$ normal points and the index set is $V$. For newly entered points with index $j'$, we can predict whether it is normal points by comparing $J_T(Z_{V'})$ with $\gamma_N$, where $V' = B\cup \{j'\}$.
\begin{proposition}
\begin{equation}
J_{T}(Z_{V'}) > \gamma_N \iff  \sum_{j \in B} w_{i'j} > \gamma_N 
\end{equation}
\end{proposition}	
\begin{proof}
	In this proof, we make some notation convention.
	$w(C) = \sum_{(i,j) \in E(C)} w_{ij}$
	$w(A, C) = \sum_{i \in A, j \in C, (i,j) \in E(V')} w_{ij}$ and
	$\P_C = \{\{i \}| i \in C \}$.
	
	If $ \sum_{i \in B} w_{ij} > \gamma_N$ we can get
$$
J_{T}(Z_{V'}) \geq I_{\P_{V'}}(Z_{V'}) = \frac{(N-1)\gamma_N + \sum_{j \in B}w_{i'j}}{N} > \gamma_N
$$

On the other hand, suppose $J_{T}(Z_{V'}) = I(Z_K) > \gamma_N$. Then $K$ must contain $j'$. If $K=V'$ then the conclusion holds. Otherwise, Suppose $K = \{j'\} \cup B', B=B'\cup J, J\neq \emptyset$. We can write 
\begin{equation}
\gamma_N = \frac{w(J,B') + w(J) + w(B')}{ \abs{B'} + \abs{J} - 1 }
\end{equation}
Since $I_{\P_K}(Z_K)$ is maximal, we have $I_{\P_K}(Z_K) > I_{\P_{V'}}(Z_{V'})$ and $I_{\P_K}(Z_K) \geq I_{\P_{B'}}(Z_{B'})$.
\begin{align*}
	\frac{w(\{i'\}, B') + w(B')}{\abs{B'}} >& \frac{w(B') + w(J, B') + w(J) + w(\{i'\}, B') + w(\{i'\}, J)}{\abs{B'} + \abs{J}}  \\
	\frac{w(\{i'\}, B') + w(B')}{\abs{B'}} \geq & \frac{w(B')}{\abs{B'} - 1}
\end{align*}
we can get 
\begin{align}
\abs{J} (w(\{i'\}, B') + w(B')) > & \abs{B'} (w(J, B') + w(J) + w(\{i'\}, J)) \label{eq:target}
  \\
	(\abs{B'} - 1)  w(\{i'\}, B') \geq & w(B') \label{eq:convert}
\end{align}
Take equation \eqref{eq:convert} in \eqref{eq:target}, we can get
\begin{equation}\label{eq:summation}
	\abs{J} w(\{i'\}, B') > w(J, B') + w(J) + w(\{i'\}, J)
\end{equation}	
Adding equation \eqref{eq:convert} and \eqref{eq:summation} we can get
$w(\{i'\}, B') > \gamma_N$. Then $\sum_{j \in B}w_{i'j} > \gamma_N $ follows.
\end{proof}
We get the boundary condition whether predicted data point $x$ is normal point (Suppose $V=\{1,2,\dots, N \}$ ):
\begin{equation}
	\sum_{i=1}^N \exp\{ -\frac{\norm{x-x_i}^2}{2} \}	\geq \gamma_N
\end{equation}
We make a simple experiment to show the power of info-clustering applied to outliers detection problem. We call our method info-detection. There are 300 points on planar space with 45 outliers and 255 normal points centered around the origin. We compare two algorithms, Elliptic Envelope and Info-detection. Both method achieve 100\% accuracy for this dataset. The Elliptic Envelope method needs to specify the number of outliers as a parameter while our algorithm needs no parameter to tune.  Further, the decision boundary of Elliptic Method is tight around the normal points while the boundary of Info-detection is a little loose.
\begin{figure}[!ht]
	\centering
	\includegraphics[width=11cm]{pic/outlier_boundary_illustration.eps}
	\caption{Comparison of different outliers detection algorithm}\label{fig:outlier}
\end{figure}
\bibliographystyle{plain}
\bibliography{exportlist}
\end{document}
