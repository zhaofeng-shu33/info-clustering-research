% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
\input{extra_math.tex}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
%
\title{Info-Detection: An Information-Theoretic Approach to Detect Outlier}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Feng Zhao\inst{1} \and
Fei Ma\inst{2} \and
Yang Li\inst{2}
Lin Zhang\inst{1,2}}
%
\authorrunning{F. Author et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Department of Electronic Engineering, Tsinghua University, PR China \and
Tsinghua-Berkeley Shenzhen Institute 
\email{zhaof17@mails.tsinghua.edu.cn}
}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
We propose a cluster analysis-based outlier detection method called Info-Detection. The method originates from Info-clustering. We accelerate the computing of principal sequence of partition, which is used to implement Info-clustering. The acceleration makes it possible to finish outlier detection task in real-world task. Info-Detection is an adaptive method and does not require any hyper parameters like k neighbor or outlier fraction. Experiments show that better accuracy can be achieved with an affordable time overhead.

\keywords{Outlier Detection \and Info-clustering \and Principal Sequence of Partition.}
\end{abstract}
%
%
%
\section{Introduction}
Outlier Detection is an important task in data mining. Many algorithms such as local outlier factor, elliptic envelope and one-class SVM exist. Many of them give an outlier score to each data point and the outliers are chosen as top n. For real-world task, $n$, the number of outlier is unknown and mismatched $n$ produces no good result. Also, besides distance metric, many methods have other hyper parameters to tune. To solve this problem, we propose an adaptive method called Info-Detection, which can find the number of outliers and split given data into inliers and outliers, give outlier scores and predict new observations in constant times.
 
Info-Detection comes from Info-clustering, proposed by Chen Chuan et al. in \cite{RN1}. Info-clustering has solid theoretical foundations from information theory and can produce hierarchical tree for the random variables to be clustered. Children with the same parent node in the tree shares more information than siblings with different parent. Combined with graph structure, Info-clustering can be transfered to cluster dataset directly, which produces same clustering result as minimum cost clustering with $\beta = 1$ in \cite{RN7}.  Using the tree structure, we can score the outliers. Graph-based clustering method does not have prediction ability. But for this specific task, based on Info-clustering theory, we will show that a natural prediction scheme exists. This scheme only has constant time complexity and can be applied directly for new observation.

One drawback of original Info-clustering comes from the algorithm complexity. In this paper, we improve the implementation of info-clustering, which is one order of magnitude faster than the original one. This new implementation makes it possible for info-clustering to be applied on outlier detection task.
Info-clustering is originally implemented by solving the principal sequence of partition problems for different critical values. For given critical value, a procedure called Dilworth truncation is invoked. However, different invocation of Dilworth truncation is independent and does not utilizes the intrinsic hierarchical structure of the method. By using the hierarchical tree structure, we compute latter Dilworth truncation on smaller graph. This is the basic idea of the new implementation. 

The paper is organized as follows. In section II, we make a brief introduction on info-clustering and show how it can be used to detect outlier. In Section III, we review the original algorithm of info-clustering and give our new accelerated implementation. In Section IV, we conduct experiments of outlier detection and compare Info-detection with other methods. Finally, we make the conclusion in Section V.

The notation convention of this paper is as follows: the directed graph is denoted by $G(V, E)$. Node index set $V=\{1, 2,\dots, \abs{V}\}$, node set $Z_V=\{Z_i | i \in V\}$, edge set $E=\{(i, j)\}$. Each edge is associated with a non-negative weight $w_{ij}$. $\P$ is a partition of $V$. That is, $P=\{C_1, \dots, C_k\}, \cup_{i=1}^k C_i=V$ and $i\neq j \Rightarrow C_i \cap C_j =\emptyset $. $F(\cdot)$ is the graph in-cut function, defined as $f(C)=\sum_{i \neq C, j\in C, (i,j) \in E} w_{ij}$. $\Pi$ is the collection of all partitions of $V$ and $\Pi'=\Pi\backslash\{V\}$. A partial order $ \P_1 \preceq \P_2$ on $\Pi$ is defined as
$C \in \P_1 \Rightarrow \exists C' \in \P_2 \, s.t. \, C \subseteq C'$.
Finally, $f[\cdot]$ is a function defined on $\Pi$ by $f[\P]=\sum_{C\in \P}f(C)$.

\section{Formulation of Info-Detection}
Info-Detection originates from info-clustering. Given a graph $G(V,E)$, info-clustering defines the cluster as follows:
\begin{align}
I_{\P}(Z_V) & := \frac{ f[\P] }{  \abs{\mathcal{P}} - 1 }\\
I(Z_V) & := \min_{\mathcal{P} \in \Pi'(V)} I_{\mathcal{P}}(Z_V)  \label{eq:ms} \\
C_{\gamma}(Z_V) & := \textrm{maximal}\{ B \in V \vert \abs{B} > 1, I(Z_B) > \gamma \}
\end{align}
where $I(Z_B)$ is the shared information of $Z_B$. For the definition of $I_{\P}(Z_B)$, the subgraph $G(B,E(B))$ is considered instead.

For the given threshold $\gamma$, $C_{\gamma} (Z_V)$ represents a collection of non-intersecting subset of $V$. Every two sets from $\bigcup_{\gamma} C_{\gamma}(Z_V)$ are either disjoint or have subset relationship. Therefore, they can be put in a tree $\mathcal{T}$ with the property that $A$ is a parent of $B$ if $B\subset A$. $\mathcal{T}$ also includes $\{j\}$ as leaf node. 

For sufficiently large $\gamma$, $C_{\gamma} (Z_V)$ will become empty set. The largest threshold value which makes such transition is denoted by $\gamma_N$, which has the following form:
\begin{equation}\label{eq:gammaN}
\gamma_N = \max_{B\subseteq V, \abs{B}>1} I(Z_B)
\end{equation}
In agglomerative interpretation of info-clustering \cite{RN8}, each singleton element $\{i\}$ is regarded as a leaf node of the hierarchical tree. $\gamma_N$ represents the first step to process the tree node. That is, merging elements of B to form a stem node. Traditional hierarchical clustering method using linkage criterion restricts $\abs{B}=2$. Our information criterion defined in \eqref{eq:gammaN} does not have such restriction. 

For our Info-Detection proposal, we use $\gamma_N$ to detect anomaly in two cases. For existing nodes, suppose $\gamma_N=I(Z_B)$ and $B$ is maximal. Then nodes in $V\backslash B$  are outlier. We can score outlier $Z_j$ with the minimal depth of the set containing $j$ in the hierarchical tree. 
\begin{example}
	\begin{figure}[!ht]
		\centering
		\includegraphics[width=8cm]{pic/outlier_example.eps}
		\caption{Info-Detection applied to 6 points on Cartesian plane}\label{fig:ex}
	\end{figure}
	Consider a graph consisting of 6 points on Cartesian plane (Fig. \ref{fig:ex}). The grid has one unit and the arc weight $w_{ij} = \exp(-d_{ij}^2)$ where $d_{ij}$ is the Euclidean distance of $x_i$ and $x_j$. For example $w_{45} = \exp(-5)$. The arc direction is from $i$ to $j$ for $i<j$. From equation \eqref{eq:gammaN}, We can compute $B=\{1,2,3,4\}$ and $\gamma_N = \frac{4\exp(-1)+2\exp(-2)}{3}\approx 0.58$. The whole hierarchical tree $\mathcal{T}$ can be computed, which is shown on the left figure of \ref{fig:ex}. The outlier score for $Z_6$ is higher than $Z_5$ from the tree depth interpretation.
\end{example}
For newly added node $i'$. Let $V'=B\cup \{i'\}$, we can compute $\gamma'_N$ for $V'$ and compare it with $\gamma_N$. If $\gamma'_N>\gamma_N$, $i'$ is normal observation since it is more integrated with existing nodes. Otherwise, $i'$ is anomaly. It can be shown that we do not need to compute $\gamma'_N$ to determine whether $\gamma'_N>\gamma_N$. We summarize our main result as follows:
\begin{proposition}\label{prop:main}
\begin{equation}
\gamma'_N > \gamma_N \iff  \sum_{i \in B} w_{ii'} > \gamma_N 
\end{equation}
\end{proposition}
From Proposition \ref{prop:main}, we can see that the computational overhead to check whether new data is normal is linear to the size of existing normal data. 
The prediction requires $\gamma_N$, whose computation is not a trivial task. It has been found that the mathematical structure of info-clustering is the same with that of principal sequence of partition(PSP) of graph. We use $\P_1, \dots, \P_k$ to denote the sequence where $\P_1 = \{V\}, \P_{i+1} \preceq \P_i$ and $\P_k = \{\{1\},\dots,\{k\}\}$.
Each $\P_i$ is the solution to the following optimization problem:
\begin{align}
h_{\P}(\lambda) &=  f[\P] - \abs{\P} \lambda  \label{eq:hPL}\\
h(\lambda) &= \min_{\P \in \Pi'(V)} h_{\P}(\lambda) \label{eq:hlambda}
\end{align}
We call $\lambda^*$ is a critical value for PSP if $\P_i, \P_{i+1}$ are both minimizer for \eqref{eq:hlambda}.
The largest critical value is equal to $\gamma_N$. However, it is difficult to solve this critical value directly without solving other critical values. The next section is focused on the algorithm to solve equation \eqref{eq:hlambda} for all $\lambda$.
\section{Improved principal sequence of partition}
The canonical way to solve equation \eqref{eq:hlambda} uses divide and conquer techniques to find the critical values and use Dilworth truncation (DT) to solve \eqref{eq:hlambda} at each specific $\lambda$ \cite{RN7}.  This implementation has $\abs{V}^2 \textrm{MF}(G)$ time complexity. \textrm{MF} represents the time complexity of maximum flow algorithm for a graph $G$. 

An improvement is proposed in \cite{RN4} using parametric maximum flow to finish the job. Although this method can achieve $\abs{V} \textrm{MF}(G)$ theoretically, it also increases the spacial complexity to $ \abs{V} \textrm{S}(G)$ where $\textrm{S}$ represents the spacial complexity to store a graph $G$. For dense graphs with $\abs{V}^2$ non-zero weight, the spacial complexity is formidable. 
% analysis of spatial complexity of pmf
Another problem of parametric maximum flow improvement is the floating point accuracy. Since this method uses the flow map of last computation to initiate the next initial flow map. The excess flow of some nodes may not be exactly zero. The non-zero excess can lead to unpredictable result and currently there is no good idea to tackle this issue. 

The above two problems prohibits such improvement from practical implementation. In this section, we abandon this approach and give another improvement which also achieves $\abs{V} \textrm{MF}(G)$ time complexity in general, only $\textrm{S}(G)$ spatial complexity and no floating point issue.

Since the PSP has the inclusion relationship, we can restrict further invocation of DT to smaller graphs. To be more specific, suppose we get $P_i = {C_1, \dots, C_t}$ for the first invocation of DT. Then we can compute PSP for each $C_i(i=1,\dots, t)$ respectively and construct those $P_j(j>i)$ from the subgraph computation. For $P_j(j<i)$, we can contract the graph $G$  to $G^t$ which has $t$ nodes by contracting each $C_i$ to a single node. By applying DT to $G^t$ can we get $P_j(j<i)$. We summarize our improved algorithm in Algorithm \ref{alg:psp_i_simplified}.

\begin{algorithm}
	\caption{An Improved Principal Sequence of Partition Algorithm}\label{alg:psp_i_simplified}
	\begin{algorithmic}[1]
		\REQUIRE a directed graph $G(V, E)$; edge cost function $c(e)$ for $e\in E$
		\ENSURE a hierarchical tree $\mathcal{T}(K, E)$ where $K \subseteq 2^{V}$ is node set and $E$ is edge set. $c'$ is edge weight map.
		\STATE initialize tree $\mathcal{T}$ with $V$ as root node, $\{j\}(j\leq \abs{V})$ as leaf node and no stem node.
		\STATE \texttt{Split}($G, V$)
		\FUNCTION{\texttt{Split}($\widetilde{G}, \widetilde{V}$)}
		\STATE $w$ is the summation of all edge weights within $\widetilde{G}$ 
		\STATE $\gamma' = \frac{w}{\abs{V(\widetilde{G})}-1}$ where $V(\widetilde{G})$ is the node set of graph $\widetilde{G}$
		\STATE $(\tilde{h}, P') = \texttt{DT}(\widetilde{G}, \gamma')$ where $\P'$ is minimizer of equation \eqref{eq:hlambda} and $\tilde{h}$ is the corresponding minimum value.  \label{alg:DT}
		\IF{$\tilde{h} = - \gamma'$}
		\STATE add edge weight $\gamma'$ in $\mathcal{T}$ starting from $\widetilde{V}$ to its children.
		\ELSE
		\FOR{$S$ in $P'$ and $\abs{S}>1$}
		\STATE make each children of $\widetilde{V}$ in $\mathcal{T}$ have new parent $S$		
		\STATE make the parent of $S$ be $\widetilde{V}$
		\STATE \texttt{Split}($\widetilde{G}[S], S$) where $\widetilde{G}[S]$ is the subgraph of $\widetilde{G}$ restricted on $S$
		\STATE contract $S$ to a single node % graph \widetilde{G} is modified
		\ENDFOR 
		\STATE \texttt{Split}($\widetilde{G}, \widetilde{V}$)		
		\ENDIF
		\ENDFUNCTION
	\end{algorithmic}
\end{algorithm}
	
\begin{figure}[!ht]
	\includegraphics[width=\textwidth]{pic/alg_illustration.eps}
	\caption{Improved PSP for the graph in (b), hierarchical tree evolves from (a), (c) to (d) }\label{fig:alg_eg}
\end{figure}

\begin{example}
	We use a simple example to explain Algorithm \ref{alg:psp_i_simplified}. Consider a graph $G(V, E)$ with $V=\{1,2,3,4\}, E=\{(1,2),(1,3),(2,3),(1,4),(2,4)\}$. The weight values are $w_{13}=2, w_{12}=5, w_{23}=2, w_{14}=1, w_{24}=1$. The graph is illustrated
	in Fig. \ref{fig:alg_eg} (b). Initially, the hierarchical tree is shown in Fig. \ref{fig:alg_eg} (a). Computing $\gamma' = \frac{11}{4-1}, \tilde{h} = -\frac{16}{3} < -\gamma' $ and $\P' = \{\{1,2,3\},\{4\}\}$ from line \ref {alg:DT} in Algorithm \ref{alg:psp_i_simplified} we get the hierarchical tree shown in Fig. \ref{fig:alg_eg} (c).
	
	Then we run PSP for the subgraph $G[\{1,2,3\}]$, $\gamma' = \frac{9}{2}, \tilde{h} = -5 < -\gamma'$ and $\P= \{\{1,2\},\{3\}\}$ we get the tree shown in Fig. \ref{fig:alg_eg} (d). The other computation calculate the edge weight of $\mathcal{T}$ shown in Fig. \ref{fig:alg_eg} (d).
\end{example}		

To analyze the time complexity of Algorithm \ref{alg:psp_i_simplified}. We suppose $\abs{V} = n, \abs{E} = O(n^2)$ and highest-relabel preflow algorithm is used to solve the maximum flow problem. Then DT has $O(n^4)$ time complexity. 
We use $T(n)$ to represent the time complexity when the graph has $n$ nodes. We have the following recursive relationship. Under very general conditions, we can show that $T(n) = O(n^4)$. 

In Algorithm \ref{alg:psp_i_simplified}, the computation is done on the input graph and only has $O(n^2)$ space complexity. This is superior than parametric preflow improvement. Besides, Algorithm \ref{alg:psp_i_simplified} does not have floating accuracy problem since each DT computation is freshly started. 

Our improvement of algorithm is not restricted to Info-Detection scenario. It can be used in general graph partition problems when PSP structure is needed.

\section{Experiment Result}
In this section, we first illustrate the decision boundary of info-detection by two artificial datasets. Then we conduct an experiment matrix, in which info-detection is compared with other commonly used outlier detection algorithms on both artificial and real-world datasets.

The two artificial datasets are prepared as follows:
\begin{enumerate}
\item We generate two Gaussian blobs with the same center and standard deviation 0.5. There are 255 points in total and 45 noisy points outside.
\item We generate two semi-circles which located at different centers. The 300 points on the two semi-circles are evenly distributed among all angles but deviated from the radius in Gaussian distribution. There are 45 points not belonging to the "moon" structure and are treated as outlier.
\end{enumerate}

By using the artificial dataset, we can split each into inlier set and outlier set. The boundary curve between the two sets can be used to distinguish new points and has the following equation from Proposition \ref{prop:main}:
\begin{equation}
\sum_{j \in B} \exp(-\gamma \norm{x - x_j}^2)= \gamma_N
\end{equation}
The weight is chosen by Gaussian kernel, which produces reasonable result for these two artificial dataset. As shown by Fig. \ref{fig:boundary}, the boundary curve for info-detection is approximately the closure of the inlier figure. We also draw the decision boundary for the Elliptic Envelope method. Since this method assumes the inlier data is distributed in convex region, the decision boundary is an ellipse, which is not like the ground truth.
\begin{figure}[!ht]
	\centering
	\includegraphics[width=\textwidth]{pic/outlier_boundary_illustration.eps}
	\caption{Detection boundary lines of different methods on different artificial dataset}	\label{fig:boundary}
\end{figure}

Besides the two artificial dataset, we also prepare two real-world dataset: Lymphography (first used by \cite{Lazarevic}) and Glass (\cite{hics}). Lymphography dataset has 148 instances with 6 outliers while Glass dataset has 214 instances with 7 outliers. We compare info-detection method with two commonly used method: local outlier factor detection and isolation forest. We use two metrics to measure the overall performance of the detection: true positive rate(TPR) and false negative rate(FNR). The inliner is treated as positive sample. It is difficult for a method to achieve high score on both two metrics. We manage to maximize FNR while controlling TPR $\geq 90\%$. The result is shown in Table \ref{tab:compare}.
\begin{table}[!ht]
\centering
\InputIfFileExists{../../experiment/outlier_detection/build/id_compare.tex}{}{}
\caption{Comparison of info-detection with other outlier detection algorithm on artificial and real-world dataset}\label{tab:compare}
\end{table}

All results in Table \ref{tab:compare} achieves TPR $\geq 90\%$. It can be seen that info-detection achieves highest FNR for these dataset, which means it detects no less outliers than other methods.

\section{Conclusion}
In this paper, we propose Info-Detection, which is an adaptive method to detect outliers in dataset. We design an efficient algorithm to solve the hierarchical structure based on existing PSP algorithms. By experiments we show that Info-Detection performs quite well.
%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\bibliographystyle{splncs04}
\bibliography{exportlist}
%
\appendix
\section{Proposition Proofs}
\begin{proof}[Proof of Proposition \ref{prop:main}]
		In this proof, we make some notation convention.
		$w_C = \displaystyle\sum_{(i,j) \in E(C)} w_{ij},$
		$w(A, C) = \displaystyle\sum_{\substack{i \in A, j \in C \\ (i,j) \in E(V')}} w_{ij}$ and
		$\P_C = \{\{i \}| i \in C \}$.
		
		If $ \sum_{i \in B} w_{ij} > \gamma_N$ we can get
		$$
		J_{T}(Z_{V'}) \geq I_{\P_{V'}}(Z_{V'}) = \frac{(N-1)\gamma_N + \sum_{j \in B}w_{i'j}}{N} > \gamma_N
		$$
		
		On the other hand, suppose $J_{T}(Z_{V'}) = I(Z_K) > \gamma_N$. Then $K$ must contain $j'$. If $K=V'$ then the conclusion holds. Otherwise, Suppose $K = \{j'\} \cup B', B=B'\cup J, J\neq \emptyset$. We can write 
		\begin{equation}
		\gamma_N = \frac{w(J,B') + w(J) + w(B')}{ \abs{B'} + \abs{J} - 1 }
		\end{equation}
		where $w_J = \sum_{(i,j) \in E(J)} w_{ij}, w(J, B') = \sum_{i \in J, j \in B', (i,j) \in E(V')} w_{ij}$. Since $I_{\P_K}(Z_K)$ is maximal, we have $I_{\P_K}(Z_K) > I_{\P_{V'}}(Z_{V'})$ and $I_{\P_K}(Z_K) \geq I_{\P_{B'}}(Z_{B'})$.
		\begin{align*}
			\frac{w(\{i'\}, B') + w(B')}{\abs{B'}} >& \frac{w(B') + w(J, B') + w(J) + w(\{i'\}, B') + w(\{i'\}, J)}{\abs{B'} + \abs{J}}  \\
			\frac{w(\{i'\}, B') + w(B')}{\abs{B'}} \geq & \frac{w(B')}{\abs{B'} - 1}
		\end{align*}
		we can get 
		\begin{align}
			\abs{J} (w(\{i'\}, B') + w(B')) > & \abs{B'} (w(J, B') + w(J) + w(\{i'\}, J)) \label{eq:target}
			\\
			(\abs{B'} - 1)  w(\{i'\}, B') \geq & w(B') \label{eq:convert}
		\end{align}
		Take equation \eqref{eq:convert} in \eqref{eq:target}, we can get
		\begin{equation}\label{eq:summation}
		\abs{J} w(\{i'\}, B') > w(J, B') + w(J) + w(\{i'\}, J)
		\end{equation}	
		Adding equation \eqref{eq:convert} and \eqref{eq:summation} we can get
		$w(\{i'\}, B') > \gamma_N$. Then $\sum_{j \in B}w_{i'j} > \gamma_N $ follows.
\end{proof}	
\section{Experiment Detail}
Since info-detection is a graph-based method, how to choose good weight between two data point is important for application of info-detection to dataset. For our experiment, several metrics are tried: 
\begin{enumerate}
\item Gaussian kernel: $w_{ij} = \exp(-\gamma \norm{x_i - x_j}^2)$
\item Laplacian kernel: $w_{ij} = \exp(-\gamma \norm{x_i - x_j}_1)$
\item kernel weight combined with k-neighbor filtering. Suppose after kernel computation and sorting we have $w_{i1} > w_{i2} > \dots > w_{in}$, then the weight assignment scheme is
$$
w'_{ij}  = 
\begin{cases}
 w_{ij} & j \leq k \\
 0 & j > k
\end{cases}
$$
This weight choice scheme has two parameters to tune: $k$ and $\gamma$. But it has two advantages. On one hand, it makes the graph have less edge and info-detection can run much faster. On the other hand, it restricts the linkage of one graph node to its k-nearest neighbors, which has more intuition in many cases.

The source code of our experiment is available \footnote{\scriptsize\url{https://github.com/zhaofeng-shu33/info-clustering-research/tree/master/experiment/outlier\_detection}} for reproduction.
For our experiment comparison, there are some additional notice.
\begin{itemize}
\item The accuracy of isolation-forest is determined by its intrinsic randomness. With the same parameter configuration, we select the result of one run of it, not possibly the best result. Info-detection and local outlier detector are deterministic method and do not have this problem.
\item For local outlier factor and isolation forest, it needs to know the number of outliers in advance. We set this parameter to be ground truth for these two methods to produce best result. For info-detection, we try different metric and parameters and select the best.
\end{itemize}

\end{enumerate} 
\end{document}
