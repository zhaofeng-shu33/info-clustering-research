\documentclass{article}
\usepackage[preprint]{neurips_2019}
\usepackage{hyperref}       % hyperlinks
\input{extra_math.tex}
\title{Supplementary Material}
\begin{document}
\maketitle
\appendix
\section{Proofs}
This section gives proof details about propositions in the paper.
\input{proposition_top_bottom.tex}
Before processing, we give some properties of multivariate similarity $I(Z_V)$.
\begin{lemma}\label{lem:ref_combination}
For any $B \in \P' \in \Pi'$ and $\P_B \in \Pi'_B$, let $\P = \P_B \cup \P' \backslash \{B\} $
be the refinement of $\P'$. Then we have
\begin{equation}\label{eq:ref_combination}
I_{\P}(Z_V) = \theta I_{\P'}(Z_V) + (1-\theta) I_{\P_B}(Z_B)
\end{equation}
\end{lemma}
\begin{proof}
	First we have $\abs{\P} = \abs{\P'} -1 + \abs{\P_B}$,
	Since
	\begin{align*}
		I_{\P}(Z_V) & = { f[\P'] + f_B[\P_B] \over \abs{\P} - 1} \\
		I_{\P'}(Z_V) & = { f[\P'] \over \abs{\P'} - 1} \\
		I_{\P_B}(Z_B) & = { f_B[\P_B] \over \abs{\P_B} - 1}
	\end{align*}
	As a result, there exists $\theta = {\abs{\P'} - 1 \over \abs{\P} - 1} \in (0,1)$ such that 
	\eqref{eq:ref_combination} holds.
\end{proof}
\begin{lemma}\label{lem:mi_split}
For any $\P:=\{C_1, \dots, C_k\} \in \Pi'$, we have
\begin{align}
I_{\P}(Z_V) =& I(Z_{C_1} \wedge \dots \wedge Z_{C_k}) \label{eq:mmi_representation} \\
=& { 1 \over k-1} \sum_{i=1}^{k-1} I(Z_{C_i} \wedge Z_{\cup_{j=i+1}^k C_j}) \label{eq:mi_mmi}
\end{align}
\end{lemma}
\begin{proof}
$I(Z_1 \wedge Z_2)$ is a kind of representation of mutual information.
And we use \eqref{eq:mmi_representation} to represent the multivariate mutual information.

Equation \eqref{eq:mi_mmi} is true for $k=2$. Suppose it holds for $k=m$. Then
let $B=\{C_m, C_{m+1}\}$ , $\P=\{C_1, \dots, C_{m+1}\}$
and use the result of lemma \ref{lem:ref_combination}, 
we have $\theta = \frac{m-1}{m}$
\begin{align*}
I(Z_{C_1} \wedge \dots \wedge Z_{C_{m+1}}) & = 
 \frac{m-1}{m} I(Z_{C_1} \wedge \dots \wedge Z_{C_m})
+ \frac{1}{m}  I(Z_{m} \wedge Z_{m+1}) \\
& = \frac{m-1}{m} \frac{1}{m-1}\sum_{i=1}^{m-1} I(Z_{C_i} \wedge Z_{\cup_{j=i+1}^m C_j})\\
& + \frac{1}{m}  I(Z_{m} \wedge Z_{m+1})\textrm{ by induction} \\
& = \frac{1}{m} \sum_{i=1}^{m} I(Z_{C_i} \wedge Z_{\cup_{j=i+1}^{m+1} C_j})
\end{align*}
\end{proof}

\begin{lemma}\label{lem:smallZB}
Suppose $I_{\P}(Z_V) = I(Z_V)$, then we have $I(Z_V) \geq I_{\P_B}(Z_B) \geq I(Z_B)$ if $\exists \P_B \subseteq \P$.
\end{lemma}
\begin{proof}
Let $\P' = \{B\} \cup \P\backslash \P_B$, then $\P = \P_B \cup \P'\backslash\{B\}$ , use lemma \ref{lem:ref_combination}, we have
\begin{equation}
I_{\P}(Z_V) = \theta I_{\P'}(Z_V) + (1-\theta) I_{\P_B}(Z_B)
\end{equation}
Since  $I_{P}(Z_V) = I(Z_V), I_{\P'}(Z_V)\geq I(Z_V) \Rightarrow I(Z_V) \geq I_{\P_B}(Z_B) \geq I(Z_B)$ 
\end{proof}
\begin{lemma}\label{lem:LargeIZV}
	If $I_{\P}(Z_V)=I(Z_V)$ and $I(Z_C) > I(Z_V)$, then $C\subseteq B$ for $B \in \P$.
\end{lemma}
\begin{proof}
	Suppose $C \not\subseteq B$, then there is a subset of at least two parts from $\P$ intersecting with $C$. Denote the collection of parts by $\P_U :=\{C_1, \dots, C_k\} \in \Pi'$ where 
	$k \geq 2, C_i \in \P $ such that $C \subseteq \cup_{i=1}^k C_i$ and $C_i \cap C \neq \emptyset$.
	Then we have
\begin{align*}
	I(Z_V) & \overbrace{\geq}_{(a)} I_{\P_U}(Z_U)  = I(Z_{C_1} \wedge \dots \wedge Z_{C_k})\\
	& \overbrace{=}_{\ref{lem:mi_split}} {1\over k-1}\sum_{i=1}^{k-1} I(Z_{C_i} \wedge Z_{\cup_{j=i+1}^k C_j}) \\
	& \overbrace{\geq}_{(b)} \frac{1}{k-1}\sum_{i=1}^{k-1} I(Z_{C_i \cap C} \wedge Z_{\cup_{j=i+1}^k C_j \cap C}) \\
	& = I(Z_{C_1 \cap C} \wedge \dots \wedge Z_{C_k \cap C}) \geq I(Z_C)
\end{align*}
$(a)$ is from lemma \ref{lem:smallZB} and $(b)$ is from the data processing inequality of mutual information (or the restriction of graph cut function).
\end{proof}

\begin{theorem}\label{thm:lattice}
If $I(Z_V) = I_{\P_1}(Z_V) = I_{\P_2}(Z_V)$, then $I_{\P_1 \wedge \P_2}(Z_V)=I(Z_V) $
\end{theorem}
\begin{proof}
Consider $C\in \P_1, \not\in \P_1\wedge\P_2$ and $C = \cup_{i=1}^k B_i, k>1$ where $B_i \in \P_1 \wedge \P_2$.
Consider $\P_C = \{B_1, \dots, B_k\}$. Each $B_i$ belongs to one of subsets of $\P_2$. Suppose $B_1, B_2 \subseteq D \in \P_2$. Then $B_1\cup B_2 \subseteq D$,
which contradicts the maximal property of $\P_1\wedge \P_2$. Therefore, we have $\P_D = \{D_1, D_2, \dots D_k\}\subseteq \P_2$ such that $B_i \subset D_i$.
From lemma \ref{lem:smallZB}, we have $I_{\P_D}(Z_V) \leq I(Z_V)$. Since $\abs{\P_D} = \abs{\P_C}$, From the definition we have $I_{\P_C}(Z_V) \leq I_{\P_D}(Z_V) \leq I(Z_V)$.
Then using lemma \ref{lem:ref_combination}, we have
\begin{equation*}
I_{\P_1 \wedge \P_2}(Z_V) \leq \theta I(Z_V) + (1- \theta)I(Z_V) = I(Z_V)
\end{equation*} 
Then we have $I_{\P_1 \wedge \P_2}(Z_V)=I(Z_V)$.
\end{proof}

By theorem \ref{thm:lattice}, the finest partition $I_{\P}(Z_V)=I(Z_V)$ is unique, which is the meet of all partitions. Therefore, the top-down approach is unambigous. We use $\P^*$ to denote the unique finest partition and it has the following property.
\begin{theorem}\label{thm:strict_larger_mi}
The fundamental partition $\P^*$ with the singletons removed is the set of all maximal subsets $B \subseteq V$ with strictly larger mutual information. 
That is, we have
$I(Z_B) > I(Z_V)$ for $B \in \P^*, \abs{B}>1$.
\end{theorem}
\begin{proof}
Suppose $I(Z_B)=I_{\P_B}(Z_B)$.
Let $\P' = \P_B \cup  \P^* \backslash \{B\}$.
Then by lemma \ref{lem:ref_combination}, there exists $\theta \in (0,1)$ such that 
\begin{equation}\label{eq:convexZ}
I_{\P'}(Z_V) = \theta I_{\P^*}(Z_V) + (1-\theta) I_{\P_B}(Z_B)
\end{equation}
Since $\P^*$ is finest, we have $I_{\P'}(Z_V) > I(Z_V)$, then from \eqref{eq:convexZ}, we have 
$I(Z_V) < \theta I(Z_V) + (1-\theta) I(Z_B) \Rightarrow I(Z_B) > I(Z_V)$.
\end{proof}
\begin{theorem}[Laminarity]\label{thm:laminarity}
\begin{equation}\label{eq:P}
I(Z_{C_1 \cup C_2}) \geq \min\{ I(Z_{C_1}), I(Z_{C_2})\}, \textrm{ for } C_1\cap C_2 \neq \emptyset
\end{equation}
\end{theorem}
\begin{proof}
	We proceed by contradiction. Let $V=C_1 \cup C_2$ and suppose $I(Z_V) < I(Z_{C_1}),
	I(Z_V) < I(Z_{C_2})$. There exists $\P$ such that $I_{\P}(Z_V) = I(Z_V)$.
	Then by lemma \ref{lem:LargeIZV} we can get $C_1 \subset B_1, C_2 \subset B_2, B_1, B_2 \in
	\P$. Since $C_1 \cap C_2 \neq \emptyset \Rightarrow B_1 = B_2$. Therefore $V=C_1 \cup C_2 \subset B_1$ which contradicts $\abs{\P}>1$.
\end{proof}
Theorem \ref{thm:laminarity} implies the maximal requirement for the bottom-up approach is umambigious. First we notice that if the subsets are non-overlapping, we can merge them respectively. Therefore, we only need to consider $C_1, C_2$ such that $I(Z_{C_1}) = I(Z_{C_2}) = \max_{B\subseteq V} I(Z_B)$ and $C_1\cap C_2 \neq \emptyset$. , $I(Z_{C_1\cup C_2}) \geq \max_{B\subseteq V} I(Z_B)$ therefore $C_1\cup C_2$ is also a solution to the maximization.
\begin{proof}[Proof of Proposition \ref{prop:ta}]
We make the connection of the two approaches with the original definition of info-clusters in \cite{RN1}.
\begin{equation}\label{eq:def_CgammaZV}
\mathcal{C}_{\gamma}(Z_V) = \textrm{maximal}\{ B \in V \vert \abs{B} > 1, I(Z_B) > \gamma \}
\end{equation}
The smallest critical value of $\mathcal{C}_{\gamma}(Z_V)$ is $I(Z_V)$ (Theorem 1 in \cite{RN1}) and for each $B \in \P$, since  our $\P$ is fundamental partition we have $I(Z_B) > I(Z_V)$ by theorem \ref{thm:strict_larger_mi} for $\abs{B}>1$. Take $\gamma = I(Z_V)$, by lemma \ref{lem:LargeIZV}, if $I(Z_C) > \gamma$ we have $C\subseteq B' \in \P$. Therefore $B$ is maximal and $B\in \mathcal{C}_{\gamma}(Z_V)$. The same argument can be done for subtrees by replacing $V$ by subroot. Therefore, all tree node sets by top-down approach belong to $\mathcal{C}_{\gamma}(Z_V)$. By Theorem 2.4 in \cite{RN1}, we conclude all clusters in $C_{\gamma}(Z_V)$ is a tree node set by top-down approach.

The largest critical value of $\mathcal{C}_{\gamma}(Z_V)$ is $\gamma_N = \max_{B\subseteq V} I(Z_B)$. Take $\gamma = \gamma_N - \epsilon$ where $\epsilon$ is sufficient small, since $I(Z_C) > \gamma$ and $C$ is maximal, we have $C \in \mathcal{C}_{\gamma}(Z_V)$. For each tree node set, since it is also maximal for some $\gamma$, all of them belong to $\mathcal{C}_{\gamma}(Z_V)$. By Theorem 4.2 in \cite{RN8}, we conclude all clusters in $C_{\gamma}(Z_V)$ is a tree node set by bottom-up approach.
\end{proof}
By the construction of the clustering tree, we can associate each tree node with a threshold value, which is multivariate similarity computed at that step.
\section{Experiment}
In this section, we describe the details of our experiment. The source code for info-clustering algorithm and experiments can be found at \href{https://github.com/zhaofeng-shu33/lab2c\_presentation\_template}{info-cluster}.
\subsection{Feature Clustering}
For the three groups of data: \textsf{Gaussian}, \textsf{Circle} and \textsf{Iris}. The first two have some intrinsic randomness. To make the result reproducible, we use fixed the random seed to generate the data. The three cirlces data is generated using polar coordinate:
\begin{align*}
r &= 0.1*i + 0.01*(2u-1), i = 1, 2, 3\\
\theta & = 2\pi v
\end{align*}
where random variable $u,v$ conform uniform distribution within $[0,1]$. Also, $u$ and $v$ are independent.

Different metric (or affinity) can affect the clustering accuracy significantly. Some clustering algorithm such as \textsf{k-means} can only use Euclidean metric and performs badly on non-convex dataset such as \textsf{Circle}. Therefore, we do not use such algorithm in comparision. 

Some algorithm needs to specify the number of cluster.  For example, spectral clustering is a powerful method but lacks the flexibility because it needs the dimension of subspace(number of cluster).  We'd like to compare methods which have the ability to select the best cluster without knowing the ground truth. Both \textsf{affinity propagation} and \textsf{agglomerative clustering}\footnote{We can use a quality function to select the best layer in the hierachical tree, for example modularity of Newman and Girvan.} meets the requirement. The agglomerative clustering algorithm we used is the standard one which merges two objects at each step. For \textsf{affinity propagation} and \textsf{agglomerative clustering} we use the implementation of \textsf{sklearn} and tune their parameters respectively.

For real world data like Glass dataset, some pre-processing is necessary, for our experiment we just scale each feature dimension to $[0, 1]$ for all algorithms.

\begin{table}
\centering
\InputIfFileExists{build/compare_3.tex}{}{}
\caption{ accuracy for different clustering algorithms }\label{tb:e1}
\end{table}

For \textsf{Gaussian} dataset, all three algorithms can get correct clustering result (\texttt{ari = 1.0}); for \textsf{Circle} dataset, affinity propagation method performs poorly even for nearest neighbor metric; for \textsf{Glass} dataset, info-clustering method has best performance compared with other method. To achieve this score, we use nearest neighbor affinity metric with 14 neighbors. We also tune the hyperparameters of the other two algorithms but neither of them can achieve the score of info-clustering.
\subsection{Community Detection}

\bibliographystyle{plain}
\bibliography{exportlist}
\end{document}