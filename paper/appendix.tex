\documentclass{article}
\usepackage{neurips_2019}
\usepackage{hyperref}       % hyperlinks
\usepackage{xr}
\externaldocument{nips_2019}
\input{extra_math.tex}
\title{Supplementary Material}
\begin{document}
\maketitle
\appendix
\section{Proofs of Propositions}
This section gives proof on propositions appeared in the paper. Before proceeding, we introduce some additional notations used in this section. $\Pi_B$ is the collection of partition on $B\subset V$. $E(C)=\{(i,j) | (i,j)\in E, i,j \in C\}$ is an edge subset of $E$. The meet of two partitions $\P_1 \wedge \P_2 = \mathrm{maximal}\{\P \in \Pi | \P \preceq \P_1, \P \preceq \P_2\}$. The notation $\wedge$ comes from lattice theory. For a partition $\P \in \Pi'$ and set $ A \subset V$, the partition expansion is defined as $\P[A] = \{B \in \P | B \cap A = \emptyset \} \cup \{ A \cup (\cup_{B \in \Pi, B \cap A \neq \emptyset} B) \}$.

\begin{lemma}\label{lem:ref_combination}
For any $B \in \P' \in \Pi'$ and $\P_B \in \Pi'_B$, let $\P = \P_B \cup \P' \backslash \{B\} $
be the refinement of $\P'$. Then we have
\begin{equation}\label{eq:ref_combination}
I_{\P}(Z_V) = \theta I_{\P'}(Z_V) + (1-\theta) I_{\P_B}(Z_B)
\end{equation}
\end{lemma}
\begin{proof}
	First we have $\abs{\P} = \abs{\P'} -1 + \abs{\P_B}$,
 and
	\begin{align*}
		I_{\P}(Z_V) & = { f[\P'] + f_B[\P_B] \over \abs{\P} - 1} \\
		I_{\P'}(Z_V) & = { f[\P'] \over \abs{\P'} - 1} \\
		I_{\P_B}(Z_B) & = { f_B[\P_B] \over \abs{\P_B} - 1}
	\end{align*}
	As a result, there exists $\theta = {\abs{\P'} - 1 \over \abs{\P} - 1} \in (0,1)$ such that 
	\eqref{eq:ref_combination} holds.
\end{proof}
Although Lemma \ref{lem:ref_combination} has the same form with Corollary 5.3 in \cite{secretKey}, the definition of $I_{\P}(Z_V)$ is different. 
%
\begin{lemma}\label{lem:smallZB}
Suppose $I_{\P}(Z_V) = I(Z_V)$, then we have $I(Z_V) \geq I_{\P_B}(Z_B) \geq I(Z_B)$ if $\exists \P_B \subseteq \P$.
\end{lemma}
\begin{proof}
Let $\P' = \{B\} \cup \P\backslash \P_B$, then $\P = \P_B \cup \P'\backslash\{B\}$ , use Lemma \ref{lem:ref_combination}, we have
\begin{equation}
I_{\P}(Z_V) = \theta I_{\P'}(Z_V) + (1-\theta) I_{\P_B}(Z_B)
\end{equation}
Since  $I_{P}(Z_V) = I(Z_V), I_{\P'}(Z_V)\geq I(Z_V) \Rightarrow I(Z_V) \geq I_{\P_B}(Z_B) \geq I(Z_B)$ 
\end{proof}

\begin{lemma}\label{lemma:lattice}
If $I(Z_V) = I_{\P_1}(Z_V) = I_{\P_2}(Z_V)$, then $I_{\P_1 \wedge \P_2}(Z_V)=I(Z_V) $
\end{lemma}
\begin{proof}
Consider $C\in \P_1, C \not\in \P_1\wedge\P_2$ and $C = \cup_{i=1}^k B_i, k>1$ where $B_i \in \P_1 \wedge \P_2$.
Consider $\P_C = \{B_1, \dots, B_k\}$. Each $B_i$ belongs to one of subsets of $\P_2$. Suppose $B_1, B_2 \subseteq D \in \P_2$. Then $B_1\cup B_2 \subseteq D$,
which contradicts the maximal property of $\P_1\wedge \P_2$. Therefore, we have $\P_D = \{D_1, D_2, \dots D_k\}\subseteq \P_2$ such that $B_i \subset D_i$.
From lemma \ref{lem:smallZB}, we have $I_{\P_D}(Z_V) \leq I(Z_V)$. Since $\abs{\P_D} = \abs{\P_C}=k$, From the definition of $I_{\P}(Z_V)$ we have $I_{\P_C}(Z_V) \leq I_{\P_D}(Z_V) \leq I(Z_V)$.
Then using Lemma \ref{lem:ref_combination}, let $\P' = \P_C \cup \P_1\backslash \{C\}$ we have
\begin{align*}
	I_{\P'}(Z_V) & = \theta I_{\P_1}(Z_V) + (1-\theta) I_{\P_C}(Z_C) \\
	& \leq \theta I(Z_V) + (1- \theta)I(Z_V) = I(Z_V)
\end{align*} 
Since $I(Z_V)$ is minimum value, then we have $I_{\P'}(Z_V)=I(Z_V)$.
$\P'\wedge \P_2 = \P_1 \wedge \P_2$ and we can repeat the above procedure using $\P_1 = \P'$. It terminates in finite steps since $\abs{V} > \abs{\P'} > \abs{\P_1}$.
\end{proof}

By Lemma \ref{lemma:lattice}, the finest partition $\P^*$ of the top-down approach is unique, which is the meet of all partitions satisfying $I_{\P}(Z_V)=I(Z_V)$. Therefore, the top-down approach mentioned in the paper is unambiguous.

\begin{lemma}[Corollary 5.1 in \cite{secretKey}]\label{lemma:laminarity}
\begin{equation}\label{eq:P}
I(Z_{C_1 \cup C_2}) \geq \min\{ I(Z_{C_1}), I(Z_{C_2})\}, \textrm{ for } C_1\cap C_2 \neq \emptyset
\end{equation}
\end{lemma}

Lemma \ref{lemma:laminarity} implies the maximal requirement for the bottom-up approach mentioned in the paper is unambiguous. For $I(Z_{C_1}) = I(Z_{C_2}) = \max_{B\subseteq V} I(Z_B)$, if $C_1 \cap C_2 = \emptyset$, we can merge them respectively; if $C_1\cap C_2 \neq \emptyset$, $I(Z_{C_1\cup C_2}) \geq \max_{B\subseteq V} I(Z_B)$. Therefore $I(Z_{C_1\cup C_2}) = \max_{B\subseteq V} I(Z_B)$.

\input{proposition_triangle_inequality.tex}
\begin{proof}

We use deduction to show  
\begin{equation}\label{eq:GF}
f[\P] \geq \frac{\abs{\P}-1}{\abs{V}-1} \sum_{(i,j) \in E} w_{ij}
\end{equation}
for any $\P \in \Pi$. Then by Theorem \ref{thm:ta} in the paper, Proposition \ref{prop:triangle} holds. 

Let $n=\abs{V}$. For $\abs{\P}=n$, the equality of equation \ref{eq:GF} holds. 

Suppose equation \eqref{eq:GF} holds for any $\abs{\P} \geq k+1(k\geq 2)$, and for $\P=\{C_1, \dots, C_k\}$, with $\abs{C_1} \leq \abs{C_r}$ for $r=2,\dots, k$. Let $\abs{C_1}=n_1(\geq 2), \P_{C_1} = \{\{i\}| i \in C_1\}, \P'=\P_{C_1} \cup \P \backslash \{C_1\}$. Then we have $\abs{\P'} = k+n_1-1$. Using equation \eqref{eq:GF} for $\P'$ we have
\begin{align}\label{eq:PPrelation}
f[\P'] & = \sum_{i\in C_1, k \not\in C_1} w_{ik} + \sum_{r=2}^k \sum_{(i,j) \in E(C_r)} w_{ij}\geq \frac{k+n_1 -2}{n-1}\sum_{(i,j) \in E} w_{ij} \\
f[\P] & =f[P'] - \sum_{(i,j) \in E(C_1)} w_{ij}
\end{align}
Applying triangle inequality $w_{ij} \leq w_{ik} + w_{jk}$ for given $k\not\in C_1$ and sum it over all $i, j \in C_1, i\neq j$, we have
$$
\sum_{(i,j) \in E(C_1)} w_{ij} \leq \sum_{(i,j) \in E(C_1)} (w_{ik} + w_{jk}) = (n_1-1)\sum_{i\in C_1} w_{ik}
$$
Summation over $k \not\in C_1$ we have 
$$
(n - n_1) \sum_{(i,j) \in E(C_1)} w_{ij} \leq (n_1 - 1) \sum_{i \in C_1, k \not\in C_1} w_{ik}
$$
Also
\begin{align*}
\sum_{(i,j) \in E} w_{ij}  & \geq \sum_{(i,j) \in E(C_1)} w_{ij} + \sum_{i\in C_1, k\not\in C_1} w_{ik} \\
(n_1 - 1)\sum_{(i,j) \in E} w_{ij}  & \geq (n_1 -1 )\sum_{(i,j) \in E(C_1)} w_{ij} + (n_1-1)\sum_{i\in C_1, k\not\in C_1} w_{ik} \\
& \geq (n_1 -1 )\sum_{(i,j) \in E(C_1)} w_{ij} + (n - n_1) \sum_{(i,j) \in E(C_1)} w_{ij}\\
& = (n-1) \sum_{(i,j) \in E(C_1)} w_{ij}
\end{align*}
We then have $\sum_{(i,j) \in E(C_1)} w_{ij} \leq \frac{n_1-1}{n-1}\sum_{(i,j) \in E} w_{ij}$. From equation \eqref{eq:PPrelation} we have 
$f[\P] \geq \frac{k-1}{n-1}\sum_{(i,j) \in E} w_{ij}$. That is, the result holds for $\abs{\P}=k$.
\end{proof}
\begin{corollary}\label{cor:complete}
	For a complete graph $G(V,E)$ with equal weight $w$ on each edge, we have $I(Z_{V})=\frac{nw}{2}$ where $n=\abs{V}$. And $I(Z_S) < I(Z_V)$ for any subset $S\subsetneq V$.
\end{corollary}
\begin{proof}
From Proposition \ref{prop:triangle}, $I(Z_{V})$ is achieved by $\P=\{\{1\},\dots,\{n\} \}$ and $I(Z_V) = I_{\P}(Z_V) = \frac{n(n-1)w/2}{n-1} = \frac{nw}{2} $. From bottom-up approach, $I(Z_V) = \max_{S\subset V} I(Z_S)$, therefore $I(Z_S) \leq I(Z_V)$. If $I(Z_S) = I(Z_V)$, consider $\P_S=\{i | i \in S\}$, then $I(Z_S) = I_{\P_S}(Z_S) = \frac{\abs{S}w}{2} < I(Z_V)$, a contradiction. Therefore, $I(Z_S) < I(Z_V)$ holds for any subset $S\subsetneq V$.
\end{proof}
\input{proposition_submodular_structure}
\begin{proof}
There are certain breakpoints $a_i - b_i$ which determines whether $y_i^{\lambda}$ takes constant values. We can get an increasing list of such breakpoints as $\tilde{\lambda}_1 < \dots < \tilde{\lambda}_n$ while $\abs{n}\leq \abs{V}$. If $b_i = +\infty, y^{\lambda}_i = a_i - \lambda$ and there is no turning point for this one.

We first show that if $ u < v $ within a breakpoint interval. That is $\tilde{\lambda}_j \leq u < v \leq \tilde{\lambda}_{j+1}$, then $T^v \subseteq T^v$. In such interval, let $S = \{i | y^{\lambda}_i = b_i\}$ and define $g_1(A) = \sum_{i\in A\backslash S} (a_i - \lambda), g_2(A)= \sum_{i \in A \cap S} b_i$. We can rewrite $\tilde{h}_{\lambda}(A) = f(A) - \lambda - g_1(A) - g_2(A) = g_3(A) + \lambda\abs{A}$ where $g_3(A)=f(A)-g_2(A)-\lambda(\abs{S}+1) - g_4(A)$, $g_4(A) = \sum_{i\in A\backslash S} a_i = \sum_{i \in A} a_i - \sum_{i \in A\cap S} a_i $. We can show that $g_3$ is a submodular function.
Let $A_1 = \argmin_{A} \tilde{h}_u(A), A_2 = \argmin_{A} \tilde{h}_v(A)$. Ignoring the constant part, we will have
\begin{align}
g_3(A_1) + u \abs{A_1}& \leq g_3(A) + u \abs{A}, \forall A \subseteq V \label{eq:fA1}\\
g_3(A_2) + v \abs{A_2}& \leq g_3(A) + v \abs{A}, \forall A \subseteq V \label{eq:fA2}
\end{align}
Using the submodular property of $g_3$ we have
\begin{align*}
g_3(A_1 \cap A_2) + g_3(A_1 \cup A_2)\leq  &g_3(A_1) + g_3(A_2) \\
\Rightarrow g_3(A_1 \cap A_2) + g_3(A_1 \cup A_2) + u \abs{A_1} + v\abs{A_2} 
\leq & g_3(A_1) + u \abs{A_1} + g_3(A_2) + v \abs{A_2}, \textrm{ by (\ref{eq:fA1}, \ref{eq:fA2})} \\
\Rightarrow g_3(A_1 \cap A_2) + g_3(A_1 \cup A_2) + u \abs{A_1} + v\abs{A_2} 
\leq &g_3(A) + u \abs{A}+ g_3(B) + v \abs{B}, \forall A, B \subseteq V \\
\textrm{ let } A =A_1 \cup A_2, B= A_1 \cap A_2 \Rightarrow u \abs{A_1} + v\abs{A_2} 
\leq & u \abs{A_1 \cup A_2}+ v \abs{A_1\cap A_2} \\
\Rightarrow u  \abs{A_2 \backslash A_1} \geq v \abs{A_2 \backslash A_1}
\end{align*}
Since $ u < v$, we have $\abs{A_2 \backslash A_1}=0$, that is $A_2 \subseteq A_1$, or $T^v \subseteq T^v$.

By the continuity of $\tilde{h}(\lambda)$, if $u<v$ belongs to different interval, we can use the breakpoints as the springboard. That is, if $\lambda_j < u \leq \lambda_{j+1} \dots \leq \lambda_{j+k} \leq v$,  we will have $ A_1 \supseteq A_{\lambda_{j+1}} \dots \supseteq A_{\lambda_{j+k}} \supseteq A_2$.
\end{proof}
\begin{lemma}\label{lemma:sub}
	If node set $S_1 \cap S_2 \neq \emptyset, S_1 \not\subseteq S_2$ and $I(Z_{S_1}) \geq I(Z_{S_2})$, then $S_2$ is not a cluster of $G$.
\end{lemma}
\begin{proof}
	From Lemma \ref{lemma:laminarity},
	$I(Z_{S_1\cup S_2}) \geq \min\{I(Z_{S_1}), I(Z_{S_2})\} = I(Z_{S_2})$. From bottom-up formulation,  $S_2$ cannot be a cluster of $G$.
\end{proof}
\input{proposition_reweight.tex}

\begin{proof}[Proof of Proposition \ref{prop:reweight}]
	We will show that if $U \neq S_1$ and $U \neq S_2$, $U$ cannot be a cluster of $G$.
	
For any $U \in S_1 \cup S_2 $ and $\abs{U} \leq n$, If $U \in S_1$ or $U \in S_2$, then by Corollary \ref{cor:complete}, $I(Z_U) < I(Z_{S_1})$.
If $U \cap S_1 \neq \emptyset$ or $U \cap S_2 \neq \emptyset$ holds, by Lemma \ref{lemma:sub}, $I(Z_{S_1})= I(Z_{S_2}) > I(Z_U) \Rightarrow U$ is not a cluster of $G$.

For $\abs{U} > n$ but $U \neq V = S_1 \cup S_2$. Suppose $S_2 \subset U$. Let $\P' = \{\{i\}| i\in U\}$, $I(Z_U) \leq I_{\P'}(Z_U) = \frac{\frac{n^2(n-1)}{2} + m_1 + \frac{k(k-1)n}{2}}{n+k-1}$ where $m_1 \leq nk$ and $k=\abs{U} - n < n$. 
We compare it to $I(Z_{S_1}) = \frac{n^2}{2}$ and we can show that $I(Z_{S_1}) - I_{\P'}(Z_U) \geq \frac{nk(n-1-k)}{2(n+k-1)}$. If $k<n-1$, $I(Z_{S_1})>I(Z_U)$, $U$ is not a cluster by Lemma \ref{lemma:sub}.
If $k=n-1$, and $I(Z_U) = I(Z_{S_1})$. Since $U\cap S_1 \neq \emptyset$, Using Lemma \ref{lemma:laminarity} we have $I(Z_V)= I(Z_{U\cup S_1}) \geq \min\{I(Z_U), I(Z_{S_1})\} = \frac{n^2}{2}$. Then we must have $I(Z_V) = I(Z_U)$ and $U$ cannot be a cluster in this case.

 In either case, $U$ is not a cluster of $G$ for $1<\abs{U}<2n, U \neq S_1, U\neq S_2$. Then $I(Z_V)$ can be computed by comparing $I_{\P}(Z_V) = \frac{m+n^2(n-1)}{2n-1}$ and $I_{\{S_1, S_2\}}(Z_V)=m$.

\end{proof}

\section{Algorithm Formulation}
In this section, we give the complete formulation of parametric Dilworth truncation algorithm. There are three parts in different level: \texttt{pdt} (Algorithm \ref{alg:pdt}) at the top level, \texttt{pmf} (Algorithm \ref{alg:pmfC}) at the middle level and \texttt{mf} (Maximum Flow) at the bottom level.

Algorithm \ref{alg:pdt} is an adaption and concrete implementation of Fig. 3 in \cite{RN4}.

\begin{algorithm}
	\caption{parametric Dilworth truncation $(\P, \mathcal{L})=\texttt{pdt}(G(V,E), c(e))$}\label{alg:pdt}
	\begin{algorithmic}[1]
		\REQUIRE a directed graph $G(V, E)$; edge cost function $c(e)$ for $e\in E$
		\ENSURE a nested family of partition $\P^{\lambda}$ and its corresponding turning point list $\mathcal{L}$
		\STATE initialize $y^{\lambda}_v = (0, +\infty)$ for $ v \in V$, $\P^{\lambda} = [\{\emptyset\}], \mathcal{L} = [+\infty]$
		\FOR{$j = 1$ to $\abs{V}$}
		\STATE  $(\mathcal{A}, L) = \texttt{pmf}(G(V,E), c(e), j, y^{\lambda})$
		\FOR{$u=1$ to $\abs{V}$}
		\IF{$u=j$}
		\STATE $y^{\lambda}_j = (f(\{j\}), +\infty)$
		\ELSE
		\STATE $ i = \max_i \{ u \in \mathcal{A}[i]\}$
		\IF{$i$ exists and $L[i] > a_u - b_u$}
		\STATE $y_u^{\lambda} = (a_u, a_u - L[i])$
		\ENDIF
		\ENDIF
		\ENDFOR
		\STATE $i, k = 0, \mathcal{Q} = \{\emptyset\}, \widetilde{\mathcal{\P}}^{\lambda} = [], \widetilde{\mathcal{L}} = []$, append $+\infty$ to $L$.
		\WHILE{$i<\texttt{length}(\mathcal{L})$ \texttt{ and } $k<\texttt{length}(L)$}
		\IF{$ \P^{\lambda}_i[\mathcal{A}_k] \neq \mathcal{Q}$}
		\STATE $\mathcal{Q} = \P^{\lambda}_i[\mathcal{A}_k]$
		\STATE append $\mathcal{Q}$ to $\widetilde{\mathcal{\P}}^{\lambda}$ and $\min\{\mathcal{L}[i], L[k]\}$
		to $\widetilde{\mathcal{L}}$
		\ELSE
		\STATE $\widetilde{\mathcal{L}}[-1] = \min\{\mathcal{L}[i], L[k]\}$ \footnotemark
		\ENDIF
		\STATE $(i, k) \leftarrow \begin{cases} (i+1, k) & \mathcal{L}[i] < L[k] \\  (i, k+1) & \mathcal{L}[i] > L[k]\\ (i+1, k+1) & \mathcal{L}[i] = L[k]\end{cases}$
		\ENDWHILE
		\STATE $(\P^{\lambda}, \mathcal{L}) \leftarrow (\widetilde{\mathcal{\P}}^{\lambda},  \widetilde{\mathcal{L}})$
		\ENDFOR
	\end{algorithmic}
\end{algorithm}
\footnotetext{$\widetilde{\mathcal{L}}[-1]$ is the last element of $\widetilde{\mathcal{L}}$}

We make some remarks about Algorithm \ref{alg:pmfC}, which is discussed in detail in the paper.
\begin{enumerate}
\item For different $\lambda$, we need to initialize the capacity of $\widetilde{G}$ by equation \eqref{eq:clambda}.
\begin{equation}\label{eq:clambda}
c^{\lambda}(i, j) = 
\begin{cases}
\max\{0, -\min\{a_i-\lambda, b_i\}\} &  i = s, j \neq t \\
w_{it} + \max\{0, \min\{a_i - \lambda, b_i\}\} & i\neq s, j = t\\
0 & i = s, j = t\\
w_{ij} & i \neq s, j \neq t
\end{cases}
\end{equation}
Since $\widetilde{G}$ for a specified $\lambda$ is used only once, we can modify the capacity directly on previous $\widetilde{G}$ without creating new graph data structure.

\item For \texttt{mf}, we use push-relabel maximum flow algorithm \cite{Goldberg}.

 $(T,g',\tilde{f})=$\texttt{mf}$(\widetilde{G}(\widetilde{V}, \widetilde{E}), c^{\tilde{\lambda}_2}(e), \tilde{f})$. The first parameter of \texttt{mf} is graph topology structure; the second parameter is edge capacity map and the third parameter is a valid flow to be used as the initial flow of the push-relabel algorithm. The third parameter is optional. \texttt{mf} returns three values: $T$ is minimum cut of target side($t\in T$); $g'$ is the maximum flow value; $\tilde{f}$ is the maximum flow map.

For $\texttt{mf}$, we can choose different active node selecting scheme. For example, highest label selection rule is known to be fastest in practice.

\item For line \ref{findLambda} in  Algorithm \ref{alg:pmfC}, we can simplify the equation as
\begin{equation}\label{eq:solveL}
	\sum_{v\in T_l \backslash T_r} y^{\lambda}_v  = f(T_l)-f(T_r)
\end{equation}
\begin{proof}
	\begin{align*}
	c^{\lambda}(T_l) -
	c^{\lambda}(T_r)  =& f(T_l) + \sum_{j \in T_l} \max\{0, -y^{\lambda}_j\} + \sum_{j \in V\backslash T_l} \max\{0, y^{\lambda}_j\} \\
	-& [f(T_r) + \sum_{j \in T_r} \max\{0, -y^{\lambda}_j\} + \sum_{j \in V\backslash T_r} \max\{0, y^{\lambda}_j\}] \\
	= & f(T_l) - f(T_r) + \sum_{T_l \backslash T_r} (\max\{0, -y^{\lambda}_j\} - \max\{0, y^{\lambda}_j\} )
	\end{align*}
	From line \ref{findLambda} equal to zero, we have equation \eqref{eq:solveL}.
\end{proof}
The right-hand side of equation \eqref{eq:solveL} is constant while the left-hand side is a piecewise linear decreasing function. Since $T_l \supseteq T_r$, the left-hand side summation is not empty. We can show that equation in line \ref{findLambda} always has a solution for Algorithm \ref{alg:pmfC}.
\begin{lemma}
	Within function \texttt{split} in Algorithm \ref{alg:pmfC}, $c^{\lambda}(T_l) = c^{\lambda}(T_r)$ is a equation about $\lambda$. It has a solution.
\end{lemma}
\begin{proof}
	we only need to show for the two parameters of function \texttt{split}, the following property always holds.
\begin{equation}\label{eq:solutionProperty}
\exists \lambda_1, \lambda_3 \,s.t.\, c^{\lambda_1}(T_l) \leq c^{\lambda_1}(T_r), c^{\lambda_3}(T_l) \geq c^{\lambda_3}(T_r)
\end{equation}
Then by the continuity of $c^{\lambda}(T)$, there exists $\tilde{\lambda} \in [\lambda_1, \lambda_3]$ such that line \eqref{findLambda} holds.

For the initial computation in Algorithm \ref{alg:pmfC},
	$T_l = T_0, T_r = T_1$. $\lambda_1 = -\epsilon$ by line \ref{alg:T0property}. On the other hand, for a sufficient large $\lambda=\lambda', T_1$ is minimum value of $c^{\lambda}(T)$, therefore $\lambda_3 = \lambda'$. 

Within \texttt{split}$(T_l, T_r)$, suppose equation \eqref{eq:solutionProperty} holds with $(\lambda_1, \lambda_3)$, then 
$c^{\lambda_1}(T_l) \leq c^{\lambda_1}(T')$ and $c^{\tilde{\lambda}_2}(T_l) = h' > c^{\tilde{\lambda}_2}(T')$ by line \ref{alg:lineLarge}. That is, equation \eqref{eq:solutionProperty} holds for \texttt{split}$(T_l, T')$ with $(\lambda_1, \tilde{\lambda}_2)$. Similarly, from $c^{\lambda_3}(T') \geq c^{\lambda_3}(T_r)$ and $c^{\tilde{\lambda}_2}(T') < h' = c^{\tilde{\lambda}_2}(T_r)$ we can get  equation \eqref{eq:solutionProperty} holds for \texttt{split}$(T_l, T')$ with $(\tilde{\lambda}_2, \lambda_3)$.

By deduction, we conclude equation \eqref{eq:solutionProperty} holds and line \eqref{findLambda} always has solution.
\end{proof}
\item In line \ref{alg:flowModify} of Algorithm \ref{alg:pmfC}, we use flow $\tilde{f}$ coming from line \ref{alg:T0property}. The modification is quite simple:
\begin{align}
\tilde{f}'(j,t) &= \min\{\tilde{f}(j,t), c^{\lambda}(j,t) \} \\
\tilde{f}'(i,j) &= \tilde{f}(i,j), j \neq t, (i,j) \in \widetilde{E}
\end{align}
This is because $\tilde{\lambda}_2 > -\epsilon$, by equation \eqref{eq:clambda}, $c^{\lambda}(s,j)$ increases and $c^{\lambda} (j,t)$ decreases. By restricting $\tilde{f}'(j,t)$ not to exceed the capacity constraint can we get a valid preflow.
\end{enumerate}

\begin{algorithm}
	\caption{parametric maximum flow $(\mathcal{A}, L) = \texttt{pmf}(G(V,E), w(e), t, y^{\lambda})$}\label{alg:pmfC}
	\begin{algorithmic}[1]
		\REQUIRE a directed graph $G(V, E)$;  edge cost function $w(e)$ for $e \in E$; $t \in V$; tuples $y^{\lambda}_i = (a_i, b_i)$ for $i \in V$.
		\ENSURE a nested family of minimal value to $c^{\lambda}(T)$, as a reversely ordered set list $\mathcal{A}$ and its corresponding turning value $\lambda$ as an ordered value list $L$.
		\STATE Let $L$ be the turning point list, initialized as empty array.
		\STATE Construct $\widetilde{G}$: let $\widetilde{V}=V\cup\{s\}, \widetilde{E} = E\cup \{(s,j),(j,t)|j \in V, j\neq t\}$
		\STATE Initialize $c^{-\epsilon}(e)$ for $e \in \widetilde{E}$ according to equation \eqref{eq:clambda}.
		\STATE $ (T_0, \_, \tilde{f})  = \mathtt{mf}(\widetilde{G}(\widetilde{V}, \widetilde{E}), c^{-\epsilon}(e))$. Also let $T_1 = \{t\}$. \label{alg:T0property}
		\STATE $\mathcal{A}$ contains the decreasing set list, initialized as $[T_0, T_1]$;
		\STATE \texttt{split}$( T_0, T_1)$
		\FUNCTION{\texttt{split}$(T_l, T_r)$}
		\STATE Let $\tilde{\lambda}_2$ be the value of $\lambda$ such that $c^{\lambda}(  T_l) = 
		c^{\lambda}(T_r)$ \label{findLambda}
		\STATE $h' = c^{\tilde{\lambda}_2}(T_r) $
		\STATE initialize $\tilde{f}'$ from $\tilde{f}$ such that it is a valid preflow for $(\widetilde{G}(\widetilde{V}, \widetilde{E}), c^{\tilde{\lambda}_2}(e))$ \label{alg:flowModify}

		\STATE Initialize $c^{\tilde{\lambda}_2}(e)$ for $e \in \widetilde{E}$ according to equation \eqref{eq:clambda}.
		\STATE $(T', g', \tilde{f}') = \mathtt{mf}(\widetilde{G}(\widetilde{V}, \widetilde{E}), c^{\tilde{\lambda}_2}(e), \tilde{f}')$.
		\IF{$h' > g'$} \label{alg:lineLarge}
		\STATE insert $T'$ to $\mathcal{A}$
		\STATE \texttt{split}$(T_l, T')$
		\STATE \texttt{split}$(T', T_r)$
		\ELSE
		\STATE insert $\tilde{\lambda}_2$ to $L$
		\ENDIF
		\ENDFUNCTION
	\end{algorithmic}
\end{algorithm}


\section{Experiment Explanation}
In this section, we describe the details of our two experiments. 

\subsection{Data Clustering}
For the three groups of data: \textsf{Gaussian}, \textsf{Circle} and \textsf{Glass}, the first two have some intrinsic randomness in their generation. To make the result reproducible, we fix the random seed of our program. 

\textsf{Circle} dataset is generated using polar coordinate:
\begin{align*}
r &= 0.1*i + 0.01*(2u-1), i = 1, 2, 3\\
\theta & = 2\pi v
\end{align*}
where random variable $u,v$ conform uniform distribution within $[0,1]$. Also, $u$ and $v$ are independent.

Different metric can affect the clustering accuracy significantly. For this experiment, we try rbf kernel, Laplacian and k-nearest neighbors.  

The agglomerative clustering algorithm we used is the standard one which merges two objects at each step and we can choose different linkage type for this algorithm. For \textsf{affinity propagation} and \textsf{agglomerative clustering} we use the implementation of \textsf{scikit-learn} \cite{scikit-learn} and tune their parameters respectively.

For real world data like Glass dataset, some pre-processing is necessary, for our experiment we just scale each feature dimension to $[0, 1]$ for all algorithms.

For \textsf{Gaussian} dataset, all three algorithms can get correct clustering result (\texttt{ari = 1.0}); for \textsf{Circle} dataset, intuitively only k-nearest neighbor affinity works, but propagation method performs poorly even for this metric; for \textsf{Glass} dataset, info-clustering method has best performance compared with other method. To achieve this score, we use nearest neighbor metric with 14 neighbors. For agglomerative clustering, the tuned hyper-parameters is to use ward linkage with prespecified number of cluster equal to 7 ; for affinity propagation, we use Euclidean metric, preference equal to -50 and damping factor equal to 0.8. Neither of them can achieve the score of info-clustering.


\subsection{Community Detection}
There are many other community detection algorithms, but most of them can only get a flatten structure of the community. For hierarchical community detection method, some of them can only generate binary trees. For some graph models, binary tree tends to have large tree depth and is not suitable. To compare info-clustering algorithm fairly, we use two hierarchical community detection algorithms (GN and BHCD) which allow non-binary tree structure.
 
There is some randomness in the generation of the two-level community for given $(z_{\mathrm{in}_1}, z_{\mathrm{in}_2}, z_{\mathrm{out}})$. Therefore we average the distance metric by running 20 times of experiment for the same $(z_{\mathrm{in}_1}, z_{\mathrm{in}_2}, z_{\mathrm{out}})$ and given algorithm. For BHCD method, we use symmetric network configuration with $\gamma = 0.4, \lambda=0.85, \delta=0.2$ and 50 times of restart, which is tuned by grid-search method. For info-clustering and GN method, there are no hyper-parameters to tune. Notice that we assign weight to the graph by 
$w_{ij} = 1 + \abs{\{k | (i,k),(j,k) \in E \}}$
for info-clustering method. GN method does not require the weight value of the graph and we simply use the graph topology information to do the clustering task.

The plotting of this experiment shows that BHCD method produces RF distance near or equal to 1(which is maximal distance allowed). It reflects that BHCD is unsuitable for this kind of problem. For GN method, it is nearly unchanged for graph parameter change, except for $z_{\mathrm{in}_1}$. When $z_{\mathrm{in}_1}$ increases from 13 to 15 while $z_{\mathrm{in}_2}=3.0, z_o=1.0$, the distance decreases significantly. However, neither of these two approaches can outperform info-clustering. Plotting shows that info-clustering has the smallest distance for all shown parameters except for $(z_{\mathrm{in}_1}=10, z_{\mathrm{in}_2}=3.0, z_{\mathrm{out}}=1.0)$, which is a tiny nuisance. 

For this experiment, we make courtesy to ETE Toolkit \cite{ete3} and NetworkX library \cite{SciPyProceedings_11}, which make the experiment easy to implement.

\bibliographystyle{plain}
\bibliography{exportlist}
\end{document}