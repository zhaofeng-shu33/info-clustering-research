\documentclass{article}
\usepackage[preprint]{neurips_2019}
\usepackage{hyperref}       % hyperlinks
\input{extra_math.tex}
\title{Supplementary Material}
\begin{document}
\maketitle
\appendix
\section{Proofs of Propositions}
This section gives proof details about propositions in the paper.
\input{proposition_top_bottom.tex}
Before processing, we give some properties of multivariate similarity $I(Z_V)$, which will be used in the proof of Proposition \ref{prop:ta}.
\begin{lemma}\label{lem:ref_combination}
For any $B \in \P' \in \Pi'$ and $\P_B \in \Pi'_B$, let $\P = \P_B \cup \P' \backslash \{B\} $
be the refinement of $\P'$. Then we have
\begin{equation}\label{eq:ref_combination}
I_{\P}(Z_V) = \theta I_{\P'}(Z_V) + (1-\theta) I_{\P_B}(Z_B)
\end{equation}
\end{lemma}
\begin{proof}
	First we have $\abs{\P} = \abs{\P'} -1 + \abs{\P_B}$,
	Since
	\begin{align*}
		I_{\P}(Z_V) & = { f[\P'] + f_B[\P_B] \over \abs{\P} - 1} \\
		I_{\P'}(Z_V) & = { f[\P'] \over \abs{\P'} - 1} \\
		I_{\P_B}(Z_B) & = { f_B[\P_B] \over \abs{\P_B} - 1}
	\end{align*}
	As a result, there exists $\theta = {\abs{\P'} - 1 \over \abs{\P} - 1} \in (0,1)$ such that 
	\eqref{eq:ref_combination} holds.
\end{proof}
\begin{lemma}\label{lem:mi_split}
For any $\P:=\{C_1, \dots, C_k\} \in \Pi'$, we have
\begin{align}
I_{\P}(Z_V) =& I(Z_{C_1} \wedge \dots \wedge Z_{C_k}) \label{eq:mmi_representation} \\
=& { 1 \over k-1} \sum_{i=1}^{k-1} I(Z_{C_i} \wedge Z_{\cup_{j=i+1}^k C_j}) \label{eq:mi_mmi}
\end{align}
\end{lemma}
\begin{proof}
$I(Z_1 \wedge Z_2)$ is a kind of representation of mutual information.
And we use \eqref{eq:mmi_representation} to represent the multivariate mutual information.

Equation \eqref{eq:mi_mmi} is true for $k=2$. Suppose it holds for $k=m$. Then
let $B=\{C_m, C_{m+1}\}$ , $\P=\{C_1, \dots, C_{m+1}\}$
and use the result of lemma \ref{lem:ref_combination}, 
we have $\theta = \frac{m-1}{m}$
\begin{align*}
I(Z_{C_1} \wedge \dots \wedge Z_{C_{m+1}}) & = 
 \frac{m-1}{m} I(Z_{C_1} \wedge \dots \wedge Z_{C_m})
+ \frac{1}{m}  I(Z_{m} \wedge Z_{m+1}) \\
& = \frac{m-1}{m} \frac{1}{m-1}\sum_{i=1}^{m-1} I(Z_{C_i} \wedge Z_{\cup_{j=i+1}^m C_j})\\
& + \frac{1}{m}  I(Z_{m} \wedge Z_{m+1})\textrm{ by induction} \\
& = \frac{1}{m} \sum_{i=1}^{m} I(Z_{C_i} \wedge Z_{\cup_{j=i+1}^{m+1} C_j})
\end{align*}
\end{proof}

\begin{lemma}\label{lem:smallZB}
Suppose $I_{\P}(Z_V) = I(Z_V)$, then we have $I(Z_V) \geq I_{\P_B}(Z_B) \geq I(Z_B)$ if $\exists \P_B \subseteq \P$.
\end{lemma}
\begin{proof}
Let $\P' = \{B\} \cup \P\backslash \P_B$, then $\P = \P_B \cup \P'\backslash\{B\}$ , use lemma \ref{lem:ref_combination}, we have
\begin{equation}
I_{\P}(Z_V) = \theta I_{\P'}(Z_V) + (1-\theta) I_{\P_B}(Z_B)
\end{equation}
Since  $I_{P}(Z_V) = I(Z_V), I_{\P'}(Z_V)\geq I(Z_V) \Rightarrow I(Z_V) \geq I_{\P_B}(Z_B) \geq I(Z_B)$ 
\end{proof}
\begin{lemma}\label{lem:LargeIZV}
	If $I_{\P}(Z_V)=I(Z_V)$ and $I(Z_C) > I(Z_V)$, then $C\subseteq B$ for $B \in \P$.
\end{lemma}
\begin{proof}
	Suppose $C \not\subseteq B$, then there is a subset of at least two parts from $\P$ intersecting with $C$. Denote the collection of parts by $\P_U :=\{C_1, \dots, C_k\} \in \Pi'$ where 
	$k \geq 2, C_i \in \P $ such that $C \subseteq \cup_{i=1}^k C_i$ and $C_i \cap C \neq \emptyset$.
	Then we have
\begin{align*}
	I(Z_V) & \overbrace{\geq}_{(a)} I_{\P_U}(Z_U)  = I(Z_{C_1} \wedge \dots \wedge Z_{C_k})\\
	& \overbrace{=}_{\ref{lem:mi_split}} {1\over k-1}\sum_{i=1}^{k-1} I(Z_{C_i} \wedge Z_{\cup_{j=i+1}^k C_j}) \\
	& \overbrace{\geq}_{(b)} \frac{1}{k-1}\sum_{i=1}^{k-1} I(Z_{C_i \cap C} \wedge Z_{\cup_{j=i+1}^k C_j \cap C}) \\
	& = I(Z_{C_1 \cap C} \wedge \dots \wedge Z_{C_k \cap C}) \geq I(Z_C)
\end{align*}
$(a)$ is from lemma \ref{lem:smallZB} and $(b)$ is from the data processing inequality of mutual information (or the restriction of graph cut function).
\end{proof}

\begin{lemma}\label{lemma:lattice}
If $I(Z_V) = I_{\P_1}(Z_V) = I_{\P_2}(Z_V)$, then $I_{\P_1 \wedge \P_2}(Z_V)=I(Z_V) $
\end{lemma}
\begin{proof}
Consider $C\in \P_1, \not\in \P_1\wedge\P_2$ and $C = \cup_{i=1}^k B_i, k>1$ where $B_i \in \P_1 \wedge \P_2$.
Consider $\P_C = \{B_1, \dots, B_k\}$. Each $B_i$ belongs to one of subsets of $\P_2$. Suppose $B_1, B_2 \subseteq D \in \P_2$. Then $B_1\cup B_2 \subseteq D$,
which contradicts the maximal property of $\P_1\wedge \P_2$. Therefore, we have $\P_D = \{D_1, D_2, \dots D_k\}\subseteq \P_2$ such that $B_i \subset D_i$.
From lemma \ref{lem:smallZB}, we have $I_{\P_D}(Z_V) \leq I(Z_V)$. Since $\abs{\P_D} = \abs{\P_C}$, From the definition we have $I_{\P_C}(Z_V) \leq I_{\P_D}(Z_V) \leq I(Z_V)$.
Then using lemma \ref{lem:ref_combination}, we have
\begin{equation*}
I_{\P_1 \wedge \P_2}(Z_V) \leq \theta I(Z_V) + (1- \theta)I(Z_V) = I(Z_V)
\end{equation*} 
Then we have $I_{\P_1 \wedge \P_2}(Z_V)=I(Z_V)$.
\end{proof}

By Lemma \ref{lemma:lattice}, the finest partition $I_{\P}(Z_V)=I(Z_V)$ is unique, which is the meet of all partitions. Therefore, the top-down approach is unambiguous. We use $\P^*$ to denote the unique finest partition and it has the following property.
\begin{lemma}\label{lemma:strict_larger_mi}
The fundamental partition $\P^*$ with the singletons removed is the set of all maximal subsets $B \subseteq V$ with strictly larger mutual information. 
That is, we have
$I(Z_B) > I(Z_V)$ for $B \in \P^*, \abs{B}>1$.
\end{lemma}
\begin{proof}
Suppose $I(Z_B)=I_{\P_B}(Z_B)$.
Let $\P' = \P_B \cup  \P^* \backslash \{B\}$.
Then by lemma \ref{lem:ref_combination}, there exists $\theta \in (0,1)$ such that 
\begin{equation}\label{eq:convexZ}
I_{\P'}(Z_V) = \theta I_{\P^*}(Z_V) + (1-\theta) I_{\P_B}(Z_B)
\end{equation}
Since $\P^*$ is finest, we have $I_{\P'}(Z_V) > I(Z_V)$, then from \eqref{eq:convexZ}, we have 
$I(Z_V) < \theta I(Z_V) + (1-\theta) I(Z_B) \Rightarrow I(Z_B) > I(Z_V)$.
\end{proof}
\begin{lemma}[Laminarity]\label{lemma:laminarity}
\begin{equation}\label{eq:P}
I(Z_{C_1 \cup C_2}) \geq \min\{ I(Z_{C_1}), I(Z_{C_2})\}, \textrm{ for } C_1\cap C_2 \neq \emptyset
\end{equation}
\end{lemma}
\begin{proof}
	We proceed by contradiction. Let $V=C_1 \cup C_2$ and suppose $I(Z_V) < I(Z_{C_1}),
	I(Z_V) < I(Z_{C_2})$. There exists $\P$ such that $I_{\P}(Z_V) = I(Z_V)$.
	Then by lemma \ref{lem:LargeIZV} we can get $C_1 \subset B_1, C_2 \subset B_2, B_1, B_2 \in
	\P$. Since $C_1 \cap C_2 \neq \emptyset \Rightarrow B_1 = B_2$. Therefore $V=C_1 \cup C_2 \subset B_1$ which contradicts $\abs{\P}>1$.
\end{proof}
Theorem \ref{lemma:laminarity} implies the maximal requirement for the bottom-up approach is umambigious. First we notice that if the subsets are non-overlapping, we can merge them respectively. Therefore, we only need to consider $C_1, C_2$ such that $I(Z_{C_1}) = I(Z_{C_2}) = \max_{B\subseteq V} I(Z_B)$ and $C_1\cap C_2 \neq \emptyset$. , $I(Z_{C_1\cup C_2}) \geq \max_{B\subseteq V} I(Z_B)$ therefore $C_1\cup C_2$ is also a solution to the maximization.
\begin{proof}[Proof of Proposition \ref{prop:ta}]
We make the connection of the two approaches with the original definition of info-clusters in \cite{RN1}.
\begin{equation}\label{eq:def_CgammaZV}
\mathcal{C}_{\gamma}(Z_V) = \textrm{maximal}\{ B \in V \vert \abs{B} > 1, I(Z_B) > \gamma \}
\end{equation}
The smallest critical value of $\mathcal{C}_{\gamma}(Z_V)$ is $I(Z_V)$ (Theorem 1 in \cite{RN1}) and for each $B \in \P$, since  our $\P$ is fundamental partition we have $I(Z_B) > I(Z_V)$ by Lemma \ref{thm:strict_larger_mi} for $\abs{B}>1$. Take $\gamma = I(Z_V)$, by lemma \ref{lem:LargeIZV}, if $I(Z_C) > \gamma$ we have $C\subseteq B' \in \P$. Therefore $B$ is maximal and $B\in \mathcal{C}_{\gamma}(Z_V)$. The same argument can be done for subtrees by replacing $V$ by subroot. Therefore, all tree node sets by top-down approach belong to $\mathcal{C}_{\gamma}(Z_V)$. By Theorem 2.4 in \cite{RN1}, we conclude all clusters in $C_{\gamma}(Z_V)$ is a tree node set by top-down approach.

The largest critical value of $\mathcal{C}_{\gamma}(Z_V)$ is $\gamma_N = \max_{B\subseteq V} I(Z_B)$. Take $\gamma = \gamma_N - \epsilon$ where $\epsilon$ is sufficient small, since $I(Z_C) > \gamma$ and $C$ is maximal, we have $C \in \mathcal{C}_{\gamma}(Z_V)$. For each tree node set, since it is also maximal for some $\gamma$, all of them belong to $\mathcal{C}_{\gamma}(Z_V)$. By Theorem 4.2 in \cite{RN8}, we conclude all clusters in $C_{\gamma}(Z_V)$ is a tree node set by bottom-up approach.
\end{proof}
By the construction of the clustering tree, we can associate each tree node with a threshold value, which is multivariate similarity computed at that step.

\input{proposition_triangle_inequality.tex}
\begin{proof}


We use deduction to show  
$\frac{f[\P]}{\abs{\P}-1} \geq \frac{\sum w_{ij, (i,j) \in E}}{\abs{V}-1}$ for any $\P \in \Pi$. For $\abs{\P}=n$, the result holds. Suppose the result holds for any $\abs{\P} \geq k+1(k\geq 2)$, and for $\abs{\P}=k$, we choose the smallest part $C_1$ with $\abs{C_1}=n_1(\geq 2)$ and divide it into singletons. Then we have $k+n_1-1$ part. Using the assumption we have
$$
\mathrm{LHS} = \sum_{i\in C_1, i\neq k} w_{ik} + \sum_{r=2}^k \sum_{i \in C_r, j \not\in C_r} w_{ij}\geq \frac{2(k+n_1 -1)\sum w_{ij}}{n-1}
$$

Then for the given $\abs{\P}=k$, $2f[\P] =\mathrm{LHS} - \sum_{i,j\in C_1, i\neq j} w_{ij}$.
Applying triangle inequality $w_{ij} \leq w_{ik} + w_{jk}$ for given $k\not\in C_1$ and sum it over all $i, j \in C_1, i\neq j$, we have
$$
\sum_{i,j \in C_1, i\neq j} w_{ij} \leq 2(n_1-1)\sum_{i\in C_1} w_{ik}
$$
Since
\begin{align*}
\sum w_{ij} &= \sum_{i,j \in C_1, i\neq j} w_{ij} + \sum_{i\in C_1, k\neq C_1} w_{ik} \\
(n - n_1) \sum_{i,j \in C_1, i\neq j} w_{ij}& \leq 2(n_1 - 1) \sum_{i \in C_1, k \not\in C_1} w_{ik} \\
\end{align*}
We have $\sum_{i,j \in C_1, i\neq j} w_{ij} \leq \frac{2(n_1-1) \sum w_{ij}}{n-1}$. Therefore, 
$2f[\P] \geq \frac{2k \sum w_{ij}}{n-1}$. That is, the result holds for $\abs{\P}=k$.
\end{proof}
\begin{corollary}\label{cor:complete}
	For a complete graph $G(V,E)$ with equal weight $w$ on each edge, we have $I(Z_{V})=\frac{nw}{2}$ where $n=\abs{V}$. And $I(Z_S) < I(Z_V)$ for any subset $S\subsetneq V$.
\end{corollary}
\begin{proof}
From Proposition \ref{prop:triangle}, $I(Z_{V})$ is achieved by $\P=\{\{1\},\dots,\{n\} \}$ and $I_{\P}(Z_V) = \frac{n(n-1)w/2}{n-1} = \frac{nw}{2} $. From bottom-up approach of Proposition \ref{prop:ta}, $I(Z_V) = \max_{S\subset V} I(Z_S)$ therefore $I(Z_S) < I(Z_V)$ holds for any subset $S\subsetneq V$.
\end{proof}
\input{proposition_submodular_structure}
\begin{proof}
There are certain breakpoints $a_i - b_i$ which determines whether $y_i^{\lambda}$ takes constant values. We can get an increasing list of such breakpoints as $\tilde{\lambda}_1 < \dots < \tilde{\lambda}_n$ while $\abs{n}\leq \abs{V}$. If $b_i = +\infty, y^{\lambda}_i = a_i - \lambda$ and there is no turning point for this one.

We first compare $ u < v $ within a breakpoint interval. That is $\tilde{\lambda}_j \leq u < v \leq \tilde{\lambda}_{j+1}$. In such interval, let $S = \{i | y^{\lambda}_i = b_i\}$ and define $g_1(A) = \sum_{i\in A\backslash S} (a_i - \lambda), g_2(A)= \sum_{i \in A \cap S} b_i$. We can rewrite $\tilde{h}_{\lambda}(A) = f(A) - \lambda - g_1(A) - g_2(A) = g_3(A) + \lambda\abs{A}$ where $g_3(A)=f(A)-g_2(A)-\lambda(\abs{S}+1) - g_4(A)$, $g_4(A) = \sum_{i\in A\backslash S} a_i = \sum_{i \in A} a_i - \sum_{i \in A\cap S} a_i $. We can show that $g_3$ is a submodular function.
Let $A_1 = \argmin_{A} \tilde{h}_u(A), A_2 = \argmin_{A} \tilde{h}_v(A)$. Ignoring the constant part, we will have
\begin{align}
g_3(A_1) + u \abs{A_1}& \leq g_3(A) + u \abs{A}, \forall A \subseteq V \label{eq:fA1}\\
g_3(A_2) + v \abs{A_2}& \leq g_3(A) + v \abs{A}, \forall A \subseteq V \label{eq:fA2}
\end{align}
Using the submodular property of $g_3$ we have
\begin{align*}
g_3(A_1 \cap A_2) + g_3(A_1 \cup A_2)\leq  &g_3(A_1) + g_3(A_2) \\
\Rightarrow g_3(A_1 \cap A_2) + g_3(A_1 \cup A_2) + u \abs{A_1} + v\abs{A_2} 
\leq & g_3(A_1) + u \abs{A_1} + g_3(A_2) + v \abs{A_2}, \textrm{ by (\ref{eq:fA1}, \ref{eq:fA2})} \\
\Rightarrow g_3(A_1 \cap A_2) + g_3(A_1 \cup A_2) + u \abs{A_1} + v\abs{A_2} 
\leq &g_3(A) + u \abs{A}+ g_3(B) + v \abs{B}, \forall A, B \subseteq V \\
\textrm{ let } A =A_1 \cup A_2, B= A_1 \cap A_2 \Rightarrow u \abs{A_1} + v\abs{A_2} 
\leq & u \abs{A_1 \cup A_2}+ v \abs{A_1\cap A_2} \\
\Rightarrow u  \abs{A_2 \backslash A_1} \geq v \abs{A_2 \backslash A_1}
\end{align*}
Since $ u < v$, we have $\abs{A_2 \backslash A_1}=0$, that is $A_2 \subseteq A_1$.

By the continuity of $\tilde{h}(\lambda)$, if $u<v$ belongs to different interval, we can use the breakpoints as the springboard. That is, if $\lambda_j < u \leq \lambda_{j+1} \dots \leq \lambda_{j+k} \leq v$,  we will have $ A_1 \supseteq A_{\lambda_{j+1}} \dots \supseteq A_{\lambda_{j+k}} \supseteq A_2$.
\end{proof}

\input{proposition_reweight.tex}
\begin{lemma}\label{lemma:sub}
	If node set $S_1 \cap S_2 \neq \emptyset$ and $I(Z_{S_1}) > I(Z_{S_2})$, then $S_2$ is not a cluster of $G$.
\end{lemma}
\begin{proof}
	From Lemma \ref{lemma:laminarity},
	$I(Z_{S_1\cup S_2}) > \min\{I(Z_{S_1}), I(Z_{S_2})\} = I(Z_{S_2})$. From bottom-up formulation in Proposition \ref{prop:ta},  $S_2$ cannot be a cluster of $G$.
\end{proof}
\begin{proof}[Proof of Proposition \ref{prop:reweight}]
	We will show that if $U \neq S_1$ and $U \neq S_2$, $U$ cannot be a cluster of $G$.
	
For any $U \in S_1 \cup S_2 $ and $\abs{U} <= n$, by Corollary \ref{cor:complete}, $I(Z_U) < I(Z_{S_1})$.
Since either $U \cap S_1 \neq \emptyset$ or $U \cap S_2 \neq \emptyset$ holds, by Lemma \ref{lemma:sub}, $I(Z_{S_1})= I(Z_{S_2}) > I(Z_U) \Rightarrow U$ is not a cluster of $G$.

For $\abs{U} > n$ but $U \neq V = S_1 \cup S_2$. Let $\P' = \{\{i\}| i\in U\}$, $I(Z_U) \leq I_{\P'}(Z_V) = \frac{\frac{n(n-1)r}{2} + m_1 + \frac{k(k-1)r}{2}}{n+k-1}$ where $m_1 \leq nk$ and $k=\abs{U} - n < n$. 
We compare it to $\frac{nr}{2}$ and we can show that $I(Z_{S_1}) - I_{\P'}(Z_U) = \frac{nk(n-1-k)}{2}$. If $k<n-1$, $I(Z_{S_1})>I(Z_U)$, $U$ is not a cluster by Lemma \ref{lemma:sub}. If $k=n-1$, and $I(Z_U) = I(Z_{S_1})$, then $m_1=nk$. In such case, for $\P=\{\{i\}|i\in V\}$ we have $I_{\P}(Z_V) = \frac{n(n-1)+n^2(n-1)}{2n-1} > I_{\P'}(Z_U)=\frac{n^2}{2}$. Then $I(Z_V) = I_{\P}(Z_V) > I_{\P'}(Z_U) \geq I(Z_U)$ and $U$ cannot be a cluster of $G$.

 In either case, $U$ is not a cluster of $G$ for $1<\abs{U}<2n, U \neq S_1, U\neq S_2$. Then $I(Z_V)$ can be computed by comparing $\P$ and $\{S_1, S_2\}$.

We call the graph $G$ is robust enough since complete linkage between the two clusters cannot destroy its structure.	
\end{proof}

\section{Algorithm Formulation}
In this section, we give the complete formulation of parametric Dilworth truncation algorithms. There are three algorithms in different level. \texttt{pdt}(Algorithm \ref{alg:pdt}) at the top level, \texttt{pmf}(Algorithm \ref{alg:pmfC})at the middle level and \texttt{mf}(Maximum Flow) at the bottom level.
\begin{algorithm}
	\caption{parametric Dilworth truncation $(\P, \mathcal{L})=\texttt{pdt}(G(V,E), c(e))$}\label{alg:pdt}
	\begin{algorithmic}[1]
		\REQUIRE a directed graph $G(V, E)$; edge cost function $c(e)$ for $e\in E$
		\ENSURE a nested family of partition $\P^{\lambda}$ and its corresponding turning point list $\mathcal{L}$
		\STATE initialize $y^{\lambda}_v = (0, +\infty)$ for $ v \in V$, $\P^{\lambda} = [\{\emptyset\}], \mathcal{L} = [+\infty]$
		\FOR{$j = 1$ to $\abs{V}$}
		\STATE  $(\mathcal{A}, L) = \texttt{pmf}(G(V,E), c(e), j, y^{\lambda})$
		\FOR{$u=1$ to $\abs{V}$}
		\IF{$u=j$}
		\STATE $y^{\lambda}_j = (f(\{j\}), +\infty)$
		\ELSE
		\STATE $ i = \max_i \{ u \in \mathcal{A}[i]\}$
		\IF{$i$ exists and $L[i] > a_u - b_u$}
		\STATE $y_u^{\lambda} = (a_u, a_u - L[i])$
		\ENDIF
		\ENDIF
		\ENDFOR
		\STATE $i, k = 0, \mathcal{Q} = \{\emptyset\}, \widetilde{\mathcal{\P}}^{\lambda} = [], \widetilde{\mathcal{L}} = []$, append $+\infty$ to $L$.
		\WHILE{$i<\texttt{length}(\mathcal{L})$ \texttt{ and } $k<\texttt{length}(L)$}
		\IF{$ \P^{\lambda}_i[\mathcal{A}_k] \neq \mathcal{Q}$}
		\STATE $\mathcal{Q} = \P^{\lambda}_i[\mathcal{A}_k]$
		\STATE append $\mathcal{Q}$ to $\widetilde{\mathcal{\P}}^{\lambda}$ and $\min\{\mathcal{L}[i], L[k]\}$
		to $\widetilde{\mathcal{L}}$
		\ELSE
		\STATE $\widetilde{\mathcal{L}}[-1] = \min\{\mathcal{L}[i], L[k]\}$
		\ENDIF
		\STATE $(i, k) \leftarrow \begin{cases} (i+1, k) & \mathcal{L}[i] < L[k] \\  (i, k+1) & \mathcal{L}[i] > L[k]\\ (i+1, k+1) & \mathcal{L}[i] = L[k]\end{cases}$
		\ENDWHILE
		\STATE $(\P^{\lambda}, \mathcal{L}) \leftarrow (\widetilde{\mathcal{\P}}^{\lambda},  \widetilde{\mathcal{L}})$
		\ENDFOR
	\end{algorithmic}
\end{algorithm}

We make some remark about Algorithm \ref{alg:pmfC}
\begin{enumerate}
\item For different $\lambda$, we need to initialize the the capacity of $\widetilde{G}$ by equation \eqref{eq:clambda}.
\begin{equation}\label{eq:clambda}
c^{\lambda}(i, j) = 
\begin{cases}
\max\{0, -\min\{a_i-\lambda, b_i\}\} &  i = s, j \neq t \\
w_{it} + \max\{0, \min\{a_i - \lambda, b_i\}\} & i\neq s, j = t\\
0 & i = s, j = t\\
w_{ij} & i \neq s, j \neq t
\end{cases}
\end{equation}
Since $\widetilde{G}$ for a specified $\lambda$ is used only once, we can modify the capacity directly on previous $\widetilde{G}$ without creating new graph data structure.
\item For \texttt{mf} algorithm we can only use push-relabel method. But we can choose different active node selecting scheme. For example, highest label is known to be practically fastest.
\item For line \ref{findLambda} in  Algorithm \ref{alg:pmfC}, we can simplify the equation as
\begin{equation}
	\sum_{v\in \widetilde{V} \backslash (S\cup T)} [-y^{\lambda}_v ] = f(T_r)-f(T_l)
\end{equation}
\end{enumerate}

\begin{algorithm}
	\caption{parametric maximum flow $(\mathcal{A}, L) = \texttt{pmf}(G(V,E), w(e), t, y^{\lambda})$}\label{alg:pmfC}
	\begin{algorithmic}[1]
		\REQUIRE a directed graph $G(V, E)$; $t \in V={1,2,\dots, \abs{V}}$; edge cost function $w(e)$ for $e \in E$; function $f(A)$ is the cut function w.r.t set $A\subseteq V$; tuples $y^{\lambda}_i = (a_i, b_i)$ for $i \in V$.
		\ENSURE a nested family of minimal value to $c^{\lambda}(T)$, as a reversely ordered set list $\mathcal{A}$ and its corresponding turning value $\lambda$ as an ordered value list $L$.
		\STATE Let $L$ be the turning point list, initialized as empty array.
		\STATE Construct $\widetilde{G}$: let $\widetilde{V}=V\cup\{s\}, \widetilde{E} = E\cup \{(s,j),(j,t)|j \in V, j\neq t\}$
		\STATE Initialize $c^{-\epsilon}(e)$ for $e \in \widetilde{E}$ according to equation \eqref{eq:clambda}.
		\STATE $ (T_0, \_)  = \mathtt{mf}(\widetilde{G}(\widetilde{V}, \widetilde{E}), c^{-\epsilon}(e))$. Also let $T_1 = \{j\}$.
		\STATE $\mathcal{A}$ contains the decreasing set list, initialized as $[T_0, T_1]$;
		\STATE \texttt{split}$( T_0, T_1)$
		\FUNCTION{\texttt{split}$(T_l, T_r)$}
		\STATE Let $\tilde{\lambda}_2$ be the value of $\lambda$ such that $c^{\lambda}(  \widetilde{V}\backslash T_l, T_l) = 
		c^{\lambda}(\widetilde{V} \backslash T_r, T_r)$ \label{findLambda}
		\STATE $h' = c^{\lambda}(\widetilde{V} \backslash T_r, T_r) $
		\STATE Initialize $c^{\tilde{\lambda}_2}(e)$ for $e \in \widetilde{E}$ according to equation \eqref{eq:clambda}.
		\STATE $(T', g') = \mathtt{mf}(\widetilde{G}(\widetilde{V}, \widetilde{E}), c^{\tilde{\lambda}_2}(e))$.
		\IF{$h' > g'$}
		\STATE insert $T'$ to $\mathcal{A}$
		\STATE \texttt{split}$(T_l, T')$
		\STATE \texttt{split}$(T', T_r)$
		\ELSE
		\STATE insert $\tilde{\lambda}_2$ to $L$
		\ENDIF
		\ENDFUNCTION
	\end{algorithmic}
\end{algorithm}

solve $\tilde{\lambda}_2$ and how to .

\section{Experiment Explanation}
In this section, we describe the details of our two experiments. The source code for info-clustering algorithm and experiments can be found at \href{https://github.com/zhaofeng-shu33/principal\_sequence\_of\_partition}{info-cluster}.


\subsection{Data Clustering}
For the three groups of data: \textsf{Gaussian}, \textsf{Circle} and \textsf{Iris}. The first two have some intrinsic randomness. To make the result reproducible, we use fixed the random seed to generate the data. The three circles data is generated using polar coordinate:
\begin{align*}
r &= 0.1*i + 0.01*(2u-1), i = 1, 2, 3\\
\theta & = 2\pi v
\end{align*}
where random variable $u,v$ conform uniform distribution within $[0,1]$. Also, $u$ and $v$ are independent.

Different metric (or affinity) can affect the clustering accuracy significantly. Some clustering algorithm such as \textsf{k-means} can only use Euclidean metric and performs badly on non-convex dataset such as \textsf{Circle}. Therefore, we do not use such algorithm in comparision. 

Some algorithm needs to specify the number of cluster.  For example, spectral clustering is a powerful method but lacks the flexibility because it needs the dimension of subspace(number of cluster).  We'd like to compare methods which have the ability to select the best cluster without knowing the ground truth. Both \textsf{affinity propagation} and \textsf{agglomerative clustering}\footnote{We can use a quality function to select the best layer in the hierarchical tree, for example modularity of Newman and Girvan.} meets the requirement. The agglomerative clustering algorithm we used is the standard one which merges two objects at each step. For \textsf{affinity propagation} and \textsf{agglomerative clustering} we use the implementation of \textsf{scikit-learn}(\cite{scikit-learn}) and tune their parameters respectively.

For real world data like Glass dataset, some pre-processing is necessary, for our experiment we just scale each feature dimension to $[0, 1]$ for all algorithms.

For \textsf{Gaussian} dataset, all three algorithms can get correct clustering result (\texttt{ari = 1.0}); for \textsf{Circle} dataset, affinity propagation method performs poorly even for nearest neighbor metric; for \textsf{Glass} dataset, info-clustering method has best performance compared with other method. To achieve this score, we use nearest neighbor affinity metric with 14 neighbors. We also tune the hyperparameters of the other two algorithms but neither of them can achieve the score of info-clustering.


\subsection{Community Detection}
There are many other community detection algorithms, but most of them can only divide the graph into several parts and get a one-level structure. For hierarchical community detection, some methods generate binary trees. For given nodes, binary tree tends to have large tree depth and is not suitable in many cases. To compare info-clustering algorithm fairly, we use other two hierarchical community detection algorithms (GN and BHCD) which allow non-binary tree structure.
 
There is some randomness in the generation of the two-level community. For info-clustering algorithms, it is sensitive to input graph and we average the normalized RF distance by running 20 times of experiment for the same graph configuration. For the other two algorithms, the distance remains unchanged for the same graph parameter $(z_{\mathrm{in}_1}, z_{\mathrm{in}_2}, z_{\mathrm{out}})$, hence we do not need to rerun many times.

For this experiment, we make courtesy to ETE Toolkit\cite{ete3} and NetworkX library \cite{SciPyProceedings_11}, which make the experiment easy to implement.

\bibliographystyle{plain}
\bibliography{exportlist}
\end{document}