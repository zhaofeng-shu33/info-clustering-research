\documentclass{article}
\usepackage{neurips_2019}
\usepackage{hyperref}       % hyperlinks
\input{extra_math.tex}
\title{Supplementary Material}
\begin{document}
\maketitle
\appendix
\section{Proofs of Propositions}
This section gives proof on propositions appeared in the paper. Before proceeding, we introduce some additional notations used in this section. $\Pi_B$ is the collection of partition on $B\subset V$. $E(C)=\{(i,j) | (i,j)\in E, i,j \in C\}$ is an edge subset of $E$. The meet of two partitions $\P_1 \wedge \P_2 = \max\{\P \in \Pi | \P \preceq \P_1, \P \preceq \P_2\}$. The notation $\wedge$ comes from lattice theory.

\begin{lemma}\label{lem:ref_combination}
For any $B \in \P' \in \Pi'$ and $\P_B \in \Pi'_B$, let $\P = \P_B \cup \P' \backslash \{B\} $
be the refinement of $\P'$. Then we have
\begin{equation}\label{eq:ref_combination}
I_{\P}(Z_V) = \theta I_{\P'}(Z_V) + (1-\theta) I_{\P_B}(Z_B)
\end{equation}
\end{lemma}
\begin{proof}
	First we have $\abs{\P} = \abs{\P'} -1 + \abs{\P_B}$,
 and
	\begin{align*}
		I_{\P}(Z_V) & = { f[\P'] + f_B[\P_B] \over \abs{\P} - 1} \\
		I_{\P'}(Z_V) & = { f[\P'] \over \abs{\P'} - 1} \\
		I_{\P_B}(Z_B) & = { f_B[\P_B] \over \abs{\P_B} - 1}
	\end{align*}
	As a result, there exists $\theta = {\abs{\P'} - 1 \over \abs{\P} - 1} \in (0,1)$ such that 
	\eqref{eq:ref_combination} holds.
\end{proof}
Although Lemma \ref{lem:ref_combination} has the same form with Corollary 5.3 in \cite{secretKey}, but the definition of $I_{\P}(Z_V)$ is different. 
%
\begin{lemma}\label{lem:smallZB}
Suppose $I_{\P}(Z_V) = I(Z_V)$, then we have $I(Z_V) \geq I_{\P_B}(Z_B) \geq I(Z_B)$ if $\exists \P_B \subseteq \P$.
\end{lemma}
\begin{proof}
Let $\P' = \{B\} \cup \P\backslash \P_B$, then $\P = \P_B \cup \P'\backslash\{B\}$ , use lemma \ref{lem:ref_combination}, we have
\begin{equation}
I_{\P}(Z_V) = \theta I_{\P'}(Z_V) + (1-\theta) I_{\P_B}(Z_B)
\end{equation}
Since  $I_{P}(Z_V) = I(Z_V), I_{\P'}(Z_V)\geq I(Z_V) \Rightarrow I(Z_V) \geq I_{\P_B}(Z_B) \geq I(Z_B)$ 
\end{proof}

\begin{lemma}\label{lemma:lattice}
If $I(Z_V) = I_{\P_1}(Z_V) = I_{\P_2}(Z_V)$, then $I_{\P_1 \wedge \P_2}(Z_V)=I(Z_V) $
\end{lemma}
\begin{proof}
Consider $C\in \P_1, C \not\in \P_1\wedge\P_2$ and $C = \cup_{i=1}^k B_i, k>1$ where $B_i \in \P_1 \wedge \P_2$.
Consider $\P_C = \{B_1, \dots, B_k\}$. Each $B_i$ belongs to one of subsets of $\P_2$. Suppose $B_1, B_2 \subseteq D \in \P_2$. Then $B_1\cup B_2 \subseteq D$,
which contradicts the maximal property of $\P_1\wedge \P_2$. Therefore, we have $\P_D = \{D_1, D_2, \dots D_k\}\subseteq \P_2$ such that $B_i \subset D_i$.
From lemma \ref{lem:smallZB}, we have $I_{\P_D}(Z_V) \leq I(Z_V)$. Since $\abs{\P_D} = \abs{\P_C}=k$, From the definition of $I_{\P}(Z_V)$ we have $I_{\P_C}(Z_V) \leq I_{\P_D}(Z_V) \leq I(Z_V)$.
Then using lemma \ref{lem:ref_combination}, let $\P' = \P_C \cup \P_1\backslash \{C\}$ we have
\begin{align*}
	I_{\P'}(Z_V) & = \theta I_{\P_1}(Z_V) + (1-\theta) I_{\P_C}(Z_C) \\
	& \leq \theta I(Z_V) + (1- \theta)I(Z_V) = I(Z_V)
\end{align*} 
Since $I(Z_V)$ is minimum value, then we have $I_{\P'}(Z_V)=I(Z_V)$.
$\P'\wedge \P_2 = \P_1 \wedge \P_2$ and we can repeat the above procedure using $\P_1 = \P'$. It terminates in finite steps since $\abs{V} > \abs{\P'} > \abs{\P_1}$.
\end{proof}

By Lemma \ref{lemma:lattice}, the finest partition $I_{\P}(Z_V)=I(Z_V)$ is unique, which is the meet of all partitions. Therefore, the top-down approach mentioned in the paper is unambiguous. We use $\P^*$ to denote the unique finest partition and it has the following property.

\begin{lemma}[Corollary 5.1 in \cite{secretKey}]\label{lemma:laminarity}
\begin{equation}\label{eq:P}
I(Z_{C_1 \cup C_2}) \geq \min\{ I(Z_{C_1}), I(Z_{C_2})\}, \textrm{ for } C_1\cap C_2 \neq \emptyset
\end{equation}
\end{lemma}

Lemma \ref{lemma:laminarity} implies the maximal requirement for the bottom-up approach mentioned in the paper is unambiguous. For $I(Z_{C_1}) = I(Z_{C_2}) = \max_{B\subseteq V} I(Z_B)$, if $C_1 \cap C_2 = \emptyset$, we can merge them respectively; if $C_1\cap C_2 \neq \emptyset$, $I(Z_{C_1\cup C_2}) \geq \max_{B\subseteq V} I(Z_B)$ therefore $C_1\cup C_2$ is also a solution to the maximization.



\input{proposition_triangle_inequality.tex}
\begin{proof}


We use deduction to show  
\begin{equation}\label{eq:GF}
f[\P] \geq \frac{\abs{\P}-1}{\abs{V}-1} \sum_{(i,j) \in E} w_{ij}
\end{equation}
for any $\P \in \Pi$. Then by theorem in the paper, Proposition \ref{prop:triangle} holds. 

Let $n=\abs{V}$. For $\abs{\P}=n$, the equality of equation \ref{eq:GF} holds. 

Suppose equation \eqref{eq:GF} holds for any $\abs{\P} \geq k+1(k\geq 2)$, and for $\P=\{C_1, \dots, C_k\}$, with $\abs{C_1} \leq \abs{C_r}$ for $r=2,\dots, k$. Let $\abs{C_1}=n_1(\geq 2), \P_{C_1} = \{\{i\}| i \in C_1\}, \P'=\P_{C_1} \cup \P \backslash \{C_1\}$. Then we have $\abs{\P'} = k+n_1-1$. Using equation \eqref{eq:GF} for $\P'$ we have
\begin{align}\label{eq:PPrelation}
f[\P'] & = \sum_{i\in C_1, k \not\in C_1} w_{ik} + \sum_{r=2}^k \sum_{(i,j) \in E(C_r)} w_{ij}\geq \frac{k+n_1 -2}{n-1}\sum_{(i,j) \in E} w_{ij} \\
f[\P] & =f[P'] - \sum_{(i,j) \in E(C_1)} w_{ij}
\end{align}
Applying triangle inequality $w_{ij} \leq w_{ik} + w_{jk}$ for given $k\not\in C_1$ and sum it over all $i, j \in C_1, i\neq j$, we have
$$
\sum_{(i,j) \in E(C_1)} w_{ij} \leq \sum_{(i,j) \in E(C_1)} (w_{ik} + w_{jk}) = (n_1-1)\sum_{i\in C_1} w_{ik}
$$
Summation over $k \not\in C_1$ we have 
$$
(n - n_1) \sum_{(i,j) \in E(C_1)} w_{ij} \leq (n_1 - 1) \sum_{i \in C_1, k \not\in C_1} w_{ik}
$$
Also
\begin{align*}
\sum_{(i,j) \in E} w_{ij}  & \geq \sum_{(i,j) \in E(C_1)} w_{ij} + \sum_{i\in C_1, k\not\in C_1} w_{ik} \\
(n_1 - 1)\sum_{(i,j) \in E} w_{ij}  & \geq (n_1 -1 )\sum_{(i,j) \in E(C_1)} w_{ij} + (n_1-1)\sum_{i\in C_1, k\not\in C_1} w_{ik} \\
& \geq (n_1 -1 )\sum_{(i,j) \in E(C_1)} w_{ij} + (n - n_1) \sum_{(i,j) \in E(C_1)} w_{ij}\\
& = (n-1) \sum_{(i,j) \in E(C_1)} w_{ij}
\end{align*}
We then have $\sum_{(i,j) \in E(C_1)} w_{ij} \leq \frac{n_1-1}{n-1}\sum_{(i,j) \in E} w_{ij}$. From equation \eqref{eq:PPrelation} we have 
$f[\P] \geq \frac{k-1}{n-1}\sum_{(i,j) \in E} w_{ij}$. That is, the result holds for $\abs{\P}=k$.
\end{proof}
\begin{corollary}\label{cor:complete}
	For a complete graph $G(V,E)$ with equal weight $w$ on each edge, we have $I(Z_{V})=\frac{nw}{2}$ where $n=\abs{V}$. And $I(Z_S) < I(Z_V)$ for any subset $S\subsetneq V$.
\end{corollary}
\begin{proof}
From Proposition \ref{prop:triangle}, $I(Z_{V})$ is achieved by $\P=\{\{1\},\dots,\{n\} \}$ and $I(Z_V) = I_{\P}(Z_V) = \frac{n(n-1)w/2}{n-1} = \frac{nw}{2} $. From bottom-up approach, $I(Z_V) = \max_{S\subset V} I(Z_S)$, therefore $I(Z_S) \leq I(Z_V)$. If $I(Z_S) = I(Z_V)$, consider $\P_S=\{i | i \in S\}$, then $I(Z_S) = I_{\P_S}(Z_S) = \frac{\abs{S}w}{2} < I(Z_V)$, a contradiction. Therefore, $I(Z_S) < I(Z_V)$ holds for any subset $S\subsetneq V$.
\end{proof}
\input{proposition_submodular_structure}
\begin{proof}
There are certain breakpoints $a_i - b_i$ which determines whether $y_i^{\lambda}$ takes constant values. We can get an increasing list of such breakpoints as $\tilde{\lambda}_1 < \dots < \tilde{\lambda}_n$ while $\abs{n}\leq \abs{V}$. If $b_i = +\infty, y^{\lambda}_i = a_i - \lambda$ and there is no turning point for this one.

We first show that if $ u < v $ within a breakpoint interval. That is $\tilde{\lambda}_j \leq u < v \leq \tilde{\lambda}_{j+1}$, then $T^v \subseteq T^v$. In such interval, let $S = \{i | y^{\lambda}_i = b_i\}$ and define $g_1(A) = \sum_{i\in A\backslash S} (a_i - \lambda), g_2(A)= \sum_{i \in A \cap S} b_i$. We can rewrite $\tilde{h}_{\lambda}(A) = f(A) - \lambda - g_1(A) - g_2(A) = g_3(A) + \lambda\abs{A}$ where $g_3(A)=f(A)-g_2(A)-\lambda(\abs{S}+1) - g_4(A)$, $g_4(A) = \sum_{i\in A\backslash S} a_i = \sum_{i \in A} a_i - \sum_{i \in A\cap S} a_i $. We can show that $g_3$ is a submodular function.
Let $A_1 = \argmin_{A} \tilde{h}_u(A), A_2 = \argmin_{A} \tilde{h}_v(A)$. Ignoring the constant part, we will have
\begin{align}
g_3(A_1) + u \abs{A_1}& \leq g_3(A) + u \abs{A}, \forall A \subseteq V \label{eq:fA1}\\
g_3(A_2) + v \abs{A_2}& \leq g_3(A) + v \abs{A}, \forall A \subseteq V \label{eq:fA2}
\end{align}
Using the submodular property of $g_3$ we have
\begin{align*}
g_3(A_1 \cap A_2) + g_3(A_1 \cup A_2)\leq  &g_3(A_1) + g_3(A_2) \\
\Rightarrow g_3(A_1 \cap A_2) + g_3(A_1 \cup A_2) + u \abs{A_1} + v\abs{A_2} 
\leq & g_3(A_1) + u \abs{A_1} + g_3(A_2) + v \abs{A_2}, \textrm{ by (\ref{eq:fA1}, \ref{eq:fA2})} \\
\Rightarrow g_3(A_1 \cap A_2) + g_3(A_1 \cup A_2) + u \abs{A_1} + v\abs{A_2} 
\leq &g_3(A) + u \abs{A}+ g_3(B) + v \abs{B}, \forall A, B \subseteq V \\
\textrm{ let } A =A_1 \cup A_2, B= A_1 \cap A_2 \Rightarrow u \abs{A_1} + v\abs{A_2} 
\leq & u \abs{A_1 \cup A_2}+ v \abs{A_1\cap A_2} \\
\Rightarrow u  \abs{A_2 \backslash A_1} \geq v \abs{A_2 \backslash A_1}
\end{align*}
Since $ u < v$, we have $\abs{A_2 \backslash A_1}=0$, that is $A_2 \subseteq A_1$, or $T^v \subseteq T^v$.

By the continuity of $\tilde{h}(\lambda)$, if $u<v$ belongs to different interval, we can use the breakpoints as the springboard. That is, if $\lambda_j < u \leq \lambda_{j+1} \dots \leq \lambda_{j+k} \leq v$,  we will have $ A_1 \supseteq A_{\lambda_{j+1}} \dots \supseteq A_{\lambda_{j+k}} \supseteq A_2$.
\end{proof}
\begin{lemma}\label{lemma:sub}
	If node set $S_1 \cap S_2 \neq \emptyset$ and $I(Z_{S_1}) > I(Z_{S_2})$, then $S_2$ is not a cluster of $G$.
\end{lemma}
\begin{proof}
	From Lemma \ref{lemma:laminarity},
	$I(Z_{S_1\cup S_2}) > \min\{I(Z_{S_1}), I(Z_{S_2})\} = I(Z_{S_2})$. From bottom-up formulation,  $S_2$ cannot be a cluster of $G$.
\end{proof}
\input{proposition_reweight.tex}

\begin{proof}[Proof of Proposition \ref{prop:reweight}]
	We will show that if $U \neq S_1$ and $U \neq S_2$, $U$ cannot be a cluster of $G$.
	
For any $U \in S_1 \cup S_2 $ and $\abs{U} \leq n$, by Corollary \ref{cor:complete}, $I(Z_U) < I(Z_{S_1})$.
Since either $U \cap S_1 \neq \emptyset$ or $U \cap S_2 \neq \emptyset$ holds, by Lemma \ref{lemma:sub}, $I(Z_{S_1})= I(Z_{S_2}) > I(Z_U) \Rightarrow U$ is not a cluster of $G$.

For $\abs{U} > n$ but $U \neq V = S_1 \cup S_2$. Let $\P' = \{\{i\}| i\in U\}$, $I(Z_U) \leq I_{\P'}(Z_V) = \frac{\frac{n^2(n-1)}{2} + m_1 + \frac{k(k-1)n}{2}}{n+k-1}$ where $m_1 \leq nk$ and $k=\abs{U} - n < n$. 
We compare it to $I(Z_{S_1}) = \frac{n^2}{2}$ and we can show that $I(Z_{S_1}) - I_{\P'}(Z_U) \geq \frac{nk(n-1-k)}{2(n+k-1)}$. If $k<n-1$, $I(Z_{S_1})>I(Z_U)$, $U$ is not a cluster by Lemma \ref{lemma:sub}. If $k=n-1$, and $I(Z_U) = I(Z_{S_1})$, then $m_1=nk$. In such case, $m\geq m_1=n(n-1)$, for $\P=\{\{i\}|i\in V\}$ we have $I_{\P}(Z_V) \geq \frac{n(n-1)+n^2(n-1)}{2n-1} > I_{\P'}(Z_U)=\frac{n^2}{2}$. Then $I(Z_V) = I_{\P}(Z_V) > I_{\P'}(Z_U) \geq I(Z_U)$ and $U$ cannot be a cluster of $G$.

 In either case, $U$ is not a cluster of $G$ for $1<\abs{U}<2n, U \neq S_1, U\neq S_2$. Then $I(Z_V)$ can be computed by comparing $I_{\P}(Z_V) = \frac{m+n^2(n-1)}{2n-1}$ and $I_{\{S_1, S_2\}}(Z_V)=m$.

\end{proof}

\section{Algorithm Formulation}
In this section, we give the complete formulation of parametric Dilworth truncation algorithms. There are three parts in different level. \texttt{pdt}(Algorithm \ref{alg:pdt}) at the top level, \texttt{pmf}(Algorithm \ref{alg:pmfC})at the middle level and \texttt{mf}(Maximum Flow) at the bottom level.

Algorithm \ref{alg:pdt} is an adaption and concrete implementation of Fig. 3 in \cite{RN4}.

\begin{algorithm}
	\caption{parametric Dilworth truncation $(\P, \mathcal{L})=\texttt{pdt}(G(V,E), c(e))$}\label{alg:pdt}
	\begin{algorithmic}[1]
		\REQUIRE a directed graph $G(V, E)$; edge cost function $c(e)$ for $e\in E$
		\ENSURE a nested family of partition $\P^{\lambda}$ and its corresponding turning point list $\mathcal{L}$
		\STATE initialize $y^{\lambda}_v = (0, +\infty)$ for $ v \in V$, $\P^{\lambda} = [\{\emptyset\}], \mathcal{L} = [+\infty]$
		\FOR{$j = 1$ to $\abs{V}$}
		\STATE  $(\mathcal{A}, L) = \texttt{pmf}(G(V,E), c(e), j, y^{\lambda})$
		\FOR{$u=1$ to $\abs{V}$}
		\IF{$u=j$}
		\STATE $y^{\lambda}_j = (f(\{j\}), +\infty)$
		\ELSE
		\STATE $ i = \max_i \{ u \in \mathcal{A}[i]\}$
		\IF{$i$ exists and $L[i] > a_u - b_u$}
		\STATE $y_u^{\lambda} = (a_u, a_u - L[i])$
		\ENDIF
		\ENDIF
		\ENDFOR
		\STATE $i, k = 0, \mathcal{Q} = \{\emptyset\}, \widetilde{\mathcal{\P}}^{\lambda} = [], \widetilde{\mathcal{L}} = []$, append $+\infty$ to $L$.
		\WHILE{$i<\texttt{length}(\mathcal{L})$ \texttt{ and } $k<\texttt{length}(L)$}
		\IF{$ \P^{\lambda}_i[\mathcal{A}_k] \neq \mathcal{Q}$}
		\STATE $\mathcal{Q} = \P^{\lambda}_i[\mathcal{A}_k]$
		\STATE append $\mathcal{Q}$ to $\widetilde{\mathcal{\P}}^{\lambda}$ and $\min\{\mathcal{L}[i], L[k]\}$
		to $\widetilde{\mathcal{L}}$
		\ELSE
		\STATE $\widetilde{\mathcal{L}}[-1] = \min\{\mathcal{L}[i], L[k]\}$ \footnotemark
		\ENDIF
		\STATE $(i, k) \leftarrow \begin{cases} (i+1, k) & \mathcal{L}[i] < L[k] \\  (i, k+1) & \mathcal{L}[i] > L[k]\\ (i+1, k+1) & \mathcal{L}[i] = L[k]\end{cases}$
		\ENDWHILE
		\STATE $(\P^{\lambda}, \mathcal{L}) \leftarrow (\widetilde{\mathcal{\P}}^{\lambda},  \widetilde{\mathcal{L}})$
		\ENDFOR
	\end{algorithmic}
\end{algorithm}
\footnotetext{$\widetilde{\mathcal{L}}[-1]$ is the last element of $\widetilde{\mathcal{L}}$}

We make some remark about Algorithm \ref{alg:pmfC}, which is discussed in detail in the paper.
\begin{enumerate}
\item For different $\lambda$, we need to initialize the capacity of $\widetilde{G}$ by equation \eqref{eq:clambda}.
\begin{equation}\label{eq:clambda}
c^{\lambda}(i, j) = 
\begin{cases}
\max\{0, -\min\{a_i-\lambda, b_i\}\} &  i = s, j \neq t \\
w_{it} + \max\{0, \min\{a_i - \lambda, b_i\}\} & i\neq s, j = t\\
0 & i = s, j = t\\
w_{ij} & i \neq s, j \neq t
\end{cases}
\end{equation}
Since $\widetilde{G}$ for a specified $\lambda$ is used only once, we can modify the capacity directly on previous $\widetilde{G}$ without creating new graph data structure.

\item For \texttt{mf}, we use push-relabel maximum flow algorithm\cite{Goldberg}.

 $(T,g',\tilde{f})=$\texttt{mf}$(\widetilde{G}(\widetilde{V}, \widetilde{E}), c^{\tilde{\lambda}_2}(e), \tilde{f})$. The first parameter of \texttt{mf} is graph topology structure; the second parameter is edge capacity map and the third parameter is a valid flow to be used as the initial flow of the push-relabel algorithm. The third parameter is optional. \texttt{mf} returns three values: $T$ is minimum cut of target side($t\in T$); $g'$ is the maximum flow value; $\tilde{f}$ is the maximum flow map.

For $\texttt{mf}$, we can choose different active node selecting scheme. For example, highest label selection rule is known to be fastest in practice.

\item For line \ref{findLambda} in  Algorithm \ref{alg:pmfC}, we can simplify the equation as
\begin{equation}\label{eq:solveL}
	\sum_{v\in T_l \backslash T_r} y^{\lambda}_v  = f(T_l)-f(T_r)
\end{equation}
\begin{proof}
	\begin{align*}
	c^{\lambda}(T_l) -
	c^{\lambda}(T_r)  =& f(T_l) + \sum_{j \in T_l} \max\{0, -y^{\lambda}_j\} + \sum_{j \in V\backslash T_l} \max\{0, y^{\lambda}_j\} \\
	-& [f(T_r) + \sum_{j \in T_r} \max\{0, -y^{\lambda}_j\} + \sum_{j \in V\backslash T_r} \max\{0, y^{\lambda}_j\}] \\
	= & f(T_l) - f(T_r) + \sum_{T_l \backslash T_r} (\max\{0, -y^{\lambda}_j\} - \max\{0, y^{\lambda}_j\} )
	\end{align*}
	From line \ref{findLambda} equal to zero, we have equation \eqref{eq:solveL}.
\end{proof}
The right-hand side of equation \eqref{eq:solveL} is constant while the left-hand side is a piecewise linear decreasing function. Since $T_l \supseteq T_r$, the left-hand side summation is not empty. We can show that equation in line \ref{findLambda} always has solution for Algorithm \ref{alg:pmfC}.
\begin{proof}
	we only need to show for the two parameters of function \texttt{split}, the following property always holds.
\begin{equation}\label{eq:solutionProperty}
\exists \lambda_1, \lambda_3 \,s.t.\, c^{\lambda_1}(T_l) \leq c^{\lambda_1}(T_r), c^{\lambda_3}(T_l) \geq c^{\lambda_3}(T_r)
\end{equation}
Then by the continuity of $c^{\lambda}(T)$, there exists $\tilde{\lambda} \in [\lambda_1, \lambda_3]$ such that line \eqref{findLambda} holds.

For the initial computation in Algorithm \ref{alg:pmfC},
	$T_l = T_0, T_r = T_1$. $\lambda_1 = -\epsilon$ by line \ref{alg:T0property}. On the other hand, for a sufficient large $\lambda=\lambda', T_1$ is minimum value of $c^{\lambda}(T)$, therefore $\lambda_3 = \lambda'$. 

Within \texttt{split}$(T_l, T_r)$, suppose equation \eqref{eq:solutionProperty} holds with $(\lambda_1, \lambda_3)$, then 
$c^{\lambda_1}(T_l) \leq c^{\lambda_1}(T')$ and $c^{\tilde{\lambda}_2}(T_l) = h' > c^{\tilde{\lambda}_2}(T')$ by line \ref{alg:lineLarge}. That is, equation \eqref{eq:solutionProperty} holds for \texttt{split}$(T_l, T')$ with $(\lambda_1, \tilde{\lambda}_2)$. Similarly, from $c^{\lambda_3}(T') \geq c^{\lambda_3}(T_r)$ and $c^{\tilde{\lambda}_2}(T') < h' = c^{\tilde{\lambda}_2}(T_r)$ we can get  equation \eqref{eq:solutionProperty} holds for \texttt{split}$(T_l, T')$ with $(\tilde{\lambda}_2, \lambda_3)$.

By deduction, we conclude equation \eqref{eq:solutionProperty} holds and line \eqref{findLambda} always has solution.
\end{proof}
\item In line \ref{alg:flowModify} of Algorithm \ref{alg:pmfC}, we use flow $\tilde{f}$ coming from line \ref{alg:T0property}. The modification is quite simple:
\begin{align}
\tilde{f}'(j,t) &= \min\{\tilde{f}(j,t), c^{\lambda}(j,t) \} \\
\tilde{f}'(i,j) &= \tilde{f}(i,j), j \neq t, (i,j) \in \widetilde{E}
\end{align}
This is because $\tilde{\lambda}_2 > -\epsilon$, by equation \eqref{eq:clambda}, $c^{\lambda}(s,j)$ increases and $c^{\lambda} (j,t)$ decreases. By only restricting $\tilde{f}'(j,t)$ not to exceed the capacity constraint can we get a valid preflow.
\end{enumerate}

\begin{algorithm}
	\caption{parametric maximum flow $(\mathcal{A}, L) = \texttt{pmf}(G(V,E), w(e), t, y^{\lambda})$}\label{alg:pmfC}
	\begin{algorithmic}[1]
		\REQUIRE a directed graph $G(V, E)$; $t \in V={1,2,\dots, \abs{V}}$; edge cost function $w(e)$ for $e \in E$; function $f(A)$ is the cut function w.r.t set $A\subseteq V$; tuples $y^{\lambda}_i = (a_i, b_i)$ for $i \in V$.
		\ENSURE a nested family of minimal value to $c^{\lambda}(T)$, as a reversely ordered set list $\mathcal{A}$ and its corresponding turning value $\lambda$ as an ordered value list $L$.
		\STATE Let $L$ be the turning point list, initialized as empty array.
		\STATE Construct $\widetilde{G}$: let $\widetilde{V}=V\cup\{s\}, \widetilde{E} = E\cup \{(s,j),(j,t)|j \in V, j\neq t\}$
		\STATE Initialize $c^{-\epsilon}(e)$ for $e \in \widetilde{E}$ according to equation \eqref{eq:clambda}.
		\STATE $ (T_0, \_, \tilde{f})  = \mathtt{mf}(\widetilde{G}(\widetilde{V}, \widetilde{E}), c^{-\epsilon}(e))$. Also let $T_1 = \{j\}$. \label{alg:T0property}
		\STATE $\mathcal{A}$ contains the decreasing set list, initialized as $[T_0, T_1]$;
		\STATE \texttt{split}$( T_0, T_1)$
		\FUNCTION{\texttt{split}$(T_l, T_r)$}
		\STATE Let $\tilde{\lambda}_2$ be the value of $\lambda$ such that $c^{\lambda}(  T_l) = 
		c^{\lambda}(T_r)$ \label{findLambda}
		\STATE $h' = c^{\tilde{\lambda}_2}(T_r) $
		\STATE initialize $\tilde{f}'$ from $\tilde{f}$ such that it is a valid preflow for $(\widetilde{G}(\widetilde{V}, \widetilde{E}), c^{\tilde{\lambda}_2}(e))$ \label{alg:flowModify}

		\STATE Initialize $c^{\tilde{\lambda}_2}(e)$ for $e \in \widetilde{E}$ according to equation \eqref{eq:clambda}.
		\STATE $(T', g', \tilde{f}') = \mathtt{mf}(\widetilde{G}(\widetilde{V}, \widetilde{E}), c^{\tilde{\lambda}_2}(e), \tilde{f}')$.
		\IF{$h' > g'$} \label{alg:lineLarge}
		\STATE insert $T'$ to $\mathcal{A}$
		\STATE \texttt{split}$(T_l, T')$
		\STATE \texttt{split}$(T', T_r)$
		\ELSE
		\STATE insert $\tilde{\lambda}_2$ to $L$
		\ENDIF
		\ENDFUNCTION
	\end{algorithmic}
\end{algorithm}


\section{Experiment Explanation}
In this section, we describe the details of our two experiments. 

\subsection{Data Clustering}
For the three groups of data: \textsf{Gaussian}, \textsf{Circle} and \textsf{Glass}. The first two have some intrinsic randomness. To make the result reproducible, we fix the random seed of our program. The three circles data is generated using polar coordinate:
\begin{align*}
r &= 0.1*i + 0.01*(2u-1), i = 1, 2, 3\\
\theta & = 2\pi v
\end{align*}
where random variable $u,v$ conform uniform distribution within $[0,1]$. Also, $u$ and $v$ are independent.

Different metric (or affinity) can affect the clustering accuracy significantly. Some clustering algorithm such as \textsf{k-means} can only use Euclidean metric and performs badly on non-convex dataset such as \textsf{Circle}. Therefore, we do not use such algorithm in our comparison. 

Some algorithm needs to specify the number of cluster.  For example, spectral clustering is a powerful method but lacks the flexibility because it needs the dimension of subspace(number of cluster).  We'd like to compare methods which have the ability to select the number of cluster without knowing the ground truth. Both \textsf{affinity propagation} and \textsf{agglomerative clustering}\footnote{We can use a quality function to select the best layer in the hierarchical tree, for example modularity of Newman and Girvan.} meet the requirement. The agglomerative clustering algorithm we used is the standard one which merges two objects at each step. For \textsf{affinity propagation} and \textsf{agglomerative clustering} we use the implementation of \textsf{scikit-learn}\cite{scikit-learn} and tune their parameters respectively.

For real world data like Glass dataset, some pre-processing is necessary, for our experiment we just scale each feature dimension to $[0, 1]$ for all algorithms.

For \textsf{Gaussian} dataset, all three algorithms can get correct clustering result (\texttt{ari = 1.0}); for \textsf{Circle} dataset, affinity propagation method performs poorly even for nearest neighbor metric; for \textsf{Glass} dataset, info-clustering method has best performance compared with other method. To achieve this score, we use nearest neighbor affinity metric with 14 neighbors. We also tune the hyper-parameters of the other two algorithms (ward linkage with pre-specified number of cluster equal to 7 for agglomerative clustering; Euclidean metric, preference equal to -50 and damping factor equal to 0.8 for affinity propagation) but neither of them can achieve the score of info-clustering.


\subsection{Community Detection}
There are many other community detection algorithms, but most of them can only divide the graph into several parts and get a one-level structure. For hierarchical community detection, some methods generate binary trees. For given nodes, binary tree tends to have large tree depth and is not suitable in many cases. To compare info-clustering algorithm fairly, we use other two hierarchical community detection algorithms (GN and BHCD) which allow non-binary tree structure.
 
There is some randomness in the generation of the two-level community for given $(z_{\mathrm{in}_1}, z_{\mathrm{in}_2}, z_{\mathrm{out}})$. Therefore we average the distance metric by running 20 times of experiment for the same graph configuration parameter. For BHCD method, we use symmetric network configuration with $\gamma = 0.4, \lambda=0.85, \delta=0.2$, which is tuned by grid-search method. For info-clustering and GN method, there is no hyper-parameters to tune. Notice that we assign weight to the graph by 
$w_{ij} = 1 + \abs{\{k | (i,k),(j,k) \in E \}}$
for info-clustering method.

For this experiment, we make courtesy to ETE Toolkit\cite{ete3} and NetworkX library \cite{SciPyProceedings_11}, which make the experiment easy to implement.

\bibliographystyle{plain}
\bibliography{exportlist}
\end{document}