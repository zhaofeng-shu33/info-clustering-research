\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2018

% ready for submission
% \usepackage{neurips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
    \usepackage[preprint]{neurips_2019}

% to compile a camera-ready version, add the [final] option, e.g.:
 %    \usepackage[final]{neurips_2018}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{subcaption}
\input{extra_math.tex}
\title{A practical information theoretic clustering method}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{
  anonymous
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
\input{english_abstract.tex}
\end{abstract}

\section{Introduction}
In traditional agglomerative clustering method, two clusters are agglomerated based on pairwise inter-cluster similarity.  For example, well-known single-linkage clustering method uses nearest distance metric between two clusters and merge the two with minimal distance\cite{RN16}.  However, pairwise comparision is not optimal since it loses information shared between more than two clusters.  To overcome this problem, we define a multivariate similarity measure starting from arbitrary pairwise similarity metrics. Using this multivariate measure in the agglomerative clustering case, we merge several clusters in one step with largest similarity and build our hierachical tree.

The multivariate measure we proposed is equivalent to graph strength \cite{RN12}, it measures the coherrent similarity shared by each component. However, this measure is compute expensive in its definitional form and cannot be used directly to merge clusters. A similar hierachical clustering method called \textbf{minimum average cost} exploits the structural property of the average cost and converts the problem to submodular function minimization\cite{RN7}. In this paper, we show that the same mathematical technique can be used and the special form of submodular function minimization is equivalent to principal sequence of partition\cite{RN3}, which can be solved efficiently by parametric maximal flow algorithm\cite{RN4}. 

The clustering method used in this paper is information theoretic since it is established on the mathematical theory of \textbf{info-clustering}\cite{RN1}. This theory mainly deals with random variable clustering and we generalize and realize it to data clustering scenario, which is the main contribution of this paper. Experiment on synthesized and real-world dataset shows the info-clustering method performs well. 

The paper is organized as follows. In Section \ref{sec:models}, we formulate the info-clustering problem and show its connection with theory. In Section \ref{sec:algorithm}, we propose the algorithm workflow to solve the info-clustering problem. In Section \ref{sec:experiment}, we give the experiement result and comparision of info-clustering with other methods. Finally, we give the conclusion in Section \ref{sec:conclusion}.

Throughout this paper, the graph is denoted by $G(V,E)$, with vertex set $V = \{1, 2, \dots, \abs{V}\}$ and edge set $E=\{(i,j) | w_{ij}>0\}$\footnote{$(\cdot, \cdot)$ is ordered pair. For undirected graph $(i,j) \in E \Rightarrow (j,i) \in E$}. $\P$ is a partition of $V$ and $\{V\}$ is a trival partition. $\Pi$ is the collection of all partitions of $V$. $\Pi' = \Pi \backslash \{V\}$ is the collection of non-trival partitions. $f(\cdot)$ is graph cut function, that is $f(C) = \sum_{i\in C, j\not\in C, (i,j) \in E} w_{ij}$. $f[\cdot]$ is function defined on $\Pi$, that is
\begin{equation}
f[\P] = \sum_{\substack{i\in C, j \not\in C\\ (i,j)\in E, C\in \mathcal{P}}} w_{ij} =
\begin{cases}
\frac{1}{2} \sum_{C\in \P}f(C)   & G \textrm{ is undirected} \\
\sum_{C\in \P}f(C)   & G \textrm{ is directed}
\end{cases}
\end{equation}
\section{Models}\label{sec:models}

Consider an undirected graph partition problem, in which we want to partition the graph nodes into several groups with largest similarity within each group and smallest similarity between different group. To formulate this problem mathematically, given $G(V, E)$ where the edge weight $w$ is nonnegative and represents pairwise similarity of nodes, we try to minimize the following quantity over different parition of $V$.
\begin{definition}[Multivariate similarity]\label{def:ms}
\begin{align}
I_{\P}(Z_V) & = \frac{ f[\P] }{  \abs{\mathcal{P}} - 1 }\\
I(Z_V) & = \min_{\mathcal{P} \in \Pi'(V)} I_{\mathcal{P}}(Z_V)  \label{eq:ms}
\end{align}
\end{definition}

We call the quantity in equation \eqref{eq:ms} \textbf{multivariate similarity} because it measures the similarity shared by multiple graph nodes. The definition has the same form with that of graph strength if  the whole graph nodes are considered\cite{RN12}. However, we can compute multivariate similarity for any subset of $V$. In such case, 
we use subgraph symbol $V'$ to replace $V$ in equation \eqref{eq:ms}.
\begin{example}\label{eg:three}
For a three node graph $G=(\{1,2,3\},\{(1,2),(1,3),(2,3)| w_{12}=w_{23}=1, w_{13}=5\})$, we have $I(Z_{\{1,3\}}) = 5$ and $I(Z_V) = 2$ which is minimal value among $I_{\{1\},\{2\},\{3\}}(Z_V)=3.5, I_{\{1, 2\},\{3\}}(Z_V)=6, I_{\{1,3\},\{2\}}(Z_V)=2, I_{\{1\},\{2,3\}}(Z_V)=6$.(see also \cite{RN9} Fig. 1)
\end{example}

Based on the definition \eqref{def:ms}, we have both a top-down and bottom-up version of hierachical clustering.

\input{proposition_top_bottom.tex}
%proof in attachment.
The two procedures are illustrated by figure \ref{fig:ta}. For the top-down approach, there may be multiple partition $\P$ that reaches the minimum in \eqref{eq:ms}, we can show that there is a unique finest partition and this one is used; for the bottom-up approach, there may be multiple intersecting subsets with equal maximal multivariate similarity, we can show that the union of these subsets also reach the maximal and maximal $C$ is unique.
%proof in attachment, before the proof of the above proposition. (2 statements)
\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{pic/two_approach.eps}
\caption{hierachical tree generation based on multivariate similarity. Left figure shows the top-down approach with each subset splitted. Right figure shows the bottom-up approach with the graph contracted.}\label{fig:ta}
\end{figure}

By the construction of the clustering tree, we can associate each tree node with a threshold value, which is multivariate similarity computed at that step.
\begin{example}
\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{pic/threshold.eps}
\caption{a clustering tree (left figure) for the weighted graph (right figure) by info-clustering method. Threshold values are $0.9, 1, 2$.}\label{fig:threshold}
\end{figure}
Consider a graph $G(V, E)$ with $V=\{1,2,3,4\}$ and $E=\{(1,2),(1,3),(3,4),(2,4)\}$ with edge weight $w_{12}=1,w_{13}=0.4,w_{24}=0.5,w_{34}=2$ (See Fig. \ref{fig:threshold}). The threshold values are the multivariate similarity $I(Z_{3,4})=2, I(Z_{1,2})=1$ and $I(Z_V)=0.9$ respectively. We can write the complete clustering solution as:
\begin{equation*}
\P = 
\begin{cases}
\{\{1,2,3,4\}\} & \lambda < 0.9 \\
\{\{1,2\},\{3,4\}\} & 0.9 \leq \lambda < 1 \\
\{\{1\},\{2\},\{3,4\}\} & 1 \leq \lambda < 2\\
\{\{1\},\{2\},\{3\},\{4\}\} & \lambda \geq 2
\end{cases}
\end{equation*}
Although the clustering tree does not distinguish $\{\{1\},\{2\},\{3,4\}\}$ with $\{\{1,2\},\{3\}, \{4\}\}$ ,Notice that partition $\{\{1,2\},\{3\}, \{4\}\}$ does not exist by the threshold method.
\end{example}
Using $I(Z_V)$ to cluster data directly is computing expensive, the same clustering result can be achieved by solving the following optimalization problem for all $\lambda$ (Corollary 2 in \cite{RN1}).
\begin{proposition}\label{prop:psp}
\begin{equation}\label{eq:hL}
h(\lambda) = \min_{\P \in \Pi'(V)}\{ f[\P] - \abs{\P} \lambda \}
\end{equation}
The optimal partitions for $h(\lambda)$ are finite and they can be arranged in a decrasing sequence $\P_1, \dots, \P_k$ such that $\P_1 \succeq \P_2 \dots \succeq \P_k$, This sequence is called \textbf{principal sequence of partitions} of $h(\lambda)$\cite{RN17}.
\end{proposition}
%proof in attachment

For given $\P$, the function about $\lambda$ is linear, therefore $h(\lambda)$ is piecewise linear and each turning point of $h(\lambda)$ corresponds to $\P_i, \P_{i+1}$ respectively. We can use the parametric maximal flow algorithm to compute $\P_i$ efficiently, which is shown in the next section.

The method is information theoretic since it is an extension of mutual information in information theory. When the graph node is random variable and the edge weight is mutual information metric, then equation \eqref{eq:ms} becomes one form of multivariate mutual information proposed in \cite{RN1}. When $I(Z_V)=0$, the random variables $Z_i$ are pairwise independent. 

Info-clustering assume the weight is additive and tends to produce trival hierachical structure when the value of weights are near equal. This is shown by the following theorem:
\begin{theorem}\label{thm:trival}
The paritition to minimize equation \eqref{eq:ms} is $\{\{1\},\{2\},\dots,\{\abs{V}\}\}$ if and only if equation \eqref{eq:GF} holds.
\begin{equation}\label{eq:GF}
\frac{f[\P]}{\abs{\P}-1} \geq \frac{\sum w_{ij, (i,j) \in E}}{\abs{V}-1} \textrm{ for any } \P \in \Pi'
\end{equation}
\end{theorem}
\begin{proof}
Since $h(\lambda)$ is piecewise linear, the first line segment of $h(\lambda)$ is $ - \lambda $ and the last line segment is $ \sum w_{ij, (i,j) \in E} - \abs{V} \lambda$. Their intersection point is $(\lambda_{+}, -\lambda_{+})$ where $\lambda_{+} = \frac{\sum w_{ij, (i,j) \in E}}{\abs{V}-1}$. Since $\frac{f[\P]}{\abs{\P}-1} \geq \lambda_{+} \iff f[\P] - \abs{\P}\lambda_{+} \geq - \lambda_{+}$ Equation \eqref{eq:GF} says the minimzer of $h(\lambda)$ at $\lambda = \lambda_{+}$ is either $\{V\}$ or $\{\{1\},\{2\},\dots,\{\abs{V}\}\}$ and $h(\lambda)$ has only two line segments. From geometric point of view, the solution to equation \eqref{eq:ms} is the first turning point of $h(\lambda)$ and it is equal to that of last turning point if and only if \eqref{eq:GF} holds.
\end{proof}
By theorem \ref{thm:trival}, we can show that all weights are of equal value, then by applying info-clustering we can only get each data point as a cluster. Generally, we have the following proposition.
\input{proposition_triangle_inequality.tex}
It inspires us in order to seperate different clusters, we need small magnitude of inter cluster weights and similar weight value of intra cluster.

\section{Algorithm}\label{sec:algorithm}

In this section, we review the basic theory to solve equation \eqref{eq:hL} and give the detailed pseudocode based on the work of \cite{RN4}.

Then we call $h(\lambda)$ the Dilworth truncation of function $ \frac{1}{2} f(C)$ and the computation of $h(\lambda)$ can be done by the algorithmic pyramid shown in figure \ref{fig:pyramid}. The method, called graph principal sequence of paritition is divided into three level. At the top level, $h(\lambda)$ is computed with $\lambda$ as a symbol, called parametric Dilworth truncation (pdt) (\cite{RN4} Fig. 3). The result of top level routine is a nested family of partition and its corresponding turning point. In example \ref{eg:three}, we have $[\{1,2,3\},\{\{1,3\},\{2\}\},\{\{1\},\{2\},\{3\}\}],[2,5,+\infty) $ as a return. At the middle level, one of graph node is treated as sink node and parametric maximal flow(pmf) routinue is called.
The pmf routinue is called $\abs{V}$ times. At the bottom level, the edge cost is fixed for given $\lambda$ and maximal flow routinue is called. By the analysis in \cite{RN17}, the time complexity for pmf is the same with that of maximal flow.
\begin{figure}
\centering
\begin{subfigure}{0.45\textwidth}
\includegraphics[width=\textwidth]{pic/dt.eps}
\caption{An illustration to proposition \ref{prop:psp}}\label{fig:psp}
\end{subfigure}
\begin{subfigure}{0.45\textwidth}
\includegraphics[width=\textwidth]{pic/pdt.eps}
\caption{Algorithmic pyramid to compute principal sequence of partitions. The top level is parametric Dilworth truncation; the middle level is parametric maximal flow and the bottom level is maximal flow}\label{fig:pyramid}
\end{subfigure}
\end{figure}

For the pmf algorithm applied to our problem, we are required to solve 
\begin{equation}\label{eq:pmq}
\min_{j \in A \subseteq V} f(A) - 2\lambda - y^{\lambda}(A)
\end{equation}
where
$y^{\lambda}_i = \min\{a_i - 2\lambda, b_i\}$, where $b_i$ maybe $+\infty$. We can make the parameter dependent part on the edge cost and turn the undirected graph to the directed graph by splitting each edge into two directed arc with same capacity value. We also introduce a source
node $s$ while $j$ is treated as target node.  then change
$c^{\lambda}(s,v)=\max\{0, -y^{\lambda}_v\}, c^{\lambda}(v,j) += \max\{0, y^{\lambda}_v\}$ for $ v=1 $ to $\abs{V}$ and $v \neq  j$. Then we claim that
\begin{proposition}
The minimum minimizer to 
\begin{equation}\label{eq:pmqe}
\min_{s\in U\backslash T, j\in T}c^{\lambda}(U\backslash T, T)
\end{equation}
is the same with equation \eqref{eq:pmq}.
\end{proposition}
By doing so, we can simplify the formulation of pmf in \cite{RN17} and the pseudocode is given in Algorithm \ref{alg:pmf}.
\begin{algorithm}
\caption{paramatric maximum flow $(\mathcal{A}, L) = \texttt{pmf}(G(V,E), c(e), j, y^{\lambda})$}\label{alg:pmf}
\begin{algorithmic}[1]
\REQUIRE an undirected graph $G(V, E)$; $j \in V={1,2,\dots, \abs{V}}$; edge cost function $c(e)$ for $e \in E$; function $f(A)$ is the cut function w.r.t set $A\subseteq V$; tuples $y^{\lambda}_i = (a_i, b_i)$ for $i \in V$.
\ENSURE a nested family of minimal value to equation \eqref{eq:pmq}, as a set list $\mathcal{A}$ and its corresponding turning value $\lambda$ as a value list $L$.
\STATE Let $L$ be the turning point list, initialized as empty array, let $V'=V\cup\{s\}$.
\STATE find $(S_0, T_0)$ which is optimal to \eqref{eq:pmqe} for $ \lambda  = 0$.  Also let $T_1 = \{j\}$ and $S_1 = \{s\}\cup V \backslash \{j\}$.
\STATE $\mathcal{A}$ contains the decreasing set list, initialized as $[T_0, T_1]$;
\STATE \texttt{slice}$( S_0, T_1)$
\FUNCTION{\texttt{slice}$(S, T)$}
  \STATE Let $\lambda_2$ be the value of $\lambda$ such that $c^{\lambda}(S, V'\backslash S) = 
c^{\lambda}(V'\backslash T, T)$ \label{findLambda}
\STATE for $\lambda = \lambda_2$, find $(S', T')$ which is optimal to \eqref{eq:pmqe} for $\widetilde{G}$.
\IF{$S' \neq S$ and $T' \neq T$}
\STATE insert $T'$ to $\mathcal{A}$
\STATE \texttt{slice}$(S, T')$
\STATE \texttt{slice}$(S', T)$
\ELSE
\STATE insert $\lambda_2$ to $L$ 
\ENDIF
\ENDFUNCTION
\end{algorithmic}
\end{algorithm}

\section{Experiment}\label{sec:experiment}
In this section, we demonstrate info-clustering algorithm on two kinds of datasets. The first is about feature clustering, in which we need to cluster multi-dimentional vectors. For this kind of task, we need to construct the clustering graph first by using proper similarity measure. The other dataset is in community detection, coming from \cite{RN22}, in which a two-level hierachical structure is defined. By comparision with other clustering method, we will show that info-clustering algorithm gives reasonable good result and can recover the two-level hierachical structure under certain conditions. 

For feature clustering task, we prepare three groups of data:
\begin{enumerate}
\item Four Gaussian blobs with unit variance and centers at $(3,3), (3,-3), (-3,3), (-3,-3)$.  We generate 25 points for each blob.
\item Three concentric circles with radius $0.1,0.2,0.3$ and number of data points as $60, 100, 140$. The angles of these points are uniform distributed and they also oscillate around the radius direction.
\item Iris flower dataset with 50 samples, 3 classes, 4 dimensions of feature.
\end{enumerate}
We use adjusted rand index as the clustering metric to score different clustering method, the best performance for each algorithm is shown in Table \ref{tb:e1}.  It can be seen from the table that info-clustering algorhthm is competitive with other clustering algorithms.
\begin{table}
\centering
\InputIfFileExists{build/compare_3.tex}{}{}
\caption{ accuracy for different clustering algorithms }\label{tb:e1}
\end{table}

For community detection task, we use a two-level artifical dataset proposed in \cite{RN22}. 
The community structure is shown in Figure \ref{fig:c1}. In macro-level, it has 4 communities and each macro community contains 4 micro communities. We use $z_{\mathrm{in}_1}$ to represent the internal average degree of nodes within micro community; $z_{\mathrm{in}_2}$ as node degree between different micro communities but within one macro community; $z_{\mathrm{out}}$ as node degree between different macro communities. By varying $z_{\mathrm{in}} + z_{\mathrm{in}_2} + z_{\mathrm{out}}$ we get different set of experiment and in each given community parameter, we compare the detection accuracy between different methods. The result is shown in Table.

\begin{figure}
\centering
\includegraphics[width=6cm]{pic/two_level.eps}
\caption{a community with 256 nodes, 16 components in micro-level, each 4 components compose a macro-level larger part of this community. $z_{\mathrm{in}_1} = 14, z_{\mathrm{in}_2} = 3, z_{\mathrm{out}}=1$}\label{fig:c1}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=10cm]{pic/tree.pdf}
\caption{a community with $z_{\mathrm{in}_1} = 14, z_{\mathrm{in}_2} = 2.3$ and $z_{\mathrm{out}}=0.1$. The tree depth is only 4.}\label{fig:c2}
\end{figure}

\section{Conclusion}\label{sec:conclusion}


\subsubsection*{Acknowledgments}



\bibliographystyle{plain}
\bibliography{exportlist}


\end{document}
